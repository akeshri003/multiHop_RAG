{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ec355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\r\n",
      "You should consider upgrading via the '/Users/aryankeshri/Downloads/Entivin_Task/lang_graph_rag/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b750a935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b350963",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2=True\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=\"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\"\n",
    "LANGCHAIN_PROJECT=\"multi-hop-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64213a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path = \"/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8068ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 0}, page_content='Multi-hop Question Answering\\nSuggested Citation: Vaibhav Mavi, Anubhav Jangra and Adam Jatowt (2023), “Multi-\\nhop Question Answering”, : Vol. xx, No. xx, pp 1–75. DOI: 10.1561/XXXXXXXXX.\\nVaibhav Mavi\\nNew York University, United States of America\\nvaibhavg152@gmail.com\\nAnubhav Jangra\\nIndian Institute of Technology Patna, India\\nanubhav0603@gmail.com\\nAdam Jatowt\\nUniversity of Innsbruck, Austria\\njatowt@acm.org\\nThis article may be used only for the purpose of research, teaching,\\nand/or private study. Commercial use or systematic downloading (by\\nrobots or other automatic processes) is prohibited without explicit\\nPublisher approval. Boston — Delft\\narXiv:2204.09140v2  [cs.CL]  31 May 2024'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 1}, page_content='Contents\\n1 Introduction 3\\n1.1 Question Answering . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 What is Multi-hop Question Answering (MHQA)? . . . . . 4\\n1.3 Applications of MHQA . . . . . . . . . . . . . . . . . . . 5\\n1.4 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2 Formulating the Multi-Hop Question Answering Task 10\\n3 A Comprehensive Study of Datasets: Analysis and Guidelines15\\n3.1 Dataset Creation . . . . . . . . . . . . . . . . . . . . . . 15\\n3.2 Existing Datasets: Statistics, Comparisons and Examples . 24\\n3.3 Critiques and Challenges . . . . . . . . . . . . . . . . . . 25\\n4 Existing Approaches for MHQA 28\\n4.1 Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n4.2 Reading Comprehension . . . . . . . . . . . . . . . . . . . 40\\n4.3 Answer Prediction Module . . . . . . . . . . . . . . . . . 52\\n4.4 Auxiliary Tasks . . . . . . . . . . . . . . . . . . . . . . . . 55\\n4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . 57\\n5 LLMs for MHQA 58\\n5.1 Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . 58'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 2}, page_content='5.2 Reasoning Chain Generation . . . . . . . . . . . . . . . . 59\\n5.3 Hybrid Text-table Reasoning . . . . . . . . . . . . . . . . 61\\n5.4 Question Decomposition . . . . . . . . . . . . . . . . . . 61\\n5.5 Graph Construction . . . . . . . . . . . . . . . . . . . . . 62\\n5.6 Multi-hop Retrieval Augmented Generation . . . . . . . . 63\\n5.7 Critique and Limitations . . . . . . . . . . . . . . . . . . . 63\\n6 MHQA Taxonomy 67\\n7 How to Evaluate MHQA Systems? 71\\n7.1 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . 71\\n7.2 Adversarial Evaluation . . . . . . . . . . . . . . . . . . . . 74\\n7.3 Verifying the Extent of Multi-Hop Reasoning . . . . . . . .75\\n8 Multi-Hop Question Generation 79\\n8.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n8.2 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\n8.3 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\n9 Future of MHQA 85\\n9.1 Flexible Any-Hop Models . . . . . . . . . . . . . . . . . . 86\\n9.2 Explainable Multi-Hop QA . . . . . . . . . . . . . . . . . 86\\n9.3 Better Datasets . . . . . . . . . . . . . . . . . . . . . . . 87\\n9.4 Better Evaluation Metrics . . . . . . . . . . . . . . . . . . 87\\n9.5 Methods to Incorporate Commonsense . . . . . . . . . . . 88\\n9.6 Arithmetic Questions . . . . . . . . . . . . . . . . . . . . 89\\n9.7 Better Incorporation of Powerful LLMs . . . . . . . . . . . 90\\n9.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\nAppendices 91\\nA Background 92\\nA.1 BM25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\nA.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . 93\\nA.3 Transformers for Language Modeling . . . . . . . . . . . . 93\\nA.4 Graph Neural Networks . . . . . . . . . . . . . . . . . . . 95'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 3}, page_content='A.5 Large Language models . . . . . . . . . . . . . . . . . . . 96\\nReferences 100'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 4}, page_content='Multi-hop Question Answering\\nVaibhav Mavi1, Anubhav Jangra2 and Adam Jatowt3\\n1New York University, United States of America;\\nvaibhavg152@gmail.com\\n2Indian Institute of Technology Patna, India; anubhav0603@gmail.com\\n3University of Innsbruck, Austria; jatowt@acm.org\\nABSTRACT\\nThe task of Question Answering (QA) has attracted signif-\\nicant research interest for long. Its relevance to language\\nunderstanding and knowledge retrieval tasks, along with the\\nsimple setting makes the task of QA crucial for strong AI\\nsystems. Recent success on simple QA tasks has shifted the\\nfocus to more complex settings. Among these, Multi-Hop\\nQA (MHQA) is one of the most researched tasks over the\\nrecent years. In broad terms, MHQA is the task of answer-\\ning natural language questions that involve extracting and\\ncombining multiple pieces of information and doing multi-\\nple steps of reasoning. An example of a multi-hop question\\nwould be “The Argentine PGA Championship record holder\\nhas won how many tournaments worldwide?”. Answering\\nthe question would need two pieces of information: “Who is\\nthe record holder for Argentine PGA Championship tour-\\nnaments?” and “How many tournaments did [Answer of\\nSub Q1] win?”. The ability to answer multi-hop questions\\nand perform multi step reasoning can significantly improve\\nthe utility of NLP systems. Consequently, the field has seen\\na surge with high quality datasets, models and evaluation\\nstrategies. The notion of ‘multiple hops’ is somewhat ab-\\nstract which results in a large variety of tasks that require\\nVaibhav Mavi, Anubhav Jangra and Adam Jatowt (2023), “Multi-hop Question\\nAnswering”, : Vol. xx, No. xx, pp 1–75. DOI: 10.1561/XXXXXXXXX.\\n©2024 V. Mavi and A Jangra and A. Jatowt'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 5}, page_content='2\\nmulti-hop reasoning. This leads to different datasets and\\nmodels that differ significantly from each other and makes\\nthe field challenging to generalize and survey. We aim to\\nprovide a general and formal definition of the MHQA task,\\nand organize and summarize existing MHQA frameworks.\\nWe also outline some best practices for building MHQA\\ndatasets. This book provides a systematic and thorough in-\\ntroduction as well as the structuring of the existing attempts\\nto this highly interesting, yet quite challenging task.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 6}, page_content='1\\nIntroduction\\n1.1 Question Answering\\nAn eventual goal of artificial intelligence (AI) is to impart the ability\\nto reason over natural language to machines. In order to achieve this,\\nseveral natural language understanding and generation tasks have been\\nproposed that require an agent to do some reasoning to get to the goal.\\nOne such example is the task of Question Answering (QA) where given\\na question and some relevant context, the goal is to predict the correct\\nanswer. The question answering task provides a quantifiable way to\\nevaluate a system’s capability of language understanding and reasoning\\n(Qiu et al., 2019; Rajpurkaret al., 2016a; Hermannet al., 2015). It is a\\ncritical problem in the fields of natural language processing (NLP) and\\ninformation retrieval (IR), and a long-standing AI milestone.\\nAbundance of readily-available, high-quality information on the\\ninternet facilitates the need of automated QA systems that help probe\\nthis rich content based on individual needs. Due to recent advancements\\nin Deep Learning techniques (Lanet al., 2019), the machines have\\nbecome able to successfully beat human performance on datasets like\\nSQUAD 2.0 (Rajpurkaret al., 2016b). However, we have only scratched\\nthe surface of what these modern systems are capable of achieving.\\n3'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 7}, page_content='4 Introduction\\nDepending on the user requirements, the complexity of QA tasks may\\nvary. Some questions can be answered in brief (e.g., “Which color do\\nyou get when you mix red and yellow paints?”) - such questions are\\ncalled objective questionsor factoid questions. On the other hand, there\\nexist subjective questionsthat demand detailed explanations to meet\\nuser requirements (e.g., “Why does mixing red, green and blue paints\\ngive black color paint, but projecting red, green, and blue light on a white\\nsurface return white light?”). A question can also be considered complex,\\nif it requires a very niche domain expertise to answer the question (e.g.,\\n“What symptoms help diagnose chickenpox?”).\\n1.2 What is Multi-hop Question Answering (MHQA)?\\nFor questions mentioned above, there might exist a single document or\\na single passage (formally referred as a ‘context’) that can provide a\\njustifiable answer. However, there exists certain questions that cannot\\nbe answered using a singlecontext (e.g., “What is the national bird of\\nthe nation that has a negative carbon footprint?”). The task of answering\\nsuch questions is called multi-hop question answering (MHQA). The\\ngoal of MHQA is to predict the correct answer to a question that\\nrequires multiple reasoning ‘hops’ across given contexts (text, table,\\nknowledge graph etc). We look at a more detailed definition of the task\\nin Chapter 2.\\nThe success in simple QA systems (also referred assingle hop QA)\\ndoes not necessarily entail success of MHQA systems. Minet al.(2018)\\nand Qiuet al.(2019) observe that most questions in existing single-hop\\nQA datasets are answerable without much reasoning, by retrieving a\\nsmall set of sentences. Moreover, multi-step reasoning is required by\\nthe models to answer complex questions (refer to Table 1.1). Humans\\ncan easily perform these multi-step reasoning in their everyday tasks,\\nyet this is still a difficult task for machines. An agent can be said to\\nperform multi-step reasoning if it reaches one or more intermediate\\nconclusions before deriving the final answer and each of the intermediate\\nconclusions serves as a necessary premise for some other conclusion.\\nThis sequence of intermediate conclusions, including the final answer, is\\ncalled areasoning chainand each step from one conclusion to the next'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 8}, page_content='1.3. APPLICATIONS OF MHQA 5\\ncan be referred to as ahop.\\nTable 1.1:Examples of various types of multi-hop questions.\\nType of question Question Answer\\nBridge Entity-based\\n(temporal entity)\\nWho was the president of United States\\nin the year in which Mike Tyson\\ndeclared his retirement?\\nGeorge W. Bush\\nBridge Entity-based\\n(geographical entity)\\nWhat is the national bird of the nation\\nthat has a negative carbon footprint? The Raven\\nBridge Entity-based\\n(named entity)\\nWhat is the birth place of the tennis\\nplayer who has won the most grand slams?Belgrade, Serbia\\nIntersection Who is the only person to win\\nan olympic medal and a Nobel prize? Philip John Noel-\\nBaker\\nComparison Which country has won more\\nsoccer world cups - Argentina or Brazil?Brazil\\nCommonsense Reasoning\\nIf A prefers fruits over meat,\\nwhen given an option of apple and\\nchicken sandwich, what will A prefer?\\nApple\\nIt is important to note that the inability of AI systems to perform\\nmultiple steps of reasoning can be severely limiting, significantly re-\\nducing their usability. One such instance can be as shown in Fig. 1.1.\\nSay a user is interested in knowing more about ‘the daughter ofA’ and\\nthe only relevant information available in this context is ‘B’s father is\\nC and her mother isA’. In this case, the AI system has to first infer\\nthat B is female and her mother isA. The system will then have to use\\ncommon sense reasoning to conclude thatB is the entity of interest and\\nthen retrieve the required information (refer to Fig. 1.1 for visual aid).\\nSomething like this seems trivial to humans but it may fatally confuse\\nmany existing AI systems. Therefore, we argue that multi-step reasoning\\nis a crucial challenge and solving it can be a giant leap towards the\\ngoals of AI.\\n1.3 Applications of MHQA\\nAs discussed above, MHQA serves as an appropriate benchmark task\\nfor evaluating an agent’s ability to perform multi-step reasoning. Along\\nwith this scientific significance, the task of MHQA has various prac-\\ntical applications. Queries given to current web search systems can\\noften require multi-hop reasoning to reach the relevant documents.\\nUser satisfaction when using such systems can be greatly improved\\nby utilizing multi-hop reasoning models. Furthermore, conversations'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 9}, page_content=\"6 Introduction\\n♂\\nAvailable Context - B's father is C and her mother is A\\nC\\n♀\\nA\\nB\\nis father of is mother of\\n?\\nDeduction - B is the daughter of A.\\nQuestion - Who is the daughter of A?\\nFigure 1.1:An example of multi-hop reasoning\\nbetween humans and agents can be smoother and more informative if\\nthe latter can handle complex questions. Answering a multi-hop ques-\\ntion requires systems to aggregate information over multiple contexts.\\nTherefore, techniques that are successful for MHQA can inspire progress\\nin tasks such as sentence fusion (Weisset al., 2021; Gevaet al., 2019b)\\nand abstractive summarization (Nayeemet al., 2018; Lebanoffet al.,\\n2019), event occurrence time prediction (Wanget al., 2021c), as well as\\nmulti-document summarization (Maet al., 2020; Goldsteinet al., 2000;\\nHaghighi and Vanderwende, 2009; Barzilayet al., 1999) or timeline\\nsummarization (Yanet al., 2011; Ghalandari and Ifrim, 2020; Steen and\\nMarkert, 2019; Yuet al., 2021) that require information aggregation\\nover multiple documents. Additionally, most applications of QA such as\\ninformation extraction (IE) and entailment, can be immensely benefited\\nby multi-hop reasoning abilities (Boroset al., 2021).\\nKumar et al.(2019) argue that MHQA is a challenging task to an\\nextent that they quantify the difficulty of a question as the number of\\ninference steps (or hops) required to answer the question. This illustrates\\nthedirectutilityofMHQAforthetaskofDifficultycontrollableQuestion\\nGeneration (DQG) (Gao et al., 2018) that has various applications\\nincluding curriculum-learning based methods for QA systems (Kurdiet\"), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 10}, page_content='1.4. OVERVIEW 7\\nal., 2019) and designing school exams of certain difficulty levels (Sachan\\nand Xing, 2016).\\nAnother problem closely related to MHQA consists of generating\\nclarifying questions for conversational QA (chatbots) (Sunet al., 2021;\\nZaib et al., 2021). In this setting, the original question/query can be\\nambiguous and hence more information is needed to disambiguate it.\\nThe model is supposed to generate a clarifying question in natural\\nlanguage, asking the user for the missing information. This can be\\nconsidered as another task involving multi-step reasoning and can be\\ngreatly helped by improvements in MHQA.\\n1.4 Overview\\nRecently, a variety of datasets and techniques have been proposed for\\nMHQA, including ones designed for MHQA over Knowledge Bases and\\nKnowledge Graphs as well as those designed for QA over tables and\\ntext. A substantial number of recent works have focused on the task\\nof MHQA and contributed to significant advancements. High quality\\ndatasets (Yanget al., 2018; Welblet al., 2018; Kočisk` yet al., 2018;\\nMihaylov et al., 2018; Khashabiet al., 2018; Chenet al., 2020b; Khot\\net al., 2020) have encouraged better models to be proposed which in\\nturn have achieved impressive accuracy on these benchmarks. There\\nhas been a significant research in the recent years to solve the task. A\\nvariety of methods model the task as performing inference over static\\nor dynamic graphs to find the reasoning paths (Dinget al., 2019; Fang\\net al., 2020; Zhanget al., 2021; Caoet al., 2019; Thayaparanet al., 2019;\\nDe Caoet al., 2019; Zhanget al., 2020; Qiuet al., 2019; Huang and\\nYang, 2021; Shaoet al., 2020; Cao and Liu, 2021). A number of works\\nhave also attempted to decompose the multi-hop questions into single\\nhop questions or generate follow-up questions based on the retrieved\\ninformation (Min et al., 2019b; Cao and Liu, 2021; Sunet al., 2021;\\nZhang et al., 2021; Malon and Bai, 2020). The recent success of large\\nlanguage models (LLMs) has significantly influenced MHQA as well,\\nwith multiple attempts of using LLMs’ strong natural understanding\\nand emergent abilities for answering complex multi-hop questions (Zhao\\net al., 2023b; Patelet al., 2022; Balepuret al., 2023; Wanget al., 2023;'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 11}, page_content='8 Introduction\\nRahgouy et al., 2023; Xuet al., 2021). We discuss all these methods in\\na detailed and organized manner in Chapters 4, 5 and 6.\\nDue to the surge in the attention received by the task over the\\nlast decade, we believe that the community would benefit from an\\nextensive survey encompassing recent advancements in MHQA. In\\nthis work, we closely cover∼75 works from top venues including but\\nnot limited to EMNLP, ACL, NAACL, TACL, AAAI, EACL, SIGIR,\\nICLR, COLING, CoRR etc. published from 2016 to 2024. The research\\ncommunity has already several surveys in the field of question-answering,\\nsuch as for single-hop QA (Allam and Haggag, 2012; Bouzianeet al.,\\n2015; Mishra and Jain, 2016; Höffneret al., 2017; Soares and Parreiras,\\n2020; Dimitrakiset al., 2020), open-domain QA (Roy and Anand, 2021;\\nEtezadi and Shamsfard, 2023; Zhuet al., 2021), medical QA (Linet al.,\\n2021; Jinet al., 2022), visual QA (Srivastavaet al., 2020; Wuet al.,\\n2017), etc. The surveys that are most relevant to MHQA are the ones\\nfocused on QA over knowledge bases (Fuet al., 2020; Lanet al., 2021;\\nDiefenbachet al., 2018; Roy and Anand, 2021) and visual QA (Srivastava\\net al., 2020; Linet al., 2021; Wuet al., 2017). However, these can be\\nconsidered as sub-domains of the more general formulation of the MHQA\\nfield that this book aims to survey. Since the existing works go a long\\nway in summarizing their intended domains, we choose to exclude Visual\\nMHQA and MHQA over Knowledge Bases and Knowledge Graphs from\\nthe scope of this work.\\nWe observe that despite the impressive accuracy of recent models\\non MHQA benchmarks, significant concerns have been raised regarding\\nwhether the models are actually able to perform multi-step reasoning in\\norder to answer the multi-hop questions. Several works (Jansen, 2018;\\nWanget al., 2019; Chen and Durrett, 2019; Minet al., 2019a; Trivedi\\net al., 2020; Jhamtani and Clark, 2020; Inoueet al., 2020; Tanget al.,\\n2021; Tuet al., 2020) conduct experiments and demonstrate that a\\nsignificant portion of the accuracy can be ascribed to pattern matching\\nand single step reasoning (also termed asshortcut reasoning). This\\npoints to new challenges and future directions for research in MHQA.\\nAbove all, it is fair to say that despite the inspiring progress made so\\nfar, the task of MHQA is still a long way from being solved.\\nA promising direction for solving some of these challenges is the task'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 12}, page_content='1.4. OVERVIEW 9\\nof explainable MHQA, a particular setting of MHQA that requires the\\nmodel to output the correct reasoning chain (or equivalently, some kind\\nof representation of the reasoning chain) along with the correct answer.\\nThis increases the model’s accountability and interpretability to the end\\nuser since the model now has to also explain how it reached the answer.\\nInterpretability of the AI systems is crucial for their wide adoption for\\nmost high-stake applications such as finance, law and healthcare (Samek\\net al., 2017; Alvarez-Melis and Jaakkola, 2017; Arraset al., 2016; Biran\\nand Cotton, 2017; Gilpinet al., 2018). Consequently, more recent works\\n(Fenget al., 2020; Chenet al., 2019; Yanget al., 2018; Inoueet al., 2020;\\nJhamtani and Clark, 2020) have focused on this setting. Yanget al.\\n(2018) have also argued that training the model to output reasoning\\nchain can further help in training to predict the correct answer as it\\nserves as a useful auxiliary task. Tuet al.(2020) also find that using\\nthe reasoning chain as a supervision signal during training improves the\\nperformance on adversarial examples as well.\\nThe remainder of this book is structured as follows: Chapter 2 aims\\nto formalize the task of MHQA in a way that encompasses most existing\\nvariants. Chapter 3 describes existing MHQA datasets, their creation\\ntechniques, critiques and challenges1. Chapter 4 discusses traditional\\npre-LLM models in-depth in a structured way that leads to a taxonomy\\nfor existing methods in Chapter 6. Chapter 5 is dedicated to recent LLM\\nbased methods for MHQA, challenges of incorporating LLMs and their\\nproposed solutions. Chapter 7 discusses the standard evaluation metrics\\nalong with evaluation methods specifically designed for evaluating multi-\\nstep reasoning/retrieval. Chapter 8 touches upon the multi-hop question\\ngeneration problem. Chapter 9 then summarizes the insights of the book\\nand critiques of the existing methods and datasets, to propose promising\\ndirections for future research in MHQA.\\n1We discuss the datasets before methods as doing so provides on overview of the\\nexisting variants of the tasks which would be helpful to understand the intuition\\nbehind the proposed architectures.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 13}, page_content='2\\nFormulating the Multi-Hop Question Answering\\nTask\\nBefore diving into the advancements, we try to provide a formal and\\ndescriptive definition of the task. Although many attempts at formally\\ndefining the task have been made, we observe that they tend to focus\\non specific cases of the broader task. However, we aim to cover a large\\nvariety of tasks that can be considered as variants of the MHQA task.\\nTherefore, we try to propose a broader definition that encompasses\\nmany variants of the task that have been tackled. By doing so, we also\\naim to clearly define the scope of what concerns Multi-Hop Question\\nAnswering for the rest of this book.\\nFormally defining the task of multi-hop question answering is not\\nstraightforward, since the definition of ahop is ambiguous in itself.\\nFor instance, in the context of open-domain QA on text documents,\\na hop could signify reasoning across different documents (Yanget al.,\\n2018) whereas for QA over long documents, reasoning across different\\nsections or paragraphs is a hop (Sunet al., 2021). To the best of our\\nknowledge, existing works do not provide a general definition of the\\ntask that encompasses its different variants. We argue that in order to\\nsystematically tackle the problem, and to obtain a good understanding\\nof the progress in MHQA, it is crucial to first have a general definition.\\n10'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 14}, page_content='11\\nWe attribute the generality of the MHQA by keeping the notion of\\na context abstract. Depending on the task, a context can be any single\\nindependent piece of information: a sentence, a document, an image\\nor an entity in a knowledge graph. Keeping this in mind, we formally\\ndefine the task of multi-hop question answering as:\\nLet C denote the set of all contexts,Sdenote the set of all questions\\nand Adenote the set of all possible answers. Given a questionq∈S and\\na set of related contexts,C ⊆C, the task is to approximate a function\\nf : S× Cn ↦→A∪{Φ}, that satisfies:\\nf(q,C) =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\na∈A ∃ Pq = {p1,··· ,pk}⊆ C,k> 1\\n& Pq |= (a answers q)\\nΦ otherwise\\n(2.1)\\nwhere |= represents entailment, andΦ is the output whenqis unanswer-\\nable usingC1. Given a questionq and a set of contextsC, f returns an\\nanswer athat answersq by using a subset of ‘gold’ supporting contexts\\nP from C. The number of gold supporting contextsk is restricted to be\\nmore than 1 to ensure that the question is not solvable using a single\\nhop (k= 1 reduces the task to traditional QA).\\nThis definition captures the commonly adopted breakdown of the\\ntask to two sub-problems: Information Retrieval (IR) and Reading\\nComprehension (RC). Typically,f can be decomposed into (IR)g :\\nS× Cn ↦→Ck and (RC)h: S× Ck ↦→A∪{Φ}, for somek∈N, k> 1\\nsuch that:\\ng(q,C) = Pq (2.2)\\nwhere Pq ⊆C is the set of contexts relevant toq.\\nh(q,Pq) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\na P q |= (a answers q)\\nΦ otherwise\\nand (2.3)\\nf(q,C) = h(q,g(q,C)) (2.4)\\n1In case of multiple correct answers toq, Eq. 2.1 allowsf to output any a\\nthat answersq, which is the case for datasets like SQuAD (Rajpurkaret al., 2016a;\\nRajpurkar et al., 2016b). However, some applications may requiref to output all\\ncorrect answers (Minet al., 2020) when multiple correct answers exist.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 15}, page_content='12 Formulating the Multi-Hop Question Answering Task\\nReasoning chain:A reasoning chain for a questionP′\\nq = {p′\\nq,i}k\\ni=1\\nis defined as an ordered permutation of the setPq defined above, such\\nthat:\\n∀j, 1 ≤j <k, p′\\nq,j − →p′\\nq,j+1 represents thejth reasoning step and\\np′\\nq,k − →a is thekth reasoning step.\\nIt is important to note that the granularity of what constitutes a\\nreasoning chain can also be smaller than that of the contexts. For\\nexample, when the granularity of a context is passage, the reasoning\\nchain may consist of particular sentences or particular entities belonging\\nto those passages.\\nHop: Each reasoning step of the reasoning chain can be termed\\nas a hop. Furthermore, some commonsense knowledge might addition-\\nally be required to perform a reasoning step from one context (i.e.,\\na document, table, etc.) to another. In that case, the commonsense\\nreasoning can also be considered as a hop. The definition provided here\\ndoes not consider reasoning hops over external/commonsense knowledge\\nalthough it can be accommodated by allowingPq ⊆C∪Q, whereQ is\\nthe external/commonsense knowledge base. However, we omit this for\\nsimplicity.\\nAs mentioned in the introduction, many recent works focus on\\nexplainable MHQAto ensure the accountability and interpretability\\nof the models while answering multi-hop questions. Formally, explainable\\nMHQA is the setting of MHQA that requiresf to output the reasoning\\nchain P′\\nq (as an explanation for the answer) along with the answer,\\na. The set of ‘facts’ in the reasoning chain are often referred to as\\nsupporting facts (Yanget al., 2018).\\nThe given definition is generic and can be extended to accommodate\\nmultiple variations of MHQA, some of which are listed below. Note that\\nthe given list is not exhaustive and the proposed definition of MHQA\\nmay be extended with new variations.\\n• MHQA via fact composition:Ci represents independent facts.\\n• MHQA over long documents:QA over long documents can be\\nconsidered multi-hop if the question requires the model to aggre-\\ngate information across different sections, passages or sentences'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 16}, page_content='13\\nof the same document (Khashabiet al., 2018; Sunet al., 2021).\\nHere, eachCi is a section/passage in the same document.\\n• MHQA over multiple text documents:Each Ci is an inde-\\npendent document.\\n• Multiple choice MHQA:For each question, there is a small\\nset of possible answers given beforehand. Thus,Ain Eq. 2.1 is\\ndependent onq, A= A(q).\\n• Open Domain vs Closed Domain MHQA: In the open\\ndomain setting, the set of contexts relevant to the question,C\\nspans the entire corpus i.e.,C = C whereas in the closed domain\\nsetting, C, the input to the model along with the questionq,\\nis a small subset ofC and may be different for each question\\ni.e., C = Cq ⊂C. The closed domain setting guarantees thatCq\\nis sufficient to answerq and might also contain noisy irrelevant\\npassages. It is important to note that the distinction between\\nopen-domain and closed-domain is regardless of the sizes ofC,C,\\nbut is determined by the input of the task - whether each question\\nis provided with a specific subset of sufficient contexts or not. As\\nwe will see in Chapter 4, the open-domain setting can be reduced\\nto closed-domain by performing a preliminary retrieval overC.\\n• MHQA over Knowledge Bases/Knowledge Graphs:2 C is\\na Knowledge Base (KB) or a Knowledge Graph (KG) withCi\\nrepresenting a triplet or a graph node.\\n• Visual MHQA:3 C is a set of images and/or videos (or equiva-\\nlently, sequence of images).\\n• Conversational QA with clarifying questions:Conversa-\\ntional QA can be regarded as an MHQA problem if answering\\nthe question requires the model to ask follow-up questions to the\\n2For the scope of this book, we do not consider QA over KB or KG since these\\nhave been extensively covered by Fuet al.(2020), Lanet al.(2021), and Roy and\\nAnand (2021).\\n3For the scope of this book, we only consider text based QA.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 17}, page_content='14 Formulating the Multi-Hop Question Answering Task\\nuser. In this task, the user asks a question and the model tries\\nto answer it based on some context. If the model cannot find\\nthe information necessary to answer the question, it generates a\\nfollow-up question for the user and enquires the missing informa-\\ntion. Here, each follow-up question can be considered as a retrieval\\nhop and figuring out the missing information can be regarded as\\na reasoning hop (Sunet al., 2021).\\n• Temporal MHQA:Ci represents here diverse temporal contexts\\nwhich could be defined in several different ways. These could be\\nsimply documents published over different time points. However,\\non a more general level, contexts could be in the form of relevant\\ntime periods. Such time periods could mark bursts in the temporal\\ndistribution of relevant documents returned for an input question,\\nor they could be formed by the focus time (Jatowtet al., 2013)\\nof relevant documents estimated either based on the embedded\\ntemporal expressions in the documents (Wanget al., 2021a; Wang\\net al., 2020), or inferred from past events and entities mentioned\\nin text (Wanget al., 2021c).\\nWe conclude this chapter by acknowledging some of the limitations\\nof the proposed definition. Firstly, there may be multiple ways of\\nmathematically denoting the same task. We do not claim the presented\\nnotation to be better than other possible notations in any way. We\\nuse the definition and notation that works best for us and allows us to\\nclearly distinguish the scope of this book. We also prefer this notation\\nas it allows us to naturally derive the commonly used concepts in\\nMHQA such as reasoning chains, contexts and hops. It also allows us to\\ncapture the two-step process of retrieval and reasoning that is commonly\\nadopted in question answering. We welcome the community to progress\\ntowards a more accurate and widely acceptable definition.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 18}, page_content='3\\nA Comprehensive Study of Datasets: Analysis and\\nGuidelines\\nThe previous chapter highlighted the different forms of the task which\\nimplies the existence of multiple datasets that are unique and equally\\nsignificant, and are suited to different variants of the problem. In this\\nchapter we aim to briefly summarize existing datasets and provide their\\ncomparison. This chapter details on the process of dataset creation,\\nstatistics, comparison among the existing datasets, and is followed by\\nour critique of these datasets. We start the discussion with existing\\ndatasets as this will provide necessary context and intuition of the data\\nfor the following chapters devoted to methods which operate on top of\\nthese datasets.\\nWe split our discussion on datasets into three broad sections. In\\nthe first section, we dive into details of how the datasets are created,\\nfollowed by a section devoted to some statistics and comparisons among\\nthe existing datasets. We end the discussion with a section explaining\\nsome critiques and observed shortcomings of the current datasets.\\n3.1 Dataset Creation\\nWe begin by describing how MHQA datasets are generated. In this\\nsection, we include major challenges for creating these datasets, followed\\n15'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 19}, page_content='16 A Comprehensive Study of Datasets: Analysis and Guidelines\\nby different steps involved in the process.\\n3.1.1 Challenges\\nCreating a dataset for multi-hop question answering is more challenging\\nthan for a traditional (i.e., single hop) QA setup. The major challenges\\ninclude:\\n• Contexts used for questions should form a valid and unambiguous\\nreasoning chain. This implies that a context used for a particular\\nquestion should have some information overlap or some kind of\\nentailment relation with at least one of the other contexts present\\nin the expected reasoning chain (Yanget al., 2018).\\n• Since the questions should require reasoning over multiple con-\\ntexts, the process of question generation itself needs to somehow\\nencapsulate information across those particular contexts.\\n• The creation process needs to ensure that the question is not\\nanswerable by using any single context. For instance, the question\\n“Who was the president of United States in the year in which World\\nWar II began?” requires two contexts containing “World War II\\nbegan in 1939.” and “Franklin D. Roosevelt was the president of\\nUnited States in 1939.” However, there may be a separate context\\ncontaining “Franklin D. Roosevelt, president of United States at\\nthe start of World War II, was unwilling to...”. This challenge is\\nmore prevalent in the setting of Open Domain QA because of the\\nincomplete prior knowledge about the possible contexts.\\nTherefore, it becomes crucial to look at the methods adopted during\\ncreation of existing datasets, along with their advantages and drawbacks\\nand possible ways to mitigate these. In general, the dataset creation\\ntask can involve three major steps, which we discuss next.\\n1. Generating reasoning chains\\n2. Question generation by crowd-sourcing\\n3. Automatic/manual filtering'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 20}, page_content='3.1. DATASET CREATION 17\\n3.1.2 Reasoning Chain Candidates\\nMethods for coming up with reasoning chains are highly specific to the\\ntask and domain and thus, differ significantly for each dataset. Thus,\\nwe look at a few datasets individually.\\n1. HotpotQA (Yanget al., 2018):HotpotQA contains only 2-hop\\nquestions formed using the first passages of documents from the\\nEnglish Wikipedia dump1. Two passages are chosen as a reasoning\\nchain (termedcandidate passage pair) if they satisfy either of the\\ntwo conditions:\\n• There exists a hyperlink from the first document to the\\nsecond. The entity which forms the hyperlink is termed as\\nthe bridge entity and the questions are termed asbridge\\nquestions.\\n• The entities for those passages belong to the same category\\n(e.g. Michael Jordan and Kobe Bryant). These are specifically\\nsampled from 42 manually created lists. Such pairs are used\\nfor creatingcomparison questions.\\n2. MultiHop-RAG (Joshiet al., 2023a):MultiHop-RAG uses\\na set of diverse news articles and prompts GPT-4 to extract\\nfactual sentences from each article. These articles are again passed\\nto GPT-4 for paraphrasing the factual sentence into a natural\\nlanguage claim. A topic and an entity are also generated in this\\nstep which are used as bridge entities for the reasoning chains.\\n3. HybridQA (Chenet al., 2020b):In HybridQA, the definition\\nof a context can be either a passage or a table. It is argued that\\nusing a table as context avoids the ambiguity of the questions\\nthat could arise when using texts. To ensure at least two hops,\\nthe questions are restricted to have reasoning chains containing\\nat least one table and one text document. The tables are filtered\\nfrom the set of tables released in WikiTables (Bhagavatulaet al.,\\n2013), and the Wikipedia hyperlinks present in the cells of the\\n1https://en.wikipedia.org/'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 21}, page_content='18 A Comprehensive Study of Datasets: Analysis and Guidelines\\ntable are used to retrieve relevant passages. At most 12 sentences\\nof the first passage of each Wikipedia page are treated as the\\npassages.\\n4. NarrativeQA (Kočisk`y et al., 2018):The dataset is for an-\\nswering multi-hop questions on long stories. To ensure that the\\nquestions are indeed multi-hop and answering requires non local-\\nized reasoning, the annotators are asked to form questions using\\nhuman-generated summaries of the stories. The stories are col-\\nlected from books from Project Gutenberg2, while movie scripts\\nare scraped from the web.\\n5. OpenBookQA (Mihaylov et al., 2018): OpenBookQA re-\\nquires question answering on the scientific domain using abook\\n(in simpler words, a collection) of scientific facts along-with a\\nbroad common knowledge(large open-domain scientific sentences).\\nThe book and common knowledgeare the two contexts required\\nto answer the questions in OpenBookQA.Book is collected by\\nfiltering a subset of the WorldTree corpus identified by Jansen\\net al. (2018) where thecommon knowledge is collected from a\\ncollection of 14M scientific facts across Wikipedia, ConceptNet\\nand other scientific corpora.\\n6. QASC (Chenet al., 2020b):QASC is also a dataset for two-\\nhop QA using scientific facts. The process of generating reasoning\\nchains is very similar to that of OpenBookQA. The two contexts\\nof the reasoning chain are chosen from a set of good quality seed\\nfacts and a large corpus of auxiliary facts, respectively.\\n7. MultiRC (Khashabiet al., 2018):The purpose of this dataset\\nis to create multi-domain multi-hop questions. Documents across\\nvarious domains are selected from multiple corpora. Here, the\\nmultiple contexts are part of the same passage and the task of\\ncandidate reasoning chain generation is left to the annotators;\\neach document is given to the annotators and they are asked to\\nform valid multi-hop reasoning questions.\\n2https://www.gutenberg.org/'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 22}, page_content='3.1. DATASET CREATION 19\\n3.1.3 Generating Questions\\nGenerating multi-hop QA pairs requires accumulation of information\\nacross different contexts which itself is an unsolved problem (Minet al.,\\n2019b; Jansen, 2018; Khotet al., 2020; Kumaret al., 2019; Suet al.,\\n2020; Guptaet al., 2020; Sachanet al., 2020; Panet al., 2021; Yuet al.,\\n2020).\\nAutomatic generation: Welbl et al. (2018) automatically generate\\nquestions using existing KBs where WikiData and DrugBank (Wishart\\net al., 2008) are used as the knowledge bases, and Wikipedia and\\nMedline3 are used as the document corpus. However, the question and\\nanswer types in the two datasets are highly constrained, owing to the\\ncreation technique. For instance, the only question type in the MedHop\\ndataset is of the kind(drug1,interacts_with,?), where in the WikiHop\\ndataset we have(item1,property, ?). Therefore, automatic generation\\nusing KBs is argued to result in datasets limited by the incompleteness\\nof entity relations and schema of the KB used (Zhanget al., 2020; Yang\\net al., 2018). Furthermore, automating the generation of free-form text\\nquestions is known to be a challenging task and requires substantial\\ntraining data (Gatt and Krahmer, 2018; Novikovaet al., 2017; Min\\net al., 2019b). However, with the latest research in LLMs, automatic\\nquestion generation followed by semi-automatic verification is commonly\\nadopted for creating small scale datasets (Joshiet al., 2023a).\\nCrowd sourcing: Consequently, traditional datasets propose that the\\nquestion creation step should be done using human-intelligence. Since\\nthe task is not very straight-forward even for humans, existing works\\nhave added comprehensive guidelines for creating questions. These\\nannotation guidelines follow a common pattern, with slight nuances,\\nirrespective of the task. We aim to summarize the annotation guidelines\\nused by various datasets and hope that this could serve as a reference\\nfor the relevant future endeavours. Based on the techniques adopted\\nby the existing datasets, the common annotation instructions and best-\\npractices include:\\n3https://www.nlm.nih.gov/medline/medline_overview.html'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 23}, page_content='20 A Comprehensive Study of Datasets: Analysis and Guidelines\\n1. To ensure that the questionsrequire multi-hop reasoning:\\n• Give examples of both positive and negative instances of\\nwhat the task requires.\\n• Break down the question generation task into simpler steps\\nto prevent mistakes.\\n• Use rule-based techniques to provide in-the-loop friendly\\nhints and prevent the annotators from submitting trivial\\nincorrect samples.\\n• Use simple single-step IR or PLM (Pre-trained Language\\nModels) based techniques to check if the question submitted\\nis answerable using a single context only.\\n• Ask annotators to submit the reasoning chain along with the\\nquestion and answer.\\n• Ask a different set of annotators to answer the questions\\nusing only a single context.\\n• Ask a different set of annotators if the question requires\\nmultiple contexts to be answered.\\n2. To ensure that the questionsare answerable:\\n• Ask the annotator to also provide the answer to the question.\\n• Prohibit the use of negation words in answer choices that\\ncan trivially fool baselines.\\n• Ask for questions with very specific answers.\\n• Ask a different set of annotators to answer the question and\\ndiscard questions with significant error rates.\\n3. To ensure that the questions and answersare formed well:\\n• Restrict the annotators to experts/native speakers of the\\nintended language of the dataset.\\n• Ask a different set of annotators to verify if the questions\\nare grammatically well-formed.\\n• Place a limit on the length of the answer.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 24}, page_content='3.1. DATASET CREATION 21\\n• For multiple choice questions, randomly shuffling answer\\nchoices can avoid annotator biases, such as choice A being\\nthe correct answer more often than choice D.\\n4. To ensure that the questionsare challenging to answer:\\n• Prohibit the annotator from copying spans of text from the\\ninput contexts.\\n• Ask the annotators to make the questions challenging.\\n• Ask the annotators to create confusing and irrelevant incor-\\nrect answer choices.\\n• Ask the annotators to consider high-level relations between\\nentities and events rather than localized relations.\\nHowever, crowd-sourced datasets also have severe limitations. Dua\\net al.(2021) argue that these datasets usually only present a partial\\npicture of the underlying data distribution and are marred by numerous\\nbiases, such as annotator bias (Gevaet al., 2019a), label bias (Dua\\net al., 2020; Gururanganet al., 2018), survivorship bias (Minet al.,\\n2019a; Tuet al., 2020), and ascertainment bias (Jia and Liang, 2017).\\nFurthermore, the absence of adversarial contexts during training time\\nallows the models to learn shortcuts in reasoning and perform the task\\nwithout doing multi-hop reasoning.\\nPost-processing: The above-mentioned practices and instructions\\nmight not preclude all human errors. Thus, a further manual or rule-\\nbased automatic screening is generally done to get rid of these errors.\\nFor instance, Chenet al.(2020b) remove a question 1) if the answer\\ncannot be found from either table or passage, 2) if the answer is longer\\nthan 20 tokens, or 3) if the answer passage is easily retrieved using\\na single hop of TF-IDF retrieval. Mihaylovet al. (2018) and Khot\\net al.(2020) remove questions that the annotators are unable to answer\\ncorrectly. Khashabiet al.(2018) remove a question if the annotators\\nare able to answer it using only a single context. Welblet al.(2018)\\naim to resolve the candidate frequency imbalance by sub-sampling the\\ndataset to ensure that questions having any particular answer candidate'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 25}, page_content='22 A Comprehensive Study of Datasets: Analysis and Guidelines\\nconstitute not more than 0.1% of the dataset, and also by omitting\\narticles that are about the United States. They tackle the issue of\\ndocument-answer correlation by discarding the questions having any\\ncommonly occurring pair of a document and candidate answer. Finally,\\nJoshi et al. (2023a) use GPT-4 to evaluate examples in the dataset\\nagainst the following criteria: 1) The generated query must utilize all\\nprovided evidence in formulating the response; 2) The query should be\\nanswerable solely based on the provided evidence; 3) The response to\\nthe generated query should be either a single word or a specific entity;\\n4) The query must conform to its designated query type.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 26}, page_content='3.1. DATASET CREATION 23\\nTable 3.1: Comparison of MHQA datasets.C represents the set of all contexts present in the dataset whereasC represents\\nthe set of contexts for a single question. In OD (Open Domain setting),C = C.\\nContext\\ngranularity |C| Size #hops | C | Question Source Context\\nsource Domain Ans. type\\nHotpotQA Passage |Wikipedia| 112,779 1/2/3 10 / ODa Wikipedia Wikipedia Generic Span\\nHybridQA Table, Tables: 13k 69611 2/3 1 table Wikitables, Wikitables, Generic SpanPassage Passages: 293k passages Wikipedia Wikipedia\\nNarrativeQA Sentence Books: 783\\nMovies: 789 46765 - 1 story Multipleb Multiple Fiction Generative\\nMultiRC Sentence 871 9872 2.37 1 passage Multiplec Multiple Generic MCQ\\n|A|: 5.44\\nMedhop Passage Medline 2508 - OD Drugbank Medline Medicine MCQ\\n|A|: 8.9\\nWikihop Passage |Wikipedia| 51318 - OD Wikidata Wikipedia Generic MCQ\\n|A|:19.8\\nQASC Sentence Core: 928\\nAux: 7672 9980 2 OD Core: WorldTree\\nAux: (Beckeret al., 2020) WorldTree Science MCQ\\n|A|: 8\\nOpenBookQA Sentence Core: 1326\\nAux: 6000 5947 2 OD WorldTree WorldTree Science MCQ\\n|A|: 4\\na10 for distractor setting and OD for Full-wiki setting\\nb Summary: Wikipedia; Stories: Gutenberg; Movies: http://www.imsdb.com/, http:\\n//www.dailyscript.com/, http://www.awesomefilm.com/\\nc News: CNN, WSJ, NYT; Wikipedia; Articles on society, law and justice; Articles\\non history and anthropology; Elementary school science books; Stories: Gutenberg\\nproject, MCTest; Movies: CMU movie summary corpus'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 27}, page_content='24 A Comprehensive Study of Datasets: Analysis and Guidelines\\n3.2 Existing Datasets: Statistics, Comparisons and Examples\\nA comparison of the popular datasets used for MHQA is shown in\\nTable 3.1. Some of these datasets have multiple settings, and different\\nstatistics for different settings. We describe these settings along with\\nsome additional details for three such datasets below:\\n1. HotpotQA test set has two settings i)Full-wiki setting which\\nis open domain with the set of contexts being the first passages\\nof all Wikipedia pages, and ii)Distractor settingwhich is closed\\ndomain with a set of 10 passages (2 gold + 8 distractors) provided\\nwith each question. The 8 distractors are collected by using a\\nTF-IDF retriever with the question as the query. The dataset also\\nevaluates an auxiliary task of predictingsupporting facts. These\\nare the sentences in the gold passages which were used by the\\nannotator to create the question. Since many of thecomparison\\nquestions in the dataset are of yes/no type, many models also use\\nthe answer type prediction as an auxiliary task. This involves a\\n3-way classification of the answer being ’yes’/’no’/the extracted\\nspan.\\nThe train set of HotpotQA is split into easy/medium/hard. The\\neasy subset predominantly consists of single-hop answerable ques-\\ntions (over 70%) whereas distinction between medium and hard\\nquestions is determined by training multiple baselines and testing\\nthe answerability of the questions. Although the dev and test\\nset contain only the hard questions, authors show that the easy\\nquestions are also useful while training the model.\\n2. QAngaroodatasets(MedHopandWikiHop) containamasked\\nversion along with the original unmasked dataset for avoiding the\\ncandidate frequency imbalance. For instance, in the MedHop\\ndataset, some drugs (such as Aspirin) interact with more drugs\\nthan others (such as Isotretinoin), which can lead to candidate\\nfrequency imbalance. To mitigate this, any candidate expression is\\nrandomly replaced by a unique placeholder token (e.g. “Mumbai\\nis the most populous city in <MASK7>”). It is argued that doing\\nthis removes the answer frequency cues and also removes statis-'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 28}, page_content='3.3. CRITIQUES AND CHALLENGES 25\\ntical correlations between frequent answer strings and relevant\\ncontexts.\\n3. NarrativeQA question-answer pairs are created using the sum-\\nmaries of movies or books whereas the model is asked to answer\\nthe question based on the original story (referred to as thestory\\nversion). Another somewhat less challenging task of answering\\nquestions by directly using the summaries, referred to as the\\nsummary versionis also provided.\\n3.3 Critiques and Challenges\\nEven with carefully designed datasets it remains to be doubtful whether\\nthe datasets in fact require the model to perform multi-step reasoning\\nto conclude the answer. Minet al. (2019a) show that the questions\\nformed by compositions of two contexts (as used by many datasets)\\nare not the same as they do not generalize well to multi-hop questions\\ngenerated in typical use cases. Consequently, there have been multiple\\ninsightful methodologies proposed to test this aspect.\\nChen and Durrett (2019) design two baseline models to predict\\nthe sentence containing the answer and constrain them to score each\\nsentence from each context independently without looking at other\\nsentences. Thus, the models should ideally perform poorly when trying\\nto answer multi-hop questions. However, the performance of these\\nbaselines is overwhelmingly closer to the state-of-the-art than to a\\nrandom classifier. This is observed more so for multiple choice MHQA\\ndatasets (WikiHop and a multi-choice modification of HotpotQA) than\\nfor a span-based MHQA dataset (HotpotQA). Furthermore, a no-context\\nbaseline that predicts the answer only by looking at answer candidates\\nperforms almost as good as the baselines defined above even when the\\nnumber of candidates is increased significantly (refer to Figure 3 in Chen\\nand Durrett (2019)). Thus, it is argued that multiple choice questions\\nare easier to hack while testing and less helpful while training when\\ncompared to the span based questions.\\nMin et al.(2019a) train a single-paragraph BERT model (Kenton\\nand Toutanova, 2019) that achieves comparable performance to the'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 29}, page_content='26 A Comprehensive Study of Datasets: Analysis and Guidelines\\nstate-of-the-art on the distractor setting, while it lags behind on the\\nopen-domain setting indicating that the open-domain setting is more\\nchallenging for a single-hop model and is worthy of future study. To\\nfurther verify the hypothesis that the distractor setting is majorly\\nsingle-hop solvable, human evaluation is performed. Humans achieve\\nan accuracy of87.37 F1 when using all the ten input paragraphs and\\nthat of82.06 when one of the gold paragraphs is missing. In order to\\nimprove the quality of these distractors, an adversarial set of distractors\\nis collected by training a single-hop BERT model and picking the top\\nscoring incorrect paragraphs. Although, the performance of the single-\\nhop model drops significantly, it is recovered after fine-tuning the model\\non these distractors.\\nAnother possible way to improve the quality of distractors is to add\\na large number of such distractors. However, it is observed that even\\nwith 500 distractors, the single hop model performance is significant\\n(Min et al., 2019a). For the open domain setting, the performance goes\\ndown significantly indicating the challenging nature of open-domain\\nMHQA.\\nOn manual analysis of single-hop solvable questions, it is found that35%\\nof the questions in the HotpotQA dataset are solvable only by matching\\nthe entity type. These questions would require multi-hop reasoning\\nin the open-domain setting. However, other26% of the questions are\\nsingle-hop solvable even in the open-domain setting. These are questions\\nwhose answers can be derived by finding an entity that uniquely satisfies\\ntwo properties (intersection-type questions) but a unique answer can\\nalso be found by using only one of these properties. Another8% of the\\nquestions are non-compositional single-hop and only the remaining27%\\nrequire multi-hop reasoning.\\nSimilarly, Minet al.(2019b) split the HotpotQA dev set into single-\\nhop solvable (3426) and single-hop non-solvable (3979) questions.\\nDas et al.(2019) make a similar conclusion by observing that 1184\\n(20%) of the questions have the answer span mentioned in both of the\\nsupporting passages and the answer can be extracted by only considering\\none of them.\\nXiong et al. (2019) conduct an experiment that compares a QA\\nmodel that has access to all the supporting passages in HotpotQA,'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 30}, page_content='3.3. CRITIQUES AND CHALLENGES 27\\nto another model that has access only to the answer (or the second\\nhop) passage and observe that the model with full access only gives a\\nmarginal improvement from 49.43 EM to 50.96 EM.\\nTrivediet al.(2019) find that more than 60% of the dev questions\\nin MultiRC have at least one adjacent relevant sentence pair making\\nthe multi-hop reasoning very easy.\\nIn summary, there seems to be enough evidence that the existing\\ndatasets have some flaws and a model should be able to exploit these\\nflaws and show an impressive performance without being actually good\\nat multi-hop reasoning. At the same time, these experiments point out\\nthe potential issues and challenges relating to MHQA datasets, and\\nmotivate development of higher quality datasets.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 31}, page_content='4\\nExisting Approaches for MHQA\\nIn this chapter, we will be taking a look at the existing approaches used\\nfor solving multi-hop question answering. Since there is a very large\\nnumber of works we aim to cover, we try to categorize them on many\\nlevels into categories and sub-categories. We hope this leads to a more\\nstructured study culminating into a taxonomy proposed in the Chapter\\n6. For each level of categorization, we briefly describe the category/sub-\\ncategory followed by a deep dive into some of the representative methods\\nfor the category and finally, contrast the sub-categories and list some\\nknown pros and cons for each. To avoid over technicality, the readers\\ncan skip the deep dive into particular methods.\\nAs discussed in Chapter 2, a large number of works divide the task\\nof MHQA into two steps, a retrieval (IR) step that extracts all the\\nrelevant contexts from the corpus and a reading comprehension (MRC)\\nstep that reads the resulting contexts to find the answer. In general, the\\nexisting works have three basic units - Retriever, Reasoner (or Reader)\\nand Answer Predictor1. These units are often employed iteratively to be\\nable to perform multiple hops. A coarse level classification of methods\\n1Although initial works have a single module for reasoning as well as answer\\nprediction, more recent works recommend further segregating these two as different\\nmodules. Therefore, we consider these as separate modules in our discussion.\\n28'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 32}, page_content='29\\ncan be how these units interact with each other. One way is to complete\\nall the iterations of one unit before moving to the next one, and the\\nother is to perform two or three of the tasks in a single iteration and\\nrepeat for the corresponding hops. Thus, there are four possibilities that\\narise by considering the multi-step nature of the retrieval and reasoning\\nmodules. These four types of models are shown in Figure 4.1.\\nInformation \\nRetrieval\\nModule\\nReasoning\\nModule\\nAnswering \\nModule\\nInformation \\nRetrieval\\nModule\\nReasoning\\nModule\\nAnswering \\nModule\\n(a) Type-I:RetrItr ->ReasItr ->Ans (b) Type-II:(Retr->Reas)Itr ->Ans\\nInformation \\nRetrieval\\nModule\\nReasoning\\nModule\\nAnswering \\nModule\\nInformation \\nRetrieval\\nModule\\nReasoning\\nModule\\nAnswering \\nModule\\n(c) Type-III:RetrItr ->(Reas->Ans)Itr (d) Type-IV:(Retr->Reas->Ans)Itr\\nFigure 4.1: The four types of architectures used for MHQA.A self loop\\nat a module indicates that that module is independent of the succeeding modules\\nwhereas an incoming connection from a succeeding module represents some kind of\\nfeedback from that block. a) The IR and reasoning modules perform multiple hops\\nindependent of each other as well as of the answering module. b) At each hop, the\\nIR module sends its output to the reasoning module which then gives feedback to\\nthe IR module. Answering module then predicts the answer in a single step. c) IR\\nmodule first iteratively retrieves all the relevant documents for all the hops. The\\nreasoning module performs a hop and sends the output to the answering module.\\nAnswering module either answers the question or sends feedback to the reasoning\\nmodule asking for another hop of reasoning. d) At each hop, the IR module sends\\nits output to the reasoning module which further sends its output to the answering\\nmodule. The answering module either predicts a final answer or provides feedback to\\nthe IR module indicating that some required information is missing.\\nA few models retrieve important sentences or entities from the con-\\ntexts as an intermediate step for reasoning. Since the granularity of the\\ncontext is sentences for some datasets like MultiRC and OpenBookQA,\\nit can be difficult to determine what constitutes reasoning and what\\nconstitutes retrieving. To avoid such confusion, we refer back to our\\ndefinition of the task in Section 2. If the granularity of the output of a\\nstep is the same as the granularity of the context, we refer to that step'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 33}, page_content='30 Existing Approaches for MHQA\\nas part of the retrieval process. If the granularity is lower, we take it as\\npart of the reasoning. We begin by describing the techniques used for\\nthese three units followed by some auxiliary tasks in practice.\\nOpen \\ndomain \\nInput\\n(Set of all \\ncontexts)\\nOne\\nPass\\nIterative\\nRetrieved \\nContexts \\n/\\nClosed \\ndomain Input\\nRe-ranking\\nModule\\nPreliminary retrieval Final retrieval\\nReading \\nComprehension \\nModule\\nFiltered\\nContexts\\nConcatenation\\nFigure 4.2: Retrieval Module.The process of retrieval can be decomposed into\\ntwo steps: a) Preliminary retrieval which is a quick and high recall step to retrieve\\nall the relevant context from the set of all contexts in the open domain setting. It\\ncan be performed either as a single pass or iteratively. b) Final retrieval which is\\na more thorough step to filter out all the irrelevant contexts. Because of a smaller\\nnumber of input contexts, more complex re-ranking models can be used at this\\nstep. Some models directly pass the concatenation of the input documents to the\\nreasoning module. Note that the closed domain setting of MHQA eliminates the\\nneed for preliminary retrieval.\\n4.1 Retrieval\\nThe retrieval step can be a bottleneck when the set of available contexts\\nfor a question is large and becomes a particularly challenging task in\\ncase of Open Domain QA. Intuitively, multi-hop retrieval is significantly\\nmore challenging than a single-step retrieval since any retrieval error in\\na hop is accumulated with each subsequent hop of retrieval and leads to\\na well known problem of semantic drift (Yadavet al., 2020). Daset al.\\n(2019) verify the hypothesis by running a simple BM25 on ‘easy’ and\\n‘hard’ subsets of HotpotQA (which contain predominantly single-hop\\nand multi-hop questions respectively) to find that the accuracy drops\\nfrom 53.7% on the easy subset to 25.9% on the hard subset. Similarly,\\nFeldman and El-Yaniv (2019) run a TF-IDF retriever and find that'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 34}, page_content='4.1. RETRIEVAL 31\\nalthough it succeeds in retrieving at least one of the gold passages\\namong the top 32 passages for more than 90% questions, it often fails\\nto retrieve both the gold passages.\\nThe existing methods for retrieval can be broadly classified into\\ntwo categories as shown in Figure 4.2: open domain retrieval where the\\nmodel has to search through the entire context setC, and closed domain\\nretrieval where the model is provided with a smaller set of relevant (and\\nnoisy) contextsC ⊂C along with the questionq. Many of the techniques\\nworking in the open domain setting use a two step strategy: a) the first\\nstep (referred in this text aspreliminary retrieval) is a fast and high\\nrecall filtering to get an initial set of relevant contexts. b) the second\\nstep (referred in this text asfinal retrieval) is then reduced to the closed\\ndomain setting where more complex techniques can be used to further\\nremove the noisy contexts. This two step approach combines the best\\nof both worlds, getting the good time efficiency of coarse retrievers and\\nthe good performance of fine-grained retrievers.\\n4.1.1 Preliminary Retrieval\\nThe first step of the open domain retrieval (referred to preliminary\\nretrieval from hereon) can be a single shot retrieval as done in single\\nhop question answering i.e., the model attempts to extract all the\\nrelevant contexts in a single retrieval step. We call this strategysingle-\\npass. Another solution would be to retrieve the passages iteratively,\\nwhere each iteration of retrieval can correspond to each hop in the\\nreasoning. We describe in detail some of these approaches below.\\nSingle-pass approach As mentioned above, the single-pass approach\\nperforms the retrieval in one step. We list some of the representative\\nmethods of this approach below.\\n• Qi et al.(2019) and Chenet al.(2020b) follow a simple setting of\\nthe single-pass retrieval approach, calling the retrieval only once\\nfor each query.\\n• For the multi-choice questions setting, Yadavet al.(2019b) append\\neach candidate answer with the question to get multiple queries'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 35}, page_content='32 Existing Approaches for MHQA\\nand BM25 is used on these queries to extract topn(= 20) contexts\\n(sentences in case of the MultiRC dataset).\\n• For the text and table hybrid setting, Chenet al.(2020b) parse\\nthe passages linked to each cell in the given table and retrieve all\\nthe cells relevant to the question in one go.\\nCritique: This strategy works well for a few-hop case where the\\nnumber of contexts to be retrieved is limited and the candidate answers\\nare provided for the multi-choice scenario. However, for more than two\\nhops, the retrieval or the ranking algorithm could fail to capture the\\nterms relevant to the intermediate hops. For instance, in a three-hop\\nquestion, the information in the second hop could be unrelated to both\\nthe question and the candidate answers. Furthermore, if the candidate\\nanswers are not provided, this strategy could also fail to capture the\\nsecond hop well (Dinget al., 2019). To validate this hypothesis, Ding\\net al.(2019) and Feldman and El-Yaniv (2019) use a simple BM25 with\\nthe original question as the query to evaluate the second hop paragraph\\nrecall on HotpotQA and find the results to be indeed underwhelming.\\nIterative approach Many of the existing techniques, therefore, propose\\na multi-step retrieval. Two main classes of iterative retrieval are query\\nreformulation and entity-linking, described below.\\nQuery reformulation:Deriving a query for the current hop (qt) from\\nthe previous hop query (qt−1) is commonly known as query reformulation\\n(sometimes also referred to as pseudo-relevance feedback). Based on the\\nstrategy used for query reformulation, these methods can be further\\nclassified into two subcategories: text space reformulation and hidden\\nspace reformulation.\\nHidden space query reformulation:In this case, the query is reformu-\\nlated in the embedding space. As a representative example for hidden\\nspace query reformulation, we take a detailed look at Feldman and\\nEl-Yaniv (2019). They first encode the question using a Bi-GRU layer\\non contextualized ELMo (Peterset al., 2018) embeddings and retrieve\\nall the relevant passages using MIPS. To reduce the search space for\\nMIPS, a TF-IDF based retriever (Chenet al., 2017a) is employed to\\nget topni passages. A supervised re-ranker scores the paragraphs and'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 36}, page_content='4.1. RETRIEVAL 33\\neach of the top k paragraphs is used to modify the question hidden\\nrepresentation to give k new search vectors for the next step. The refor-\\nmulation module uses a bi-directional attention (Seoet al., 2016) on the\\nparagraph and question encodings followed by a linear layer with ReLU\\nactivation. A residual connection is added with this output being passed\\nto a Bi-GRU layer followed by another linear layer with ReLU activation.\\nMax pooling is applied to the residual output to give the updated query\\nvector. At every step, top k paragraphs are selected by the re-ranker.\\nThe paragraph encoding being independent of the question allows the\\nencodings to be pre-computed for an efficient retrieval during training\\nand inference.\\nText space query reformulation: Some methods reformulate the\\nquestion by adding to, modifying or re-weighting the text of the ques-\\ntion. Changing the questions in the text space allows the intermediate\\nquestions to be interpretable. We dive a little deeper into some of the\\nmethods following this approach:\\n• Yadavet al.(2020) concatenate the question with each answer\\ncandidate to obtain initial queries. Justification sentences are\\nretrieved using the unsupervised alignment method proposed by\\nYadavet al.(2019a) which uses GloVe embeddings (Pennington\\net al., 2014). They compute a matrix storing the cosine similarity\\nof embeddings of the query tokens with the sentence tokens. Max\\npooling across sentence tokens is applied to get the most similar\\ntokens for each query token. Dot product between this vector\\nand a vector containing the IDF values of the query tokens is\\ncalculated to produce the overall alignment score. For MultiRC,\\nthese are selected from the sentences in all the relevant paragraphs.\\nFor QASC, relevant sentences are retrieved using heuristic IR\\napproaches. The reformulation process only keeps the uncovered\\ntokensandifthenumberofsuchtokensis <T (= 2−4),newtokens\\nare added from the previously retrieved sentences. The process is\\nrepeated until either a) no new query tokens are retrieved or b) all\\ntokens are discovered. 10.7% improvement is observed when the\\ntokens are identified using soft matching over GloVe embeddings.\\n• Zhang et al. (2021) use TF-IDF with the question to get the'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 37}, page_content='34 Existing Approaches for MHQA\\nfirst hop paragraphs. For each subsequent hop, the ALBERT\\nbased reader module is used as a span extractor on the previously\\nretrieved documents to extract the text relevant to the question.\\nThe extracted span is concatenated with the query of the previous\\nstep for performing the next hop of retrieval. This is repeated\\nuntil the reader module finds an answer or a maximum number\\nof hops is reached.\\n• Qi et al.(2019) follow a similar approach to Zhanget al.(2021)\\nwith DrQA’s Document Reader model used as the span extractor.\\nDuring training, heuristics are used to find the oracle query.\\n• Yadav et al. (2021) retrieve k justification sentences using an\\nalignment technique similar to Yadavet al.(2020). The question\\nQ, is concatenated with each retrieved justificationqk, and the\\ntoken weights are assigned as: For each tokentin original question,\\nif qk contains t, weight fort is 1, else it is 2. All terms inqk have\\na weight of 1. This is expected to result a in higher coverage.\\nThe queries are used for second hop retrieval to get a final set of\\nN contexts. Then,\\n(N\\np\\n)\\nevidence chains are generated and each\\nevidence set is ranked by how many query terms are included\\n(coverage) and topn are picked for the supervised reranking.\\n• Malon and Bai (2020) propose generating text based free-form\\nfollow-up questions for performing iterative retrieval. An IR model\\n(BM25) retrieves the set of relevant contexts using the question. A\\nBERT based three-way controller module predicts whether each\\ncontext contains the final answer, contains some intermediate\\ninformation or is irrelevant. A QG model is used for generating\\na follow up question for each passage that contains intermediate\\ninformation. BERT is used for getting the answers from contexts\\ncontaining the final answer. Irrelevant passages are ignored. The\\nQG model by Zhaoet al.(2018a) is trained on the reverse SQuAD.\\nThe controller model is trained with cross-entropy loss for ternary\\nclassification on the ground truth triplets.\\nCritique: Das et al.(2019) argue that query reformulation methods\\ndo not necessarily use the information about entities present in the'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 38}, page_content='4.1. RETRIEVAL 35\\nevidence as they might not be the most frequent/ salient terms in it.\\nTherefore, many works have proposed using entity mentions in the\\nretrieved contexts for performing the second hop retrieval.\\nUsing entity links/hyperlinks: In this approach, entity mentions in\\nthe first hop retrieval are used to find Wikipedia passages with titles\\ncontaining any of these entities. This is an efficient technique since the\\nentities can be extracted as part of the pre-processing step. It can be\\nnoted that this technique is particularly effective for HotpotQA since\\nthe creation process of the HotpotQA uses the Wikipedia paragraph for\\na bridge entity as the second hop context. However, this also indicates a\\nbias of dataset design when building models (Fanget al., 2020). Trying\\nto imitate (or benefit from the knowledge of) the creation process of\\nsome dataset while building the models for this dataset can improve\\naccuracy on that dataset but is not guaranteed to perform well on the\\nother datasets. On similar lines, Daset al.(2019) even suggest not to\\nuse off-the-shelf entity linker as they are usually trained on Wikipedia\\nand can lead to data leakage.\\nBelow, we list some of the representative methods for this class of\\nmethods.\\n• Das et al.(2019) use BM25 for the first hop retrieval and then\\nretrieve Wikipedia paragraphs for each mentioned entity. Since\\nHotpotQA is known to contain both single-hop and multi-hop\\nquestions, a self-link is added for each entity retrieved in the first\\nhop to deal with single hop questions.\\n• Xiong et al.(2019) use a Hybrid TF-IDF + BM25 for the first\\nhop retrieval to get 10 documents. Span prediction and external\\nentity linking is used to get the bridge entities from the first\\nhop passages. A supervised re-ranker gives top 10 entities and\\nWikipedia passages for these entities are used as the second hop\\npassages. A span loss is added for predicting answer passage title\\nentities as an auxiliary task.\\n• Ding et al. (2019) and Fang et al. (2020) retrieve Wikipedia\\npassages whose titles contain an entity mentioned in the question\\nas the first hop passages. Hyperlinks from these passages are used'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 39}, page_content='36 Existing Approaches for MHQA\\nto get the second hop passages.\\n• Shao et al.(2021) extract key words and match them with the\\nparagraphs title and select topN1 paragraph with best TF-IDF\\nscores. Apart from these, topN2 paragraphs with best TF-IDF\\nscoresareadded.Forthesecondhop,addalltheparagraphshaving\\nhyperlinks from and hyperlinks to the first hop paragraphs.\\nCritique: Sidiropoulos et al.(2021a) evaluate the performance of\\nthe two sub-categories of iterative retrieval approaches by evaluating\\nthe performance on each of the two hops on HotpotQA dataset. It is\\nargued that entity based retrieval is limited by the availability of such\\nhyperlinks and passages whereas the reformulation approach is limited\\nby the performance of lexical term-based retrieval. They observe that\\nwhile BM25 followed by BERT based reranking performs well on the\\nfirst hop, it fails on the second hop retrieval (evaluated as the ability\\nto retrieve second passage given the first passage). Similarly, although\\nthe model by Xionget al.(2020) has the best overall performance, it\\nhas some room for improvement when evaluated for the second hop\\nretrieval. Hence, a hybrid technique is proposed where the re-rank model\\nis used for the first hop and a single-hop dense passage retrieval model\\n(Sidiropoulos et al., 2021b) is used for the second hop. Results show\\nthat the hybrid technique outperforms the existing techniques which\\ncalls for further study in this direction.\\n4.1.2 Final Retrieval\\nAfter getting an initial pool of retrieved contexts, many works sug-\\ngest using a more fine-grained and more sophisticated retrieval on the\\nretrieved contexts to get a better quality of contexts. Based on how\\nthe initial pool of retrieval output is processed, we can categorize the\\napproaches into concatenation, supervised re-ranking and unsupervised\\nre-ranking.\\nConcatenating/union Some of the methods do not filter the input\\ncontexts and directly pass a union or concatenation of the contexts to\\nthe reasoning model. For example:'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 40}, page_content='4.1. RETRIEVAL 37\\n• Yadavet al.(2020) maintain N-parallel reasoning chains in the\\npreliminary retrieval step and a union of these chains is passed to\\nthe answer prediction module.\\n• Ding et al. (2019) and Yadavet al. (2019b) do not filter the\\npassages and directly perform reasoning at a lower granularity.\\nSupervised re-rankingMajority of the methods use a supervised\\nmodule to score and rank the input contexts, and then use some crite-\\nrion to filter the less relevant documents before passing to the answer\\nprediction module:\\n• Feldman and El-Yaniv (2019) use a linear layer with sigmoid\\nactivation to get a relevance score of each sentence in a paragraph\\nwith the question. Max pooling across all sentences is applied\\nto give a relevance score and top-k most relevant paragraphs\\nare picked. Max pooling allows only one of the sentences in a\\nparagraph to be relevant to the question with a high score.\\n• Das et al.(2019) use BERT to compute query-aware embeddings\\nfor every pair of the first hop and second hop paragraph. The two\\nparagraphs are concatenated and fed to 2-layer Neural Network\\nto get a score. Topk pairs are passed to the reader module.\\n• Xiong et al.(2019) use a bi-LSTM layer (Hochreiter and Schmid-\\nhuber, 1997a) to predict relevance of each second hop passage\\nwith the question.\\n• Fang et al. (2020) use a RoBERTa encoder (Liuet al., 2019)\\nfollowed by a fine tuning layer to get the top N paragraphs.\\n• Zhang et al.(2021) pass the node (document) representations to\\na binary classifier for predicting their relevance and keep thek\\nhighest scoring documents.\\n• Yadavet al.(2021) use RoBERTa (Liuet al., 2019) for reranking\\ntrained to predict the F-1 score of evidence chain.\\n• Zhang et al. (2020) and Huang and Yang (2021) concatenate\\nparagraphs with the question and feed it to a BERT followed by'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 41}, page_content='38 Existing Approaches for MHQA\\na binary classifier and keep N(=3) paragraphs with the highest\\nscores. Qiu et al. (2019) follows a similar approach but set a\\nthreshold to retrieve a variable number of paragraphs.\\n• Shao et al.(2021) use the gated memory flow network inspired\\nby the Neural Turing Machine (Graveset al., 2014) to model the\\nprobability of a paragraph being the next context in the reasoning\\nchain, conditioned on the question and the previous paragraphs\\nin the reasoning chain. At every time stept BERT is used to\\ncompute the question aware embeddings of the paragraphs,xt. A\\nKVMemNN architecture models the memory as a set of key-value\\npairs. The model passes the key vectors andxt to linear layers and\\napplies Softmax to the output ofWxxt ·Wkki. The output after\\nSoftmax multiplied with another matrixWv is then used as weights\\nwhile summing the value vectorsvi to give the readout vectorot.\\nThe memory reading process is similar to computing self-attention\\n(Vaswaniet al., 2017a) and the authors concatenate the outputs\\nafter computingot for thehattention heads.ot and xt are passed\\nto another linear layer with tanh and sigmoid activation to give the\\nrelevance scorest. xt is written to the memory ifst >gate where\\ngate is a hyper-parameter. Hard negative examples are generated\\nby training a BERT based model to predict the relevance scorest\\nand choosing the top-8 of the non-evidence paragraphs.\\n• Dua et al.(2021) propose generative context selection that learns\\nto predict how the question would have been formed. Mathemati-\\ncally, the model tries to learnp(a,q|C) instead ofp(a|q,C) (as in\\ndiscriminative models). This probability is modeled as\\np(a,q|C) =\\n∑\\ncij\\np(a|q,cij) ·p(q|cij) ·p(cij|C) (4.1)\\nwhere p(cij|C) is theprior that computes compatibility between\\nany two contexts,p(q|cij) is the question generation model that\\npredicts the probability ofq being formed from the given contexts,\\nand p(a|q,cij) is the standard answering model. During infer-\\nence, the model retrieves the contexts asc∗\\nij = argmaxcij p(q|cij) ·\\np(cij|C). To model these probabilities, a pre-trained T5 is used'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 42}, page_content='4.1. RETRIEVAL 39\\nfor obtaining the contextual embeddings. The prior and genera-\\ntive models are trained together. Concatenation of every pair of\\ncontexts is passed to an encoder and a Pointer Generator Network\\n(PGN) (Seeet al., 2017a) decoder is used to predict the question.\\nThe training objective is to increase the likelihood of the question\\nfor gold context pairs and the unlikelihood (Wellecket al., 2019)\\nfor a sample set of negative context pairs. T5 is used for answering\\nusing the best pair of contexts. The advantage of using a genera-\\ntive model is that it avoids the annotator biases in the dataset.\\nThis hypothesis is tested by running the model on the adversarial\\nset of questions formed by Tuet al.(2020). A multi-label sentence\\nclassifier p(s|q,C) that selects relevant sentences is found to have\\na better performance but at the same time, is more biased.\\n• Chen et al. (2020b) feed each cell along with its neighboring\\ncells to the cell encoder to obtain their representations. The\\nrepresentations are aggregated and further fed to a feed-forward\\nneural network to obtain a score.\\n• Sun et al. (2021) perform the retrieval in two steps where the\\nfirst step retrieves a paragraph, and the second step retrieves a\\nsentence from these paragraphs. A weighted sum of paragraph\\nand sentence retrieval scores is computed to find the best retrieved\\nsentence. The retrieval score of the paragraph retrieved from the\\nfirst hop is broadcast to the sentences in the paragraphs. The best\\nretrieved sentence fed to BERT-large for extractive QA.\\nUnsupervised re-rankingSome of the methods for supervised re-\\nranking include:\\n• Yadavet al.(2019b) consider all\\n(n\\nk\\n)\\n(k∈[2,5]) reasoning chains\\nformed by then retrieved passages and use a simple formula to\\ncompute the scores. They defineRelevance as the average BM25\\nscores of the chain,Overlap as the word overlap between each\\nsentence pair in the chain andCoverage as the product of word\\noverlap of sentence with the question and with the answer. The\\nfinal score is given byR·C\\nO . By experiments,k= 3,4 are found to'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 43}, page_content='40 Existing Approaches for MHQA\\nbe the best values with the justification that getting two correct\\nretrieved passages in a chain of size two is tough while a chain of\\nsize 5 will suffer from noise.\\n• Chen et al.(2020b) retrieve cells from the table in the HybridQA\\ndataset in an unsupervised manner. A cell is selected if its value\\nis either mentioned in the question, or is min/max of the corre-\\nsponding column. A cell is also added if it hyperlinks to one of\\nthe passages that are retrieved by a TF-IDF retriever.\\nCritique:Supervised re-ranking can lead to a model that is more suited\\nto the task at hand but requires some training signals which may not be\\navailable for certain tasks. On the other hand, unsupervised methods are\\nmore flexible but might not be optimized for a given task. In particular,\\nYadavet al.(2019b) and Yadavet al.(2020) experimentally verify that\\nusing a supervised re-ranking method has an implied shortcoming of\\nresulting in a domain dependent performance. The performance for\\neach domain depends on the portion of training data belonging to that\\ndomain. Thus, it is attempted to proposeunsupervised re-ranking\\ntechniques that can lead to similar level of quality as supervised re-\\nranking.\\nAnother parallel classification of the re-ranking approaches is based\\non whether the module scores each retrieved context independently or\\ntogether as constituents of a possible reasoning chain. We call the two\\ncategories of methodsContext re-rankingand Chain re-ranking\\nrespectively. Table 6 lists the methods belonging to the two categories.\\nYadavet al.(2019b) show that evaluating the reasoning chain as a whole\\nleads to better performance than evaluating each passage independently.\\n4.2 Reading Comprehension\\nA reading comprehension module is responsible for reading the final set\\nof retrieved contexts, combining the information across contexts and\\nperforming the reasoning steps or hops. The output of this module can\\neither be an answer or some representation of the reasoning performed\\n(for example, a reasoning chain, a semantic graph or some latent repre-\\nsentation). A vast majority of approaches have used graphs or question'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 44}, page_content='4.2. READING COMPREHENSION 41\\ndecomposition to perform the reasoning, therefore we classify the read-\\ning comprehension techniques into graph-based, question decomposition\\nbased and miscellaneous categories.\\n4.2.1 Graph-based Techniques\\nThe general flow of the graph based methods can be summarized in a\\nthree-step process: 1) A graph of one or more types of nodes (entity,\\nsentence, paragraph, document nodes, etc.) is constructed and edges\\nare added based on some lexical heuristics. 2) Contextual encodings of\\nthe nodes are passed to one or more layers of a Graph Neural Network\\n(Kipf and Welling, 2016a) (or a Graph Attention network (Veličković\\net al., 2018) or a Graph Convolutional Network (Kipf and Welling,\\n2016b)). These layers update the representation of each node using the\\nrepresentation of each of its neighbouring nodes. In this way, afternsuch\\nlayers, nodes separated by a path of≤nedges have shared information\\nwith each other. Therefore, each layer is expected to perform one hop\\nof the reasoning process. 3) The updated embeddings are passed to the\\nanswering module.\\nAs can be expected, some methods slightly deviate from the proposed\\ngeneral flow. We describe these differences below along with some details\\non graph building and reasoning steps.\\n• Ding et al. (2019) build an entity graph by starting with the\\nentities extracted during the retrieval process. For each node, a\\nBERT model takes its representation along with the question and\\na retrieved passage and extracts entities from the paragraph to\\nbe added to the graph. The process is repeated until either a)\\nno new nodes can be added or b) a maximum number of nodes\\nare extracted. Sentences containing the extracted entities are\\nconsidered as ‘clues’ for the respective entities. Clues are then\\nused for updating the node representations using a Graph Neural\\nNetwork (GNN). Clues are also used later for the auxiliary task\\nof supporting facts prediction.\\n• Fang et al.(2020) propose constructing a hierarchical graph net-\\nwork with four types of nodes that represent question text, relevant'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 45}, page_content='42 Existing Approaches for MHQA\\nparagraphs, sentences in these paragraphs and entities in these\\nsentences. Bidirectional edges are introduced between the first hop\\nsource sentences and second hop target paragraphs. Edges are also\\nintroduced between paragraph and its sentences; sentence and\\nthe entities it contains; question and the entities it contains; be-\\ntween all paragraphs; between each sentence and its previous and\\nnext sentences. RoBERTa is used for encoding and a bi-attention\\nand a Bi-LSTM layer are used for getting initial contextualized\\nrepresentations of the nodes. Graph Attention Network (GAT)\\n(Veličković et al., 2018) is applied over the hierarchical graph\\nand the updated representations are merged with the original\\ncontextual representations using a gated attention mechanism.\\nThe merged representations are passed to the answer prediction\\nmodule. The proposed gated merging is supposed to be effective\\nfor dealing with smaller number of hops.\\n• Xu et al. (2021) build a use abstract meaning representations\\n(AMR) (Banarescuet al., 2013) to build a semantic graph from a\\npassage. Graphs across passages are connected by adding edges\\nbetweenthesameconceptsindifferentgraphs.Anon-parametrized\\nmethod is employed for generating reasoning chains. A depth first\\nsearch (DFS) is used to find all paths from the question nodes to\\nthe answer nodes and all the edges on the path are used to recover\\nthe facts from the context that form the reasoning chain. The\\nreader reads these reasoning chains and the question to predict\\nthe answer. A reinforcement learning based chain-aware loss is\\nproposed to boost the performance of the system.\\n• Similar to Xuet al. (2021) ll13 and Li and Du (2023) build a\\nsemantic graph. However, the entities and relations are extracted\\nusing GPT-3.5.\\n• Zhang et al.(2021) construct a graph of documents and add edges\\nif the documents have a shared entity. Question aware embeddings\\nof the document are generated by Albert and used as initial node\\nrepresentations. GAT is used to update the representations of each\\nshared entity. To update the representations of non-entity tokens,'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 46}, page_content='4.2. READING COMPREHENSION 43\\nthey are fed to a transformer along with the updated representa-\\ntions for multi-document fusion. The updated representations are\\npassed to the answer prediction module.\\n• Cao et al.(2019) make entity graph of all entity mentions of the\\nanswer candidate in supporting documents. Cross-document edges\\nare added between the mentions about the same entity and within-\\ndocument edges are added between every node pair belonging\\nto the same document. For initial representations, GloVe and\\nELMo are used for token-level and contextualized embeddings\\nrespectively. Contextualized embeddings are crucial since graph\\nnodes only contain entity tokens. To deal with entities containing\\nmultiple words, average is taken over their embeddings. The two\\nembeddings are passed to a 1 layer NN (replaced by Bi-LSTM for\\nencoding the question) for getting the initial node representations\\nwhich are concatenated with the NER and POS embeddings before\\nfeeding to a GCN layer.\\n• Thayaparan et al.(2019) compute similarity between GloVe em-\\nbeddings of each word of the sentence with each word of the query\\nand take the mean among m(=5) closest sentence words to get\\na sentence score and select top k(=25 or 30) sentences. A graph\\ncontaining sentence nodes and document nodes is created with\\nonly the selected sentences. Edges are added between documents\\nif they have a shared entity and between a paragraph and all its\\nsentences. Adding edges among sentences adds complexity without\\nmuch improvement. Initial representation of a node is computed by\\npassing the matrix of GloVe embeddings of the entities contained\\nin the sentence or paragraph to a bi-linear attention (Kimet al.,\\n2018) layer on nodes and query. To compress the representations\\ninto fixed size, self-attention is used. T(=3) layers of Gated Graph\\nNeural Network is used to update the representations.\\n• De Caoet al.(2019) create an entity graph similar to Caoet al.\\n(2019) but have some additional edge types: co-reference edges:\\nbetween co-reference mentions of the same entity (separate type\\nsince these are less reliable) and complement edges which signify'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 47}, page_content='44 Existing Approaches for MHQA\\nno connection between the two nodes. Co-reference edges is a\\nseparate edge type since these are less reliable owing to the error\\nin the co-reference system. These are not required for the masked\\nversion. The query representations are formed by passing ELMo\\n(Peters et al., 2018) to a Bidirectional RNN (Rumelhartet al.,\\n1985). Initial query dependent node representations are formed by\\npassing the ELMo contextual embeddings to a feed forward net-\\nwork. L layers of a gated relational-GCN (Schlichtkrullet al., 2017)\\nR-GCN, a version of GCN are applied to get the final representa-\\ntions which are passed to the prediction module. Relational-GCN\\nis able to accommodate different edge types by using different\\nweight matrices for the neighbouring nodes connected by different\\nedge types. An ablation experiment verifies that, although useful,\\nco-reference edges have the least contribution to performance.\\nAnother experiment tries to predict the edge type by training a\\nmodel but the performance is poor.\\n• Chen et al.(2020b) solve the text-table hybrid MHQA by using\\nthe retrieved cells from the previous stage to then decide which\\nneighboring cell to hop to.\\n• Qiu et al.(2019) construct an entity graph and add edges between\\nentities with the same mention text, between entities belonging to\\nthe same sentence, and between each entity in a paragraph and\\neach entity in its title. Question and contexts are concatenated\\nand passed to a BERT model followed by a bi-attention layer to\\nget the contextual node representations. The following four steps\\nare performed iteratively: 1) Mean-max pooling over tokens in\\nan entity mention to update the entity embeddings. 2) Attention\\nbetween query and entity is used to compute the mask weights\\nwhich are multiplied to the entity representations before feeding\\nto a GAT. 3) Updated entity representations are concatenated to\\nentity tokens and representations of all tokens are updated using\\nan LSTM layer. 4) Bi-directional attention between query and\\nentity representations is used to update the query representation.\\nUpdating embeddings of each token is necessary since the answer\\nmight not be an entity.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 48}, page_content='4.2. READING COMPREHENSION 45\\n• Zhang et al.(2020) create two different graphs: a) sentence graph\\nwith edges between sentences belonging to the same paragraph\\nand between sentences that share an entity. b) entity graph with\\nedges between entities appearing in the same sentence, between\\ndifferent mentions of same entities and between each entity in the\\nparagraph to each entity in its title. The intuition is that humans\\nfirst focus on question-related paragraphs, then sentences, and\\nthen the important words. BERT word embeddings are passed to\\na bidirectional attention layer to get the contextual embedding.\\nThey also propose a novel similarity matrix for the layer. Self\\nattention is applied to the query and query aware node embeddings\\nare passed to a GAT. Self attention is applied on the sentence node\\nrepresentations and the output is appended by the representation\\nof each word in the sentence. An LSTM is applied to fuse the\\noutput of 2 graphs.\\n• Huang and Yang (2021) form a sentence graph and add an edge\\nbetween sentencessi and sj with the weightwij given by:\\nwij =\\n\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f3\\n1\\n1+e−n+K1 si and sj have n> 0 shared entities\\n1\\n1+ed+K2 si and sj belong to the same paragraph\\nseparated byd sentences.\\n(4.2)\\nThese weights allow the model to deal with the edge types in a\\nnovel way. The concatenated paragraphs along with the question\\nare fed to a BERT followed by a bi-attention layer. Sentence\\nrepresentations are obtained by extracting token level embeddings\\nfrom the paragraph encoding and a weighted addition of token\\nembeddings where the weights are calculated using a two layer\\nMLP (Multi Layer Perceptron). Most methods using GNN per-\\nform message passing for each node in parallel. This requires the\\nrepresentation of nodes to be updated exactlyL times whereL\\nneeds to be specified as a hyper-parameter. IfLis large, it leads to\\nover smoothing. If it is too small, it inhibits long-path reasoning.\\nMoreover, this algorithm performs unnecessary updates leading to\\ninefficiency. Thus, a novel message passing algorithm is proposed'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 49}, page_content='46 Existing Approaches for MHQA\\nwhich performs a BFS starting from the question and passing the\\nmessages through every edge that is visited in the process.\\nCritique: Thayaparan et al.(2019) argue that the advantage of using\\ngraph-structured representations lies in reducing the inference steps\\nnecessary to combine multiple pieces of information related in a path-\\nlike manner. The fact that the required graphs (or some parts of it) can\\nbe created offline gives it a computational advantage. However, graph\\nstructure also has certain limitation (Shaoet al., 2021), particularly for\\ncomparison type questions in HotpotQA where the evidence paragraphs\\nabout the two entities are independent. Many of the techniques assume\\nthat the relation between nodes is directional which may not always\\nbe the case. For gated GNNs, computational efficiency as well as the\\nlearning ability of the model degrades with the increasing number of\\nnodes and edge types in the graph used.\\nWhile the graph structure is prevalent for multi-hop reasoning, Shao\\net al.(2020) argue that it is not necessary and that graph attention\\ncan be considered as a special case of self-attention. Treating Qiuet al.\\n(2019) as the baseline, results are compared after removing the graph\\nfusion module. It is observed that the performance gained by using\\nthe graph structure can be easily compensated for by fine-tuning the\\nBERT based encoder. While the graph structure enables the model to\\nfocus only on adjacent nodes, a model without any prior knowledge\\ncan still learn this behaviour. This is verified by further experiments.\\nWhen the graph fusion module is replaced by self-attention layers the\\nresults are very similar whereas replacing it by a transformer leads to\\na significant improvement. Graph attention reduces to self-attention\\nfor a fully connected graph making it a special case of self-attention.\\nAttention patterns are visualized and observed similar to Kovalevaet al.\\n(2019). It is found that the pre-trained transformers are able to capture\\nseveral types of attention patterns: a) between entities, b) between\\nco-referenced entities, c) between an entity and its attribute, and d)\\nbetween an entity and a sentence. Depending on the structure of the\\ngraph, these patterns may or may not be covered by graph attention.\\nTherefore, self-attention can be said to be more general and flexible\\nthan graph attention.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 50}, page_content='4.2. READING COMPREHENSION 47\\n4.2.2 Question Decomposition Techniques\\nThe key idea here is to decompose the multi-hop question into multiple\\nsingle hop questions and use a single-hop QA model to answer each\\nof the questions. This approach is particularly effective for questions\\nlike the bridge questions in HotpotQA that are constructed using a\\nbridge entity. These questions can be easily decomposed into two sub-\\nquestions by finding the bridge entity. Patelet al. (2022) manually\\nlabel the decomposition for questions across multiple datasets and use\\nGPT-3 and RoBERTa to answer the questions and observe that question\\ndecomposition can help fix60% of the errors made on the original multi-\\nhop questions. However, this technique exploits the knowledge about the\\nstructure of question and thus, is difficult to adapt when the structure\\nof the questions can be flexible. Below we list some representatives\\nmethods for this approach:\\n• Min et al.(2019b) use the hypothesis that each sub-question can\\nbe formed from a multi-hop question by copying and lightly edit-\\ning a key span from it. Questions in HotpotQA are categorized\\ninto four categories: bridge (47%), comparison (22%), intersec-\\ntion (23%)(questions asking for an entity that satisfy multiple\\nproperties) and others (8%). Editing methods are proposed to\\nbe dependent on the question type for the first three categories.\\nQuestion text is segmented into several spans by training a pointer\\nnetwork that predictspij, the probability of theith word to be\\nthe jth index to split the question. 400 annotations are manually\\ngenerated for training. For each question, three such indices for\\nbridge questions, two indices for intersection and four indices for\\ncomparison questions are predicted by maximizing the joint prob-\\nability of the decomposition. These spans are modified slightly\\ndepending on the reasoning type. Any single-hop QA model can\\nbe used for answering each single hop question. Concatenation of\\nthe question, the reasoning type, the answer, and the evidence is\\nencoded using BERT and scored using 1 layer feed forward NN\\nwith sigmoid activation. The reasoning type is decided as the one\\nresulting in the maximum score. An alternative way where the\\nreasoning type is predicted before decomposing and answering is'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 51}, page_content='48 Existing Approaches for MHQA\\ntried and found to be giving poor results. To verify the original\\nhypothesis, same technique is tested with span-based questions\\nreplaced by human written questions. There is a little difference in\\nmodel performance indicating that the span-based sub-questions\\nare as effective as free-form sub-questions.\\n• Cao and Liu (2021) break down the task of MHQA into two\\ncomponents: Coarse grained decomposition and Fine grained in-\\nteraction. The decomposition module is responsible for making the\\nhigh dimensional vector distribution of entity nouns or pronouns\\nmore inclined to the intermediate answer to the question. The fine\\ngrained interaction module is a modified bidirectional attention\\nmodule. The resulting context representations are passed to a\\nself-attention layer and then used for supporting facts prediction\\nby passing to a Bi-GRU layer.\\n• Sun et al. (2021) propose MHQA methods for four different\\ndatasets: HybridQA, QASPER, HotpotQA-Long and ShARC-\\nLong. QASPER is originally proposed as single hop QA over long\\ndocuments whereas ShARC is proposed as a conversational QA\\ntask. The paper utilizes the fact that long documents are often\\nstructured into sections and subsections which can be used as\\nseparate contexts with limited dependence among them. A pre-\\ntrained ETC model (Ainslieet al., 2020) is used as the question\\nencoder and the context encoder. ETC is a pre-trained Mask\\nLanguage Model that employs a global-local attention mecha-\\nnism. ETC assigns to each sentence a special global token that\\nonly attends to local tokens in the sentence, and its embedding\\nis trained to summarize the information of local tokens in the\\nsentence. ETC additionally adopts Contrastive Predictive Coding\\n(CPC) (Van den Oordet al., 2018) to train the embedding of\\nglobal tokens to make them aware of other sentences in the context.\\nIt takes in a sequence of sentences and outputs the contextual-\\nized representations of each sentence. For conversational QA, the\\nquestion is formed by concatenating the original question with a\\nsentence formed by each pair of follow-up question and answer.\\nETC is again used to encode the question paragraph. For the'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 52}, page_content='4.2. READING COMPREHENSION 49\\n2-hop questions (HotpotQA, HybridQA), a null sentence is passed\\nto ETC along with the question to get two question encodings.\\nThe paragraph embeddings are computed by a weighted sum of\\nsentence embeddings, where weights are the attention score of the\\nquery vector to that sentence. At every step, each sentence and\\nevery paragraph in the document is attended to the query and the\\noutput is used to update the query representations. The updated\\nquery representation are then combined with the embedding of\\nnext query sentence to give the query vector for the next hop.\\nCritique:Min et al.(2019b) also find that some of the questions are\\nnot composed of two single-hop questions but require implicit multi-hop\\nreasoning, hence cannot be decomposed. Secondly, for some questions,\\nanswer for each sub-question does not exist explicitly in the text and\\nhas to be inferred with commonsense reasoning.\\n4.2.3 Miscellaneous Techniques\\nIn this section, we list some interesting approaches that do not fall into\\nthe above defined categories.\\n• Question generation:Li et al.(2023) propose multi-task train-\\ning of a model to perform both multi-hop question answering and\\nsub-questions generation. The motivation is that a model capable\\nof asking good sub-questions corresponding to a complex question\\nshould perform well at MHQA. The system consists of a GNN\\nbased QA component and an LM based QG component that are\\ntrained together with a shared encoder.\\n• Entailment based MHQA: Equation 2.1 presents MHQA\\nroughly as an entailment task, where the premise consists of\\nmultiple contexts and the hypothesis is \"a answers q\" (referred\\nas Haq henceforth). This indicates the utility of entailment mod-\\nels for the task. However, modelling MHQA as entailment faces\\nthree major challenges: a) A larger set of possible answers makes\\nthis method difficult to scale. b) MHQA requires aggregation of\\nmultiple contexts and single sentence based entailment models\\ncan not be used directly. c) Entailment models are usually not'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 53}, page_content='50 Existing Approaches for MHQA\\ntrained to filter irrelevant information. Trivediet al.(2019) aim\\nto tackle the two latter challenges on Multiple choice datasets,\\nOpenBookQA and MultiRC. Two simple baselines are proposed:\\nConcatenate that concatenates all sentences as the input to an\\nentailment task, andMax of local decisionsthat uses entailment\\non each sentence independently and then aggregate the results\\nwith a max operation. The two baselines perform poorly validating\\nthe challenges mentioned above. Therefore, a novel method ‘Mul-\\ntee’ comprising of a sentence relevance module and a multi-level\\naggregation is proposed. The sentence relevance module uses a\\npre-trained entailment model to produce hypothesis aware repre-\\nsentation of each sentence which are passed to a Bi-LSTM layer\\nto give contextual representation of each sentence which is fed to\\na feed forward layer to get the relevance scoresαi. The multi-level\\naggregation module pass each sentence along with the hypothesis\\nto k ESIM (Chenet al., 2017b) entailment stacks to generatek\\nparagraph level vectors which are concatenated and passed to\\na feed forward layer to predict the entailment. Each entailment\\nstack hasmlayers,lof these layers process each sentence indepen-\\ndently and the outputs are aggregated by usingαi’s. Rest of the\\nlayers process this aggregated output to result in paragraph level\\nembeddings. The aggregation is also done in the cross attention\\nlayer to produce a cross attention matrix containing attention\\nbetween each hypothesis token and each paragraph token. The\\nsentence relevance weights are used here as well. Each entailment\\nstack used is pre-trained on SNLI (Bowmanet al., 2015) and\\nMultiNLI (Williamset al., 2018).\\n• Commonsense knowledge for MHQA:Bauer et al.(2018)\\nargue that a pre-trained model may not be able to capture every\\ngrounded common-sense knowledge even with large corpora and\\npropose using ConceptNet (Speeret al., 2016) for extracting the\\nsame and using it, form reasoning paths leading to the answer.\\nA tree of various reasoning paths, all starting from the question\\nconcepts (c′\\n1s) is formed by following four steps: a) Selecting\\nrelations r′\\n1s from ConceptNet that linkc1 to another concept'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 54}, page_content='4.2. READING COMPREHENSION 51\\nc2 from the context. b) selecting relationsr′\\n2s that link c2 to\\nanother conceptc3 from the context. c) Selecting all neighbouring\\nconcepts c4 of c3 connected byr3. d) Selecting relationsr4 that\\nlink c4 to another conceptc5 from the context. This results in a\\nlarge number of reasoning paths that are scored in two steps: a)\\nn−score is computed by term frequency of each ofc2,c3,c5 in\\nthe contextC. c4 is scored with its Point-wise Mutual Information\\n(PMI) (Church and Hanks, 1990) withc1−3. Each node’s score\\nis normalized across all its siblings in the tree using Softmax. b)\\nc−score The node scores are accumulated starting from the leaf\\nnode and updating each non-leaf node recursively.\\nc_score(ci) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nn_score(ci) ci is a leaf node\\nn_score(ci) +\\nc_score(c′\\ni+1)+c_score(c′′\\ni+1)\\n2 otherwise\\n(4.3)\\nwhere c′\\ni+1 and c′′\\ni+1 are the top two scoring children ofci. At\\neach level of the tree, for every node, only the two top scoring\\nchildren are maintained and the rest are pruned resulting in a\\ntotal of24 reasoning paths. Context representation is attended\\nto the commonsense representations formed by embedding the\\ntokens in each of the reasoning paths. Updated context represen-\\ntation is combined with the original one using a sigmoid gate.\\nThe gate incorporates the fact that the commonsense might be\\noptional. Hence, the unit is called NOIC (Necessary and Optional\\nInformation Cell).\\n• Reasoning over tables:Chen et al.(2020b) use BERT to encode\\na cell in a table where a cell is represented by its value, position,\\nhyperlinks etc. Encoding of a cell along with the encodings of its\\nneighbouring cells is fed to a feed forward model and a Softmax\\nlayer to find the cell for the next hop. The model can also hop\\nto the same cell. The cell value is prepended to the hyperlinked\\npassage and passed to the answer prediction module.\\n• Pointer network for reasoning chain prediction:Chen et al.\\n(2019) encode the question and obtain query dependent paragraph\\nencoding by using BERT. Sentence encodings are extracted from'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 55}, page_content='52 Existing Approaches for MHQA\\nthe paragraph encoding similar to Huang and Yang (2021). Al-\\nternative baselines are a) BERT-Sent that obtains query aware\\nencoding of each sentence by using BERT and b) BiDAF-Para\\nthat uses BiDAF (Seoet al., 2016) to encode paragraphs. Results\\nshow that BERT-para performs the best. An LSTM Pointer Net-\\nwork is trained to predict probability of each sentence being the\\nt−thsentence in the reasoning chain i.e.,P(si = rct). The ground\\ntruth reasoning chain is determined using a chain extractor model\\ndiscussed in Section 4.4. The model is trained using both Negative\\nLog Likelihood (NLL) and Reinforcement Learning (RL). How-\\never, RL does not improve the performance significantly. During\\ntest time, a beam of possible chains is maintained since the best\\nchain may not be evident until it is entirely retrieved.\\n• Bi-directional attention baseline:Yang et al.(2018) propose\\na baseline by modifying the architecture of Songet al. (2018).\\nQuestion and the concatenation of paragraphs is passed to an\\nRNN for combining the character and word level embeddings.\\nBi-directional attention is applied across the question and context\\nembeddings to get query aware context representations. A residual\\nconnection is added to the output of another RNN on these\\nrepresentations before passing to a self-attention layer.\\n4.3 Answer Prediction Module\\nAfter the reasoning module outputs a reasoning chain or some latent\\nspace representations of the reasoning, an answer prediction module\\npredicts the final answer. This module can be directly categorized based\\non the type of answer required by the task at hand:\\n4.3.1 Candidate Answering\\nThis category corresponds to the multi-choice setting of the MHQA\\ntask. Methods dealing with candidate answer prediction usually start\\nthe retrieval/reasoning process by using a candidate-aware embedding\\nof the question, and output the representations for each candidate\\nindependently. We look at some of these methods below:'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 56}, page_content='4.3. ANSWER PREDICTION MODULE 53\\n• A Bi-directional attention layer on query and context (Caoet al.,\\n2019), or a self-attention layer on the embeddings may also be\\nemployed.\\n• Some of the methods use these representations to perform an\\nindependent binary classification for each candidate (for multiple-\\ncorrect type questions such as MultiRC) (Yadavet al., 2019b;\\nYadavet al., 2020; Yadavet al., 2021; Trivediet al., 2019).\\n• On the other hand, some methods perform a multi-way classifica-\\ntion for all candidates (for single-correct type questions such as\\nOpenBookQA, WikiHop) (Yadavet al., 2020; Yadavet al., 2021;\\nCao et al., 2019; De Caoet al., 2019; Chenet al., 2019).\\n• De Caoet al.(2019) use an ensemble of five models, each trained\\nwith a different weight initialization.\\n• Yadavet al.(2020) use both the answering methods for QASC and\\nfind that the multi-way classification results in an improvement\\nof 5% accuracy.\\n4.3.2 Span Answering\\nFor tasks requiring the answer as a text span from the contexts, the\\nfollowing methods have been proposed:\\n• Yanget al.(2018) suggest a span answering approach where the\\noutput of the reasoning module is passed to another RNN for\\nsupporting facts prediction. Another RNN is used to predict the\\nstart and end token of the answer span. Finally, a three-way\\nclassifier layer is used to predict the answer type among ’yes’, ’no’\\nand the extracted span. Many methods focusing on the first two\\nunits use this baseline as the answer prediction unit (Feldman and\\nEl-Yaniv, 2019; Daset al., 2019; Shaoet al., 2021; Zhanget al.,\\n2020; Cao and Liu, 2021; Qiuet al., 2019; Shaoet al., 2020).\\n• Qi et al.(2019) make two changes to the above baseline: a) Con-\\ncatenating passages before encoding makes the representations\\ndepend on the order while concatenating. Thus, a shared RNN'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 57}, page_content='54 Existing Approaches for MHQA\\nencoder first encodes each passage independently and then con-\\ncatenates the representations. b) All the attention layers are re-\\nplaced by self-attention on the concatenated question and context\\nrepresentation.\\n• Ding et al.(2019) and Fanget al.(2020) replace the RNNs in the\\nabove baseline by MLPs and directly feed the node representations\\nfor answering. Xionget al.(2019) deal only with bridge question\\nand hence use a single MLP for span prediction on entities. Thaya-\\nparan et al.(2019) only deal with supporting facts prediction and\\nuse a single MLP for the same.\\n• Zhang et al.(2021) use an AlBERT model to predict the answer\\nspan among theK identified passages. The model then predicts if\\nthe answer was found in the given passage. If not, the predicted\\nspan is used for iterative retrieval for the next hop.\\n• Min et al.(2019b) use an off-the-shelf single-hop answering module\\nto answer the sub-questions formed by the reasoner.\\n• Malon and Bai (2020) use a simple BERT model to answer the\\nquestion from the paragraph identified as the answer paragraph.\\nDua et al.(2021) do the same from the concatenation of the two\\nidentified passages.\\n• Chen et al.(2019) concatenate question and all sentences of all\\nthe retrieved reasoning chains and feed to BERT to perform the\\n4 tasks.\\n• Huang and Yang (2021) propose to use the output of the GNN\\nmodel. Sentence scores are computed using MLPs on the sentence\\nnode representations. Paragraph scores are computed using MLPs\\non max pooling over the sentence representations. The two scores\\nare combined with the span extraction score to predict the final\\nanswer. Separate MLPs are used for predicting the answer type\\nand the supporting facts.\\n• Sun et al. (2021) Concatenate the embeddings at all retrieved\\nsteps {k0,··· ,k∗,··· ,k0,··· ,k∗}and perform a weighted sum'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 58}, page_content='4.4. AUXILIARY TASKS 55\\nto getKthat is used to make the final prediction. The Softmax\\nweight Yj is computed across the retrieved sentences from all\\nsteps.\\n4.3.3 Generative Answering\\nPreviously, generative answering models had a low prevalence because\\nof the unsatisfactory performance of models to generate relevant texts\\nas well as the lack of reliable evaluation techniques. As a representative\\nof the earlier approaches, Baueret al.(2018) use a self-attention layer\\nfollowed by a PGN decoder to get the final answer.\\nHowever, with the recent advancements relating to large language\\nmodels (LLMs), generative answering approaches have become a lot\\nmore popular. We describe these methods in detail in Chapter 5 desig-\\nnated to LLMs for MHQA.\\n4.4 Auxiliary Tasks\\nA lot of existing approaches suggest using an auxiliary training task\\nthat can help the training of the model by providing an extra signal.\\nHere are two of the commonly used auxiliary tasks:\\n4.4.1 Reasoning Chain Prediction\\nReasoning chains are an integral part of explainable MHQA. HotpotQA\\ncontains the supporting facts that are required to answer the question,\\nhowever these are not ordered. Several works have aimed to predict the\\norder among these supporting facts by using simple lexical heuristics\\n(Jhamtani and Clark, 2020; Trivediet al., 2020; Wanget al., 2019) so\\nthat the reasoning chain formed can be used to further train or evaluate\\na model. However, it is also crucial for the models to be able to predict\\nthe reasoning chains only by using the question and contexts.\\n• Fenget al.(2020) propose a semi supervised reinforcement learning\\nfor two modules to recover the reasoning chain in acooperative-\\ngame approach. Apart from predicting the reasoning chain, the\\nmodel is also able to predict the relations among the sentences.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 59}, page_content='56 Existing Approaches for MHQA\\nThe two modules are: a) Ranker module that, givenk passages\\nand a question, selects a reasoning chain. Question and passages\\nare encoded by bi-GRU (Choet al., 2014) and a Match-LSTM\\n(Wang and Jiang, 2016) model is used to get a probability for each\\npassage containing a part of the reasoning chain. A passage is then\\nsampled and used to update the question by passing to an MLP.\\nThe process is repeated with the updated question. The ranker\\nmodule is rewarded for selecting the correct passage at the correct\\nreasoning step. b) Reasoner module that predicts the entity from\\nthe current passage to the next passage. Given the first passage\\n(called head) selected by the trained Ranker, the Reasoner uses\\na Math-LSTM model to predict the probability of each entity\\nappearing in the second passage (called tail). While the ordering of\\nthe supporting facts is generated similar to Yanget al.(2018) for\\nHotpotQA, the reasoning chains need to be manually annotated\\nfor MedHop. The manual annotation is done by extracting all valid\\npaths such that the first sentence contains an entity in the second\\nsentence and the second contains the answer. A chain is manually\\nlabelled as positive if the corresponding passage describes the\\ndrug-protein interaction. Reasoning chains extracted in this way\\nmay not be unique.\\n• Chen et al. (2019) derive pseudo gold chains using NER and\\nco-reference resolution. A sentence (only) graph is constructed\\nwith edges between sentences that belong to the same paragraph\\nand between those that have a shared entity. Reasoning chains\\nare collected by starting from the question node and finding all\\npossible paths in the graph that lead to an answer containing\\nsentence. These chainsare then ranked by two heuristics: a) shorter\\nchains are given higher scores. b) chains whose sentences have\\na high F-1 ROUGE overlap with the question are given higher\\nscores. Experimental results indicate that b) is a good criteria for\\nscoring chains. Human evaluation shows that the reasoning chains\\nproduced are of similar quality compared to the supporting facts\\npresent in the HotpotQA. A pointer network is then trained to\\npredict each sentence in the reasoning chains.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 60}, page_content='4.5. CONCLUSION 57\\n4.4.2 QA with Partial Knowledge\\nKhot et al.(2019) propose a new sub task of MHQA where the model\\nhas to figure out and fill the knowledge gap for question answering. It is\\nassumed that the first hop retrieval has been performed ideally and the\\ntask is to use the retrieved context to answer the question. A modified\\nversion of OpenBookQA is released where the core fact is part of the\\ninput and several relations have been modified.\\n4.5 Conclusion\\nIn this chapter, we covered a large number of pre-LLM machine learning\\nmethods proposed to solve MHQA. While doing so, we provided some\\nstructural patters that many methods follow. We also provided various\\nlevels of classifications of the methods, with technical details for some\\nrepresentative methods pf each category or subcategory. In chapter 6,\\nwe build on this classification to propose a taxonomy covering most of\\nthe works discussed in this chapter. In the next chapter, we provide\\ndetails for how LLMs have been incorporated for different stages of the\\ntask along with some challenges and their proposed solutions.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 61}, page_content='5\\nLLMs for MHQA\\nLarge Language Models (LLMs), including the multiple variants of GPT,\\nBERT and T5 models, have achieved remarkable success in various\\nnatural language tasks. We present a comprehensive background of\\nLLMs and various prompting techniques in the Appendix A.5. We\\nalso recommend Zhao et al. (2023b) as an additional reading for a\\ncomprehensive survey on LLMs.\\nSince language models like T5, BERT have been used for multiple\\ntasks in most methods we have discussed so far, we use the following\\ndistinction for LLM-based methods: A method is LLM-based if it makes\\nuse of the emergent abilities of the LLMs (Zhaoet al., 2023b) which\\ninclude in-context learning and instruction following1. We categorize\\nLLM based methods based on the sub-task the LLM performs.\\n5.1 Retrieval\\nNair et al.(2023) explore using LLMs for retrieving relevant evidences\\nfrom long documents. Major challenge in using LLMs for retrieval is\\ntheir limited context window size. The performance decreases drastically\\n1This distinction is made for the sole purpose of better structuring the discussion.\\n58'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 62}, page_content='5.2. REASONING CHAIN GENERATION 59\\nas the input context becomes larger than the context window size. To\\ndeal with this challenge, each section in the document is independently\\nsummarized by an LLM. These summaries are concatenated together in\\nanother LLM prompt, where the instruction is to get a list of relevant\\nsections. Once a set of relevant sections is obtained, the next step is\\nto retrieve the relevant paragraphs from each of these sections. For\\nthis, each paragraph is summarized and passed to the LLM to get the\\nset of relevant paragraphs. A bart-large (Lewiset al., 2019) trained\\nover the CNN/Daily-Mail Corpus (Nallapatiet al., 2016) is used for\\nsummarization, and GPT-3.5 is used for retrieval and answering.\\n5.2 Reasoning Chain Generation\\nExisting works have extensively explored GPT prompting for various\\nQA tasks. Although few-shot prompting on LLMs for reasoning showed\\nlimited success (Hanet al., 2022), chain of thought prompting (CoT)\\nhas shown better reasoning abilities. CoT is particularly useful for\\nexplainable MHQA since the generated chain of thought can be used as\\nthe reasoning chain (Zelikmanet al., 2022; Wang, 2021). Saparov and\\nHe, 2023 use synthetic data to evaluate GPT-3’s reasoning ability and\\nmeasure the coherence of generated reasoning chains. Results indicate\\nthat GPT-3 can perform multiple steps of reasoning but may rely on\\nbackground knowledge rather than explicit reasoning over the given\\ncontext.\\nBelow, we list a few works and describe in detail how the LLM was\\nused for generating reasoning chains.\\n• Rahgouy et al., 2023 evaluate T5 and Flan-T5 (Chunget al., 2022)\\nmodels with and without fine-tuning on chain-of-thought expla-\\nnations and in zero-shot, few-shot and CoT prompting settings\\non complex multi-hop queries. The results indicate the need for\\nimprovement in the existing methods.\\n• Zelikman et al., 2022; Wang, 2021 get promising results by directly\\nusing the chain-of-though explanations as reasoning chains.\\n• PathFiD Yavuzet al., 2022 was one of the first to use a large\\nlanguage model for modeling the task as a sequence generation'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 63}, page_content='60 LLMs for MHQA\\nproblem. However, the task was not modelled as a free-text natural\\nlanguage generation. A pre-trained T5 model was fine-tuned to\\ngenerate a ‘reasoning path’. A reasoning path is defined as a\\nsequence of sentences from the context passage along with the\\ncorresponding passage title, followed by the answer to the question.\\nThe encoder independently encodes the ‘blocks’ (sentence and\\ncorresponding paragraph title) in the retrieved context and the\\ndecoder decodes the reasoning path by selecting the relevant blocks\\nand deriving the answer.\\n• Haji et al.(2023) extend the idea of PathFiD by proposing the\\nLLM to generate parallel exploratory inference chains (EICs).\\nThey follow a multi-step generation process, prompting an LLM\\nmultiple times with different prompts in the process. During\\nthe first step, a LLM is prompted to generate some keywords\\nfrom the question and the paragraph titles. These keywords are\\nthen processed independently in parallel. A keyword is matched\\nlexically with available paragraph titles to retrieve the paragraphs\\nfor the next step, and the LLM is prompted with the retrieved\\npassage to predict some structured facts about the keyword. The\\nobject in these generated fact is used as a keyword for the next\\nreasoning step. Two independent LLMs are prompted to aggregate\\nthe facts for each keyword and to aggregate facts across keywords\\nto generate a final answer. Impressively, the resulting system is\\nable to significantly outperform a single chain-of-thought prompt.\\n• Trivediet al.,2023proposeIRCoTwhichusesinterleaving-retrieval\\nwith CoT reasoning to iteratively perform retrieval and reasoning\\ntogether. For each iteration, 1) the CoT-guided retrieval step\\n(“Retrieve”) uses the last generated CoT sentence as a query to\\nretrieve more paragraphs and adds them to the accumulating set\\nof the collected paragraphs, and 2)the retrieval-guided reasoning\\nstep generates the next sentence of the CoT using the previously\\ngenerated sentences along with the retrieved contexts. Finally,\\nanother LLM derives the answer using the generated CoT and\\nthe retrieved paragraphs.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 64}, page_content='5.3. HYBRID TEXT-TABLE REASONING 61\\n5.3 Hybrid Text-table Reasoning\\nThe following methods for utilizing LLMs for solving hybrid MHQA\\nhave been proposed:\\n• Lei et al. (2023) were one of the first to use LLMs for solving\\ntable-text hybrid MHQA. They train a sequence-to-sequence LLM\\nsimilar to Yavuzet al.(2022), to encode the retrieved contexts\\nand question. The decoder, however, performs auto-regressive\\ngeneration to predict the answer sentence. They also compare\\na prompting based approach where the retrieved contexts are\\nprovided to GPT-3.5 along with few-shot and CoT prompts. The\\nresults showed promising performance of CoT but still the fine-\\ntuned model outperformed all the prompting techniques.\\n• Chen et al.(2021) propose converting a table into text using tem-\\nplate based transformation and then use a RoBERTa to retrieve\\nsentences from the combined set of original text and converted\\ntext.\\n• Mavi et al.(2023) test the GPT-3.5 and Llama2’s ability to parse\\ntabular data by representing a table in text by using separators\\nsuch as|and ||to separate cells in a row and multiple rows respec-\\ntively and observe promising performance on tabular reasoning.\\n• Shi et al.(2024) explore using program generation and execution\\nbased framework for hybrid MHQA and get promising results.\\n5.4 Question Decomposition\\nSince GPT has shown remarkable language understanding, motivating\\nits use as a decomposition module for decomposition based MHQA\\n(Khot et al., 2023; Zhouet al., 2023).\\n• Zhou et al.(2022) explore large-scale intermediate pre-training of\\na T5 model to perform question decomposition. For constructing\\na large enough dataset for pre-training, distant supervision from\\ncomparable texts is used. In particular, parallel news articles de-\\nscribing the same events are used. The hypothesis is that seeing'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 65}, page_content='62 LLMs for MHQA\\nmultiple descriptions of the same facts will help the model make\\nbetter educated guesses. The proposed model, called DecompEn-\\ntail, first generates explicit question decomposition, then makes\\nfactual corrections on the decomposed statements with GPT-3.\\nAs a final step, an entailment model is used to derive the final\\nanswer with the generated decomposition as the premise and the\\nquestion and candidate answer as the hypothesis.\\n• Deng et al.(2022) perform question decomposition in the semantic\\nspace by using an AMR graph. They build an AMR graph from\\nthe question and add an additional ‘amr-unknown’ node denoting\\na concept that represents the answer. The AMR is segmented into\\nsub-graphs using some heuristics and each sub-graph is passed to\\na pre-trained BART model that converts it into a sub-question.\\nFinally, a single-hop QA model is used for answering the sub-\\nquestions.\\n• Wuet al.(2024) fine-tune a sequence-to-sequence LLMs to gen-\\nerate sub-questions using the given multi-hop question. A set\\nof retrieved context is also given as input during this step. The\\nsub-questions are used for performing a second round of retrieval\\nusing a DeBERTa model (Heet al., 2021). Another DeBERTa\\nmodel is used on the concatenation of the retrieved paragraphs\\nand sub-questions to generate the final answer.\\n5.5 Graph Construction\\nLi and Du (2023) prompt an LLM with a Wikipedia passage to build\\na semantic graph. This is done by prompting the LLM to extract all\\nthe entities in the document and then prompt it again to get all the\\nrelations among these entities. Finally, each passage and the correspond-\\ning semantic graphs are concatenated into a prompt for generating\\nreasoning chain and the final answer.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 66}, page_content='5.6. MULTI-HOP RETRIEVAL AUGMENTED GENERATION 63\\n5.6 Multi-hop Retrieval Augmented Generation\\nRetrieval Augmented Generation (RAG)is the term given to the\\ngeneral framework where a generative model (usually a LLM) performs\\ngeneration by using a set of facts extracted by a retriever. In RAG, an\\nexternal corpus containing multiple documents serves as the knowledge\\nbase. Each document within this corpus is segmented into a set of\\nchunks. These chunks are then transformed into vector representations\\nusing an embedding model and stored in an embedding database. Given\\na user query, the system typically retrieves the top-K chunks that best\\nmatch the query. The retrieved chunks, combined with the query and an\\noptional prompt, are then fed into an LLM to generate a final answer.\\nJoshi et al., 2023a build a novel dataset for the development and\\nbenchmarking of multi-hop RAG models. The dataset contains a knowl-\\nedge base for retrieval, a set of multi-hop queries along with their ground\\ntruth answers and corresponding supporting facts. The knowledge base\\nis built from a set of news articles. GPT-4 is prompted to extract factual\\nsentences from the news articles\\n5.7 Critique and Limitations\\nDespite the impressive linguistic and reasoning abilities of LLMs, they\\nface certain limitations. In this section, we list these limitations and\\nsome attempts at resolving these.\\n5.7.1 Hallucinations\\nThe biggest challenge that the LLMs currently face is that they are\\nknown to hallucinate at times, i.e., generating text that is factually\\nincorrect or not derivable from the given information. This poses a\\nthreat to accountability of the resulting MHQA system and can pro-\\nduce incorrect results. A large number of works have tried to propose\\nsophisticated and clever ways of overcoming this challenge. We describe\\nfew such interesting works in detail below:\\n• Self-consistency:Self-consistency(Wang et al.,2023)isproposed\\nas an alternative for the naive greedy decoding used in chain-of-'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 67}, page_content='64 LLMs for MHQA\\nthought prompting. Instead of only taking the greedy decoded\\nreasoning path, a diverse set of reasoning paths are sampled,\\nand then the most consistent answer across the reasoning paths\\nis predicted by marginalizing out the sampled reasoning paths.\\nSelf-consistency is motivated by the intuition that a complex\\nreasoning problem typically admits multiple different ways of\\nthinking leading to its unique correct answer. Assuming that the\\nhallucinations in the LLM output are random, self-consistency\\nis more likely to lead to the correct answer since it is unlikely\\nthat majority of the reasoning paths with hallucinations lead to a\\ncommon answer. Therefore, self-consistency has been adopted to\\nmitigate hallucinations.\\n• Self-refine: In self-refine (Madaanet al., 2023), the LLM gen-\\nerates an initial response to the prompt, and then iteratively\\nprovides feedback for the output and refines it. The same LLM\\nserves as the generator, feedback provider as well as the refiner.\\nDuring self-refine, the LLM can also evaluate the response on\\nfactual correctness, thus fixing the hallucinations.\\n• Joshi et al., 2023b extend self-consistency and self-refine by fine-\\ntuning external LLMs for providing feedback on the generated\\nreasoning chains. The authors collect labelled data for correctness,\\nerror types and descriptions, and the required corrections for each\\nreasoning chain and answer pairs for 2361 examples. They use\\na weighted self-consistency approach where a fine-tuned Llama2\\npredicts a score for whether each answer and its corresponding\\nreasoning path are correct or not. This score is used as the weight\\nwhile voting for the correct answer. They also explore extending\\nself-refine by using a fine-tuned Llama2 model to identify the\\nerrors in a reasoning path and refining the generated output.\\n• Zhao et al., 2023a propose ‘Verify and Edit’ that combines self-\\nconsistency and self-refine. It follows a three-step process: finding\\nuncertainpredictionsusingself-consistency,editingtheirrationales\\nby searching for supporting facts, and using the edited rationales\\nto generate final answers. An answer is considered as uncertain, if'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 68}, page_content='5.7. CRITIQUE AND LIMITATIONS 65\\nnone of the generated answers gets the majority agreement. For\\neach of the predictions corresponding to the uncertain answer,\\neach sentence in the reasoning chain is verified by generating cor-\\nresponding questions. These questions are answered after retrieval\\nand the answer is used to update the particular sentence in the\\noriginal reasoning chain. Finally, the updated reasoning chain is\\nused to generate the final corrected answer.\\n• Balepuret al.,2023proposeaclaimdecompositionbasedtechnique\\nfor self-evaluation of the LLM generated answers. The complex\\nquestion is broken down into a set of claims that a correct answer\\nto the question must satisfy. The LLM then identifies which claims\\nmention the same entities and include extra tags for them. For\\nevaluating a generated answer, the answer is replaced in each of\\nthe claims and the LLM is prompted to answer true/false based on\\nwhether the answer satisfies the claim or not. The answer is then\\nscored on the ratio of claims satisfied by it. Multiple answers are\\npredicted by sampling based generation of the reasoning chains\\nand the one with the highest score is predicted.\\n5.7.2 Knowledge Gaps\\nFeng et al., 2024b argue that the challenge of hallucinations is difficult\\nto overcome since the LLMs might always suffer from knowledge gaps.\\nA knowledge gap here refers to some missing or outdated information.\\nSince LLMs heavily rely on knowledge gained during the pre-training\\nphase, outdated or incomplete information in the pre-training corpus\\ncan confuse the model, leading to generating factually incorrect text.\\nFurther, pre-training LLMs is extremely expensive in terms of time\\nand compute. It is therefore ideal that the LLM is able to detect\\na knowledge gap and refrain from generating information in such a\\ncase. For identifying knowledge gaps, the following approaches can be\\nfollowed:\\n• Calibration-based: getting a probability score for an answer by\\neither using token probabilities and temperature during generation\\n(Radford et al., 2019; Lianget al., 2023) or directly asking the'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 69}, page_content='66 LLMs for MHQA\\nmodel to generate its confidence in its output (Tianet al., 2023).\\n• Training based:training an extra layer (Slobodkinet al., 2023),\\nor an external module (Cobbeet al., 2021) to predict whether the\\nanswerproducedhasahighenoughconfidence.Anotherinteresting\\nway is to use identifying knowledge gaps as one of the instruction\\ntuning tasks (Ouyanget al., 2022).\\n• Prompting based: techniques such as self-reflect (Kadavath\\net al., 2022), self-consistency (Wanget al., 2023) predicting none-\\nof-the-above (Kadavathet al., 2022) or more information needed\\n(Feng et al., 2024a) instead of predicting the answer.\\n• Multi-LLM collaboration:using two independently trained\\nLLMs for either providing feedback to each other, or complement-\\ning each other to fill the knowledge gaps in both (Fenget al.,\\n2024b).'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 70}, page_content='6\\nMHQA Taxonomy\\nThis chapter builds on the distinctions and classifications drawn in\\nChapter 4 and our taxonomy that covers the existing methods in a\\nstructured way. Creating a taxonomy of existing methods for multi-hop\\nQA is key for sorting through research and making comparisons. It also\\nhelps spotting where the community needs to pay more attention. We\\nsummarize these in Table 6. The titles and acronyms used in the table\\nare described below, and illustrated in Figure 6.1.\\n• General:\\n– Datasets used1: The datasets used for MHQA along with\\ntheir acronyms in Table 6 are: HotpotQA (Yanget al., 2018)\\nin the full-wiki (HP-F) and distractor (HP-D) settings. Sun\\net al. (2021) provide the modified versions of the ShARC\\n(Saeidi et al., 2018) and the HotpotQA datasets as ShARC-\\nLong (Sh-L) and HotpotQA-Long (HP-L). Other datasets\\nare referenced as: MultiRC (Khashabiet al., 2018) (MRC),\\n1Note that we do not include the datasets used in the proposed Taxonomy, but\\ndescribe it in this section for the sake of completionstill add it in the Table 6 for the\\nsake of completeness.\\n67'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 71}, page_content='68 MHQA Taxonomy\\nMulti-hop \\nQA task RC based\\nOthers\\nGraph \\nbased\\nEdge\\nType\\nNode \\nGranularity\\nEntity\\nSentence\\nParagraph\\nHomogenous\\nHeterogeneous\\nQuestion \\nDecomposition\\nN-hop\\nAny-hop\\nSpecific\\nTask \\nbased\\nDomain \\nSpecificity\\nNo. of Hops\\nTwo-hop\\nGeneric\\nSpan\\nMCQ Answer \\nType\\nGenerative\\nRetrieval\\nbased\\nFinal \\nRetrieval\\nPreliminary\\nRetrieval\\nRerank\\nConcatenation\\nRerank \\nparadigm\\nRerank Unit\\nRetrieval \\nRounds\\nRetriever \\ntype\\nSupervised\\nUnsupervised\\nContextChain\\nSingle\\nIterative\\nReformulative\\nLinking\\nDense\\nLexical\\nFigure 6.1:Overview of Our Taxonomy.\\nARC (Clarket al., 2018) (ARC), QAngaroo datasets (Welbl\\net al., 2018) WikiHop (WH) and MedHop (MH), HybridQA\\n(Chen et al., 2020b) (Hy), QASPER (Dasigiet al., 2021)\\n(QSP),OpenBookQA(Mihaylov et al.,2018)( OB),QASC(Khot\\net al., 2020) (QSC) andNr (Kočisk` yet al., 2018).\\n– Hop Constraints:Some of the methods require the type of\\nquestions to be exactly two-hop questions (Two) or require\\nthe number of hops in the question to be provided as an\\ninput (N). Other methods are flexible for answering any-hop\\nquestions without any additional input (Any).\\n– Answer types:Whether the methods proposed work for\\nMCQ type questions (MCQ), Span based questions (Span)\\nand generative answers (Gen). Note that some methods\\nfocus on more than one of these answer types.\\n– Domain Specificity:Whether the works focus on domain\\nspecific (Specific) or domain generic techniques (Generic).\\n• Retrieval:\\n– Retriever Type:Whether the preliminary retrieval step'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 72}, page_content='69\\nuses a dense (Dense) or a lexical (Lexical) retriever.\\n– No. of Retrieval Passes:Whether the preliminary step is\\nsingle pass (Single) or iterative (Iter). In case it is iterative,\\nwhether it uses query reformulation (Iter-QR) or Entity-\\nlinks/Hyperlinks (Iter-EL/H).\\n– Final Retrieval StrategyWhether the final retrieval step\\nuses a simple concatenation/union (Concat) of the contexts\\nor it is uses a re-ranking approach (RR). In case, it uses\\nre-ranking, whether the re-ranking is supervised (RR-S-\\n?) or unsupervised (RR-U-?), and whether the individual\\ncontexts (RR-?-X) or the entire chain candidates (RR-?-C)\\nare scored during the re-ranking.\\n• RC based:\\n– Node Granularity:In case the reasoner is a graph based\\nmodel, what kinds of nodes it has: entity nodes (Ent), sen-\\ntence nodes (Snt) or passage nodes (Psg). Note that the\\ngraph may have more than one type of nodes. (-) in case the\\nreasoner is not graph based.\\n– Relational Edge:Whether the graph is a relational graph\\n(Yes) (i.e., has multiple types of edges) or not (No) (i.e., has\\nsingle edge type). (-) in case the reasoner is not graph based.\\n– Question Decomposition:Whether the reasoning mod-\\nule uses question decomposition (QuesD) or not (Other).\\nThe rows where both graph-based and decomposition-based\\ncolumns are (-) are for the models that use other techniques\\nlike (Seoet al., 2016).\\nBy proposing the taxonomy, we hope to help the readers dive into\\nthese techniques, see what suits best to their needs or what needs more\\nattention in the existing setup. This should help the research community\\nkeep pushing the boundaries for solving MHQA.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 73}, page_content='70 MHQA Taxonomy\\nTable 6.1:Comprehensive study of existing work using the proposed taxonomy.\\nGeneral Retrieval Reasoning\\nDatasets Hop\\nConstraints\\nAnswer\\ntype\\nDomain\\nSpecificity Preliminary Final Graph based Ques\\nDecomp\\nType TechniqueTechniqueNode typesRelational\\ngraph?\\nDinget al.(2019) HP-F N Span Generic LexicalIter-EL/HConcat Ent Yes Other\\nFeldman and El-Yaniv (2019)HP-F N Span Generic DenseIter-QR RR-S-X - - Other\\nDaset al.(2019) HP-F Two MCQ Generic LexicalIter-EL/HRR-S-C - - Other\\nYadavet al.(2019b) ARC, MRCAny MCQ Generic LexicalSingle RR-U-C- - Other\\nXionget al.(2019) HP-F Two Span Specific LexicalIter-EL/HRR-S-X - - Other\\nFanget al.(2020) HP-F Two MCQ Generic LexicalIter-EL/HRR-S-C Ent-Snt-PsgNo Other\\nYadavet al.(2020) MRC, QSCAny MCQ Specific LexicalIter-QR Concat - - Other\\nZhanget al.(2021) HP-F Any Span Generic LexicalIter-QR RR-S-X Psg Yes Other\\nYadavet al.(2021) MRC, QSCTwo MCQ Generic DenseIter-QR RR-S-C - - Other\\nShaoet al.(2021) HP-F Two Span Generic LexicalIter-EL/HRR-S-X - - Other\\nSidiropouloset al.(2021a) HP-F Two Span Generic DenseSingle RR-S-X - - Other\\nCaoet al.(2019) WH N Span Generic - - Concat Ent-Snt-PsgNo Other\\nThayaparanet al.(2019) HP-D N Span Generic - - Concat Snt-Psg No Other\\nMinet al.(2019b) HP-D Two Span Generic - - Concat - - QuesD\\nDe Caoet al.(2019) WH N Span Generic - - Concat Ent No Other\\nZhanget al.(2020) HP-D N Span Generic - - RR-S-X Ent-Snt No Other\\nMalon and Bai (2020)HP-D Any Span Generic LexicalIter-QR RR-S-X - - QuesD\\nFenget al.(2020) MH, HP-DN Span Specific DenseSingle RR-S-C - - Other\\nChenet al.(2019) WH, HP-DN Span Generic - - Concat Snt No Other\\nHuang and Yang (2021)HP-D Any Span Generic - - Concat Snt No Other\\nCao and Liu (2021) HP-D Any Span Generic - - Concat - - QuesD\\nSunet al.(2021) HP-L, Sh-L\\nQSP, HyN Span Specific - - Concat - - QuesD\\nDuaet al.(2021) WH, HP-DTwo Span Generic - - RR-S-C - - Other\\nQiet al.(2019) HP-D N Span Generic LexicalIter-QR Concat - - Other\\nQiuet al.(2019) HP-D N Span Generic - - RR-S-X Ent No Other\\nShaoet al.(2020) HP-D N Span Generic - - RR-S-X - - Other\\nBaueret al.(2018) Nr N Gen Specific - - RR-S-X Ent Yes Other\\nTrivediet al.(2019) MRC, OBAny MCQ Generic - - Concat - - Other\\nKhotet al.(2019) OB - MCQ Specific LexicalSingle Concat - - Other'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 74}, page_content='7\\nHow to Evaluate MHQA Systems?\\nHaving evaluation metrics suited to a task is crucial for grasping the\\ntask’s nuances and gauging the performance of the existing or proposed\\nmethods accurately. It ensures researchers assess systems effectively\\nand compare results meaningfully. Future research should benefit clear\\nmetrics guide development, revealing where improvements are needed\\nand facilitating advancements in techniques tailored to the intricacies\\nof the task. In this chapter, we look at some commonly used evaluation\\nmetrics and potential limitations of using them. We follow our discussion\\nwith various sophisticated and clever experiments conducted by the\\ncommunity which shed further light on the intricacies of how the existing\\nsystems perform and suggest some directions where further research\\nand development is required.\\n7.1 Evaluation Metrics\\nDiversity in multi-hop QA tasks and datasets engenders the need for\\ndifferent evaluation metrics. The general trend in multi-hop QA methods\\nis to split the task into a retrieval (IR) component that finds the\\nrelevant contexts, and a reading (RC) component that produces the\\nanswer. Therefore, it makes sense to evaluate the two together as well\\n71'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 75}, page_content='72 How to Evaluate MHQA Systems?\\nas separately. We describe the methods used for the two evaluations\\nbelow:\\n7.1.1 Retrieval Evaluation\\nAs mentioned in Chapter 4, retrieval is often broken down into two\\nsteps. Different metrics can be used to evaluate the two steps:\\nPreliminary retrieval: Yeet al.(2019) propose three evaluation metrics:\\nP EM (Paragraph exact match) that measures the ability of the\\nretriever to retrieve all the gold paragraphs in the reasoning chain;\\nPR (Paragraph recall) that computes the recall of gold paragraphs\\namong the retrieved paragraphs; andAR (answer recall) that checks if\\nany of the retrieved paragraph contains the answer. Daset al.(2019)\\npropose acc@k which measures the fraction of cases where the model\\nwas able to retrieveall the supporting facts within topk retrieved\\ndocuments. Sidiropoulos et al. (2021a) define the per-hop retrieval\\nevaluation that treats each hop of retrieval independently. First hop\\nretrieval performance is measured by fraction of cases where the first\\ngold context is retrieved in the first hop of retrieval. For the second\\nhop, the gold paragraph is added to the set of contexts retrieved in the\\nfirst hop and the ability to retrieve the second hop gold paragraph is\\nevaluated. Since the paper only deals with 2-hop questions, the definition\\nis originally limited to 2 hops. However, we note that this might be\\nextended to n hops where thenth retrieval step is evaluated by the\\nability to retrieve thenth gold paragraphpn given∪n−1\\ni=1 ({pi}∪Ri) where\\nRi is the set of contexts retrieved duringith hop.\\nRe-ranking: Jhamtani and Clark (2020) require the models to classify\\nor rank several reasoning chains as valid explanations of the answer.\\nTherefore, they useAUC-ROC(Melo, 2013) (area under the Receiver\\nOperating Characteristics (ROC) curve) andF1 scores for classification,\\nand P@1and Normalized Discounted Cumulative Gain (NDCG)\\n(Järvelin and Kekäläinen, 2002) for ranking. P@1 measures the fraction\\nof cases where the top ranked chain is valid, whereas NDCG is a\\ncommonly used metric for evaluating rankings.MRR (Järvelin and'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 76}, page_content='7.1. EVALUATION METRICS 73\\nKekäläinen, 2002) (Mean Reciprocal Rank) is another ranking based\\nmetric used for evaluating ranking of reasoning chains (Jansen, 2018;\\nKočisk` yet al., 2018). Daset al.(2019) also use Mean Average Precision\\n(MAP) (Liu and Özsu, 2009) that considers the relative position of the\\nrelevant document in the ranked list (Kadlecet al., 2017).\\n7.1.2 Answer Evaluation\\nBased on the type of the answer, the following evaluation metrics have\\nbeen used for large scale evaluation for the task:\\nMulti-choice questions: MCQ questions are straight-forward to eval-\\nuate when there is a single correct answer, and classification accuracy\\nover the answer choices is used. To deal with multiple correct answer\\ncandidates, Khashabiet al.(2018) proposeF1a and F1m. Precision\\nand Recall are computed by evaluating each predicted answer candidate.\\nMacro harmonic mean of average precision and average recall is the\\nF1m score. F1a uses micro harmonic mean of precision and recall values\\nfor all answer candidates.\\nSpan based answers: The most commonly adopted metrics areExact\\nMatch (EM) and F1 scores on the predicted string tokens. Tang\\net al. (2021) argue that EM can often be too strict and propose an\\nalternative Partial Match (PM)where a predicted answerap is said\\nto be a partial match with the ground truth answerag if either (a)\\nF1(ap,ag) >0.8 or (b)F1(ap,ag) >0.6 and one ofap,ag is a substring\\nof the other. These evaluation metrics are known to work well when the\\nanswer spans are small (<10 tokens).\\nAuxiliary task evaluation: supporting facts:Yanget al.(2018) pro-\\npose to evaluate the reasoning chains by reporting EM and F1 on the\\nsupporting facts (note that this is different from reasoning chain as\\nsupporting facts are sentences and the contexts are passages). They\\nalso propose Joint-EM and Joint-F1 where the precision is defined\\nas Pjoint = Pans ·Psup and the recall asRjoint = Rans ·Rsup. Qiuet al.\\n(2019) compute the score of a reasoning path in the entity graph by'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 77}, page_content='74 How to Evaluate MHQA Systems?\\nmultiplying the corresponding soft masks and attention scores along the\\npath and selecting the top-k scoring paths. If any entity in a supporting\\nfact is reached by any of thek paths, that fact is said to be a hit.\\nEntity-level Supporting fact Prediction (ESP) scores are reported as\\nExact Match (EM) and Recall values over these supporting facts.\\nGenerative answers: For longer sequences of texts, directly matching\\nstrings to give a binary score fails to tell which answers are closer to\\nthe gold answers. Thus, natural language generation (NLG) evaluation\\nmetrics are required. Kočisk` yet al.(2018) propose usingBleu-1, Bleu-\\n4, Meteor (Papineni et al., 2002; Banerjee and Lavie, 2005) and\\nROUGE-L (Lin, 2004) to evaluate predictions on their dataset. Bauer\\net al.(2018) also useCIDer (Vedantamet al., 2014) for evaluating long\\nanswers on NarrativeQA which emphasizes on annotator consensus.\\nThe above discussed evaluation metrics serve the purpose of pro-\\nviding representative scores for large scale evaluation of methods and\\nfacilitating comparison among different techniques. However, multi-hop\\nQA is a complex task and further evaluation experiments designed\\nparticularly for the task might be needed to capture nuances of the\\nmodel. We discuss these in the next section.\\n7.2 Adversarial Evaluation\\nAdversarial evaluation is a commonly used evaluation technique for\\nvarious types of models. It involves testing the models against inten-\\ntionally crafted difficult examples to assess their robustness and expose\\npotential weaknesses or vulnerabilities.\\nAs indicated in Section 3.1, the particular choice of available contexts\\n(C) for a question in the dataset can lead to ‘reasoning shortcuts’\\nwhere a model can correctly answer the question by using only a single\\ncontext. To avoid such shortcuts in the HotpotQA’s distractor setting,\\nthe authors used TF-IDF for retrieving confusing contexts. Minet al.\\n(2019b) collect a different set of distractor paragraphs for the HotpotQA\\ndataset, to evaluate if the models are robust to this change. Same\\nstrategy as Yanget al.(2018) is used while making sure that there is no\\noverlapping distractor paragraph with the original set. An adversarial'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 78}, page_content='7.3. VERIFYING THE EXTENT OF MULTI-HOP REASONING 75\\nset of comparison questions is also created by altering the original\\nquestion so that the correct answer is inverted (for instance, replacing\\n’which is higher’ by ’which is lower’).\\nTuet al.(2020) use a clever technique to add fake distractors that\\ncan fool a model which uses single hop reasoning shortcuts to answer\\nthe questions. A word in the final answer is replaced by another word\\nhaving a similar GloVe embedding to create a fake answer. For instance,\\n‘Mumbai’ is replaced by ‘Delhi’. All occurrences of the word are replaced\\nin the answer passage to get a confusing distractor passage. Since the\\nbridge entity is mentioned in the title of the answer passage, all mentions\\nof a word from the title are also replaced with a similar entity. This\\nis done to break the connection between the fake distractor and the\\nfirst gold context. This ensures that there is only a single reasoning\\nchain and that a model cannot answer the question by only looking at\\nthe answer’s paragraph. Evaluation using the adversarial distractors\\nshows a significant drop in the accuracy of a baseline model. Moreover,\\ntraining using adversarial distractors leads to better performance on\\nthe original distractors as well. Therefore, more confusing distractors\\nwould lead to better training as well as testing of the MHQA models.\\n7.3 Verifying the Extent of Multi-Hop Reasoning\\nDespite the improved scores of models indicated by multiple evaluation\\nmetrics on various datasets, it remains doubtful whether the models\\nare actually performing the multi-hop reasoning and following the\\nexpected reasoning path for reaching the correct answer. Therefore,\\ndifferent evaluation techniques and modifications of existing datasets are\\nproposed as benchmarks for testing the multi-hop reasoning capabilities\\nof a model. In this section, we list various experiments and benchmarks\\nproposed for the same along with their outcomes and conclusions:\\n• In order to evaluate the interpretability of a model, Dinget al.\\n(2019) define theLogical rigorof a model as Joint EM/Ans EM.\\nIntuitively, it tries to measure among the questions that were\\nanswered correctly, what fraction also had correct supporting\\nfacts prediction. Surprisingly, baselines have scores of only 30.3%'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 79}, page_content='76 How to Evaluate MHQA Systems?\\nand 7.9%.\\n• Wang et al. (2019) modify and further annotate HotpotQA to\\nprovide three settings, where the models are provided (1) only\\nthe passage containing the answer, (2) both supporting passages\\nin random order and (3) both supporting passages in the order of\\ntheir occurrence in the reasoning chain, with the intuition that\\na model that employs multi-step reasoning to answer multi-hop\\nquestions should benefit from the supporting passages whereas\\na model that tries to guess the answer directly would instead\\nbe confused by the extra information given. Two common tech-\\nniques, BERT and HotpotReader were tested after employing\\nboth query-reformulating and co-matching approaches (see Sec-\\ntion 6 on taxonomy). It was observed that the models could gain\\nvery little performance (∼1% and 4% accuracy with query refor-\\nmulation and co-matching respectively) by using the reasoning\\nchains provided. This highlights the inability of the existing tech-\\nniques to incorporate multi-hop reasoning to perform MHQA.\\nFurther, it is found that BERT and co-matching show slightly\\nhigher improvements than their respective counter-parts.\\n• Tanget al.(2021) use BERT and DecompRC (Minet al., 2019b) to\\ngenerate single hop sub-questions comprising the 2-hop questions\\nin the HotpotQA dataset and the answers to these questions. The\\nclaim is that if a model employs multi-hop reasoning to answer a\\nquestion, it should trivially be able to answer the individual sub-\\nquestions. Surprisingly, for∼50 −60% of the questions correctly\\nanswered, at least 1 of their corresponding sub-questions could\\nnot be answered correctly. Further, of the questions where both\\nthe sub-questions were answered correctly,∼10% were incorrectly\\nanswered. This indicates that the models tend to jump directly\\nto the answer instead of breaking down the questions into simpler\\nquestions.\\n• Jhamtani and Clark (2020) propose three modifications of the\\nQASC dataset that require the model to explicitly predict the rea-\\nsoning chains along with the final answers (explainable MHQA).'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 80}, page_content='7.3. VERIFYING THE EXTENT OF MULTI-HOP REASONING 77\\ni) eQASC: For each question in QASC, up to 10 candidate rea-\\nsoning chains are automatically generated and each candidate\\nchain is annotated to be valid (if the chain can imply the answer)\\nor invalid (otherwise). ii) eQASC-perturbed: In the candidate\\nchains of QASC, one word/phrase that is likely to be a bridge\\nentity among two facts, is replaced by a similar meaning word\\nensuring that the chain remains to be valid. This is done by crowd\\nsourcing where workers were asked to replace one occurrence of\\nthe word that appears in different sentences of a candidate chain.\\niii) eOBQA: a small number of questions in OpenBookQA are\\nused for generating candidate reasoning chains using sentences\\npresent in QASC and are annotated by crowd-sourcing. This is\\ndone to test the generalization of the model on an unseen dataset.\\n• Trivediet al.(2020) use the term Disconnected Reasoning (DiRe)\\nfor when the model is able to arrive at the correct answer using\\n(possibly multiple independent) incomplete reasoning chains. To\\nmeasure disconnected reasoning, a DiRe probe is created that\\nchecks if the output ofh(q,C \\\\{p1}) and h(q,C \\\\{p2}) (refer\\nto Chapter 2 for notations) can be trivially combined to answer\\nq,C (where ’\\\\’ denotes set difference). To discourage disconnected\\nreasoning, the dataset is modified to include negative samples\\nwhere the givenC is not sufficient to answer the question i.e.,\\nC∩Pq ̸= ϕ and the model is required to identify these questions\\nas unanswerable. When running the DiRe probe, disconnected\\nreasoning is found to be reduced significantly after training using\\nthis modification.\\n• Inoue et al.(2020) argue that requiring the model to only output\\nthe supporting facts might not be enough to ensure the explain-\\nability of the model and the model should be required to also\\noutput the derivation steps. A derivation step is formalized as\\na triplet of the form⟨dh,dr,dt⟩, where dh,dt are entities (noun\\nphrases), anddr is a verb phrase representing a relationship be-\\ntween the two entities. A small subset of the HotpotQA dataset is\\nannotated by crowd-sourcing and released. Evaluating models on\\nthis set indicates the scope of improvement on this benchmark.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 81}, page_content='78 How to Evaluate MHQA Systems?\\nResults of these works are significant as they suggest that improved\\naccuracy on existing datasets may not correlate well with the models’\\nability to perform multi-hop reasoning. Furthermore, they highlight the\\ninefficacy of existing models to perform multi-hop reasoning as well as\\nthe inefficacy of the datasets to evaluate the same. This implies a need\\nfor more carefully created datasets and challenging benchmarks that\\ndo not allow the models to score well without accurately following the\\nrequired reasoning paths. Additionally, it is encouraged to formulate\\nbetter and more such tests/probes that check and prevent the models\\nfrom using loop-holes instead of doing multi-hop reasoning. Above all,\\nit is fair to say that the task of MHQA is far from solved.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 82}, page_content='8\\nMulti-Hop Question Generation\\nThe task of Multi-Hop Question Generation (MHQG) is very closely\\nrelated to MHQA and shares some of the required reasoning and natural\\nlanguage understanding abilities. The task has many applications and\\nhas also received a growing attention in the recent years. Therefore, we\\nadd a brief discussion of MHQG in this chapter.\\nThe goal is to generate a multi-hop question given a set of contexts\\nand optionally, an answer. In chapter 3, we already discussed the\\nchallenges of question generation while creating MHQA datasets. A lot\\nof those challenges directly apply to the task of MHQG as well. MHQG\\nhas widespread applications in multiple domains including education,\\nwhere generating questions that require multiple steps of reasoning can\\nbe very useful for inspiring critical thinking in students (Lindberget al.,\\n2013). QG also has a direct application for chat-bots e.g., in initiating\\nconversations, asking and providing detailed information to the user by\\nconsidering multiple sources of information. MHQG will enhance the\\nability of these chat-bots to ask useful questions (Yaoet al., 2018). It\\ncan also combine with question answering (QA) models as dual tasks\\nto boost QA systems with reasoning ability (Tanget al., 2017).\\nThe task of traditional question generation (QG) has gained a lot\\n79'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 83}, page_content='80 Multi-Hop Question Generation\\nof interest recently (Duet al., 2017; Zhaoet al., 2018b; Scialomet al.,\\n2019). However, MHQG is a more challenging task than simple QG. It\\nrequires the model to first identify scattered pieces of information that\\ncan be aptly combined to form a valid reasoning path from the answer\\nto the question, and then reason over these pieces of information to\\ngenerate a factual and coherent question.\\n8.1 Datasets\\nThe datasets for MHQA can also be used to train and evaluate MHQG\\nmodels by modifying the input and the required output of the model\\n(Su et al., 2020; Guptaet al., 2020). Since HotpotQA has annotated\\nsupporting facts, it is able to provide stronger training supervision and\\nhence, it is the most commonly used dataset for MHQG. Kumaret al.\\n(2019) use the DecompRC model (Minet al., 2019b) to decompose\\neach question in HotpotQA into two sub questions and fine tune a\\nGPT2-small model to rewrite the first question into the second. Yuet al.\\n(2020) use HotpotQA as the labelled dataset and ComplexWebQuestions\\n(Talmor and Berant, 2018) and DROP (Duaet al., 2019) as large corpora\\nfor multi-hop questions.\\n8.2 Evaluation\\nLanguage generation metrics such as BLEU (BLEU1-4), ROUGE-L,\\nMETEOR are usually adopted for MHQG (Kumaret al., 2019; Su\\net al., 2020; Guptaet al., 2020; Sachanet al., 2020; Yuet al., 2020).\\nQBLEU4 (Nema and Khapra, 2018), a QG metric which was shown to\\ncorrelate significantly better with human judgements, is also used for\\nevaluating MHQG (Kumaret al., 2019; Suet al., 2020). The task also\\noften requires human evaluation of fluency, semantics, answerability\\netc of the generated questions. Sachanet al. (2020) also use GLEU\\n(Wu et al., 2016) for their experiments. Another method of evaluating\\nMHQG is to measure the gain in performance of SOTA MHQA models\\nwhen trained with data augmentation using the generated questions\\n(Kumar et al., 2019; Panet al., 2021).'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 84}, page_content='8.3. METHODS 81\\n8.3 Methods\\nKumar et al.(2019) tackle the problem of difficulty controllable question\\ngeneration (DQG)(Gao et al., 2018) by generating questions which\\nrequire a particular number of reasoning hops. The assumption is\\nthat the difficulty of a question directly correlates to the number of\\ninference steps required to answer it. The first step of the proposed\\nalgorithm builds a context graph in the same way as (Fanet al., 2019).\\nAll sentences from the context are converted into triplets of the form\\n{s(subject),r(relation),o(object)}and a relational edge of typer is\\nadded froms to o. Co-reference resolution is used to merge the nodes\\nreferring to the same entity. Next, a nodeN0 is sampled as the final\\nanswer and withN0 as the root, a maximum spanning tree is extracted.\\nFor generating a question with difficulty (same as the number of hops)\\n= d, the tree is pruned to haved+ 1 nodes. A GPT2-small model is\\nfine-tuned on HotpotQA and used to generate an initial questionq0\\nusing N0, N1 and the context sentence connecting the two nodesS1.\\nAnother GPT-2 model is used to rewrite the question iteratively and\\nsuccessively increase the difficulty. In simpler words, for generating a\\nquestion qd with difficulty d, the re-writer model is run onq0 for d\\niterations. While the model performs well for up-to3 hops, the input to\\nthe re-writer model becomes too large for subsequent hops and results\\nin questions with poor quality.\\nSu et al.(2020) leverage a graph constructed similar to Qiuet al.\\n(2019) to generate multi-hop questions. Pre-trained GloVe embeddings\\nand answer tagging embeddings (Lindberget al., 2013) are passed to\\ntwo Bi-LSTM layers to get the initial contextual representations. An\\nattention layer and another Bi-LSTM layer is used to get the answer\\naware context embeddings. An answer aware sub-graph is computed\\nby masking the entities that are irrelevant to the answer and Graph\\nAttention Network is applied to this sub-graph. The context encodings\\nacross the hops are combined via a gated fusion module. The Answer\\nembeddings are updated using bi-attention and The Maxout Pointer\\n(Zhao et al., 2018b) framework is used on top of a Uni-directional LSTM\\ndecoder to generate the question. An additional BFS loss proposed by\\n(Qiu et al., 2019) is found to improve the performance.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 85}, page_content='82 Multi-Hop Question Generation\\nGupta et al. (2020) exploit the presence of supporting facts in\\nHotpotQA while training by adopting an RL reward of the auxiliary\\ntask of supporting facts prediction (SFP). Similar to Suet al.(2020),\\nanswer tagging features are concatenated with the document word\\nembeddings and fed to a Bi-LSTM encoder. The output of the encoder\\nis shared by the MHQG and supporting fact prediction (SFP) models.\\nThe SFP model is a binary classifier trained on HotpotQA to output\\nthe probability of each sentence being a supporting fact. The F1 score\\nbetween the predicted and ground truth supporting facts is added as\\na reward. The REINFORCE algorithm (Williams, 1992) is used with\\nthe self-critical sequence training (Rennieet al., 2017) framework to\\navoid the high variance. In order to make the training more stable, a\\nweight history similar to Rennieet al. (2017) is added. The output\\nprobabilities of the SFP model are also used to update the answer aware\\nencodings by another Bi-LSTM model. For generating the question, a\\nLSTM decoder with global attention mechanism (Luonget al., 2015) is\\nused along with the copy mechanism (Seeet al., 2017a; Gulcehreet al.,\\n2016).\\nSachan et al.(2020) argue that using standard transformers instead\\nof Graph networks should be enough to reason about the relations be-\\ntweenentitiesforformingmulti-hopquestions.Atransformerisextended\\nwith sentence id embeddings and answer token indicator embeddings\\nand trained with an additional contrastive loss as regularization. The\\ncontrastive learning setup assumes supporting fact sentences as positive\\nsamples and others as negative samples and a binary classifier consist-\\ning of a MLP. A significant mismatch in the distribution of question\\nlength over train and dev set of HotpotQA is observed and mitigated\\nby filtering out all the questions that are more than 30 words long.\\nMost of the pruned questions are from the train-easy subset. Both data\\nfiltering and contrastive training are found to boost the performance\\nsignificantly.\\nA novel graph-augmented transformer encoder (GATE) is proposed\\nwhich has two additional layers than the standard transformer encoder\\n(TE): a) Graph-attention sub-layer computes the similarity scores for\\nattention using only the nodes that are connected in a dynamically\\ncreated graph. The graph is a multi-relational graph with three types'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 86}, page_content='8.3. METHODS 83\\nof nodes: named-entity mentions, co-referent-entities and sentence-ids.\\nb) Fused attention sub-layer which uses a MLP with ReLU activation\\nto aggregate the graph-attention embeddings and the TE embeddings.\\nExperiments show that the added layers alone do not improve the\\nperformance significantly whereas an ensemble of TE and GATE does.\\nPanet al.(2021) propose a question generation technique for the task\\nof unsupervised MHQA. Following HotpotQA, their method generates\\ntwo types of questions:\\nBridge questions:Given two contexts as inputs, all entities common\\nto the two contexts are considered as bridge entities. A Google T5 model\\n(Raffel et al., 2019) is fine-tuned on SQuAD to generate two single hop\\nquestions using the answer entity and the bridge entity respectively. The\\nlatter question is converted to a declarative form,s following Demszky\\net al.(2018). The bridge entity in the bridge entity question is replaced\\nwith \"The [MASK] that {s}\" and BERT-Large is used to fill the [MASK].\\nComparison questions: Entities with NER types Nationality, Lo-\\ncation, DateTime and Number are treated as potential comparative\\nproperties. Two single hop questions are generated on two entities of\\nthe same NER type and a pre-defined template is used to combine these\\ninto a multi-hop comparison question.\\nFor generating questions using a table, a GPT-TabGen model (Chen\\net al., 2020a) is used to generate sentences that describe a given entity\\nusing information from the given table. These sentences are then used\\nfor generating eitherbridge or comparison type questions. We refer\\nthe reader to Figure 4 of the original paper (Panet al., 2021) for a\\nbetter understanding of the different types of generated questions. A\\npre-trained GPT-2 model is used to filter questions that are unnatural or\\ndis-fluent. A BART model (Lewiset al., 2019) is also used to paraphrase\\nthe generated questions. The generated questions are then used to train\\nthe model resulting in a zero shot algorithm.\\nYu et al. (2020) aim to tackle MHQG in a low resource setting.\\nSpecifically, the model uses a small amount of labelled dataDL, in the\\nform of(context,answer,question ) triplets, and a large set of multi-\\nhop questionsDU. The idea is to first learn the semantics of multi-hop\\nquestions by training a neural hidden semi-Markov model (Daiet al.,\\n2017) on the unlabelled dataDU. The model uses two latent variables'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 87}, page_content='84 Multi-Hop Question Generation\\nfor parameterizing the similar segments in questions fromDU: a) a state\\nvariable zt, indicating which segment thetth term belongs to, and b)\\na length variablelt, specifying the length of the current segment. The\\nterm probabilities are computed using a GRU decoder followed by an\\nattention layer.\\nThe patterns learned in the first step are used as priors for the QG\\nmodel for regularization. Prior is estimated by sampling a sequence of\\nstates zt having lengthlt. The reasoning chain extraction is similar to\\nas described in Suet al.(2020). For encoding the textual input, BERT\\nembeddings are passed to a bi-GRU followed by an attention layer. The\\ndecoder is another GRU with a copy mechanism (Guet al., 2016) which\\nis regularized to fit the prior pattern. The training loss is the weighted\\nsum of the cross entropy loss and RL policy gradient (Liet al., 2017).\\nThe reward function evaluates a) fluency (following Zhang and Lapata\\n(2017)), b) Answerability (using QBLEU4), and c) Semantics (using\\nWMD).\\nConclusion: The task of multi-hop question generation has gained\\nsome attention of the community and has advanced at a rapid pace.\\nSolving MHQA would go a long way in solving MHQA since we can\\nuse MHQG models to generate high quality large scale datasets for\\nthe development of powerful MHQA models. Further, by dynamically\\nconstructing intricate questions that traverse multiple pieces of informa-\\ntion, we pave the way for more nuanced understanding and exploration\\nof complex knowledge domains, ultimately driving the evolution of\\nAI-driven information retrieval and comprehension.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 88}, page_content='9\\nFuture of MHQA\\nMulti-hop QA has been researched quite extensively in the recent years\\nwith multiple diverse models proposed that aim to model the multi-\\nstep retrieval-reasoning process and achieve promising improvements on\\nexisting datasets and benchmarks. Such systems capable of performing\\nmulti-step reasoning have a variety of applications ranging from chat-\\nbot assistants that are capable of interactive conversations, to search\\nengines that are capable to retrieve results that may be relevant but\\nnot reachable directly from the query text. At the same time the task of\\nMHQA is significantly more challenging than its single hop counterpart.\\nSince paragraphs multiple hops away from the question could share\\nfew common words and little semantic relation with the question (Ding\\net al., 2019), the task to retrieve such contexts is challenging and suffers\\nfrom semantic drift. The ability of pre-LLM models to combine multiple\\ncontexts for reasoning is also limited. Further challenges for solving\\nMHQA is the difficult process of creating datasets that require the\\nmodels to perform multi-hop reasoning, as well as the task of evaluating\\nthe models’ abilities to do so without any hacks. Some challenging\\nbenchmarks and evaluation methods have been recently proposed that\\nbring out some surprising and interesting observations. These results\\n85'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 89}, page_content='86 Future of MHQA\\npoint out to several limitations of existing systems and call for further\\nresearch.\\nBelow, we list and discuss some of these directions for future research\\n(including ones originating from the currently recognized shortcomings)\\nwhich we believe could be promising to be explored.\\n9.1 Flexible Any-Hop Models\\nAs discussed in Section 6, majority of the existing methods for MHQA\\nare either limited to two hops or require the number of hops to be a\\nhyper-parameter. Since natural questions can require any number of\\nreasoning hops, this attribute of existing models is artificially limiting.\\nQA systems should be flexible and robust to the number of hops in a\\nquestion in order to be practically usable. In order to do so, methods\\nfollowing the type III and IV in Figure 4.1 should be explored with a\\ngreater interest since feedback from the answering module can serve as\\na useful stopping criteria.\\n9.2 Explainable Multi-Hop QA\\nDespite the impressive gains in performance on various multi-hop\\ndatasets, it is not evident whether the models are performing the\\nmulti-step reasoning or just guessing the answers. Therefore, following\\nthe release of HotpotQA, a large number of works have focused on\\nexplainable MHQA. Apart from the standard evaluation metrics, vari-\\nous evaluation methods and dataset benchmarks have been proposed\\nto test the explainability of the models which have revealed several\\nsignificant results. Further such benchmarks and evaluation strategies\\nare encouraged to measure and reflect the true progress of models in\\nperforming multi-hop reasoning. While LLM prompting strategies such\\nas CoT prompting make the task naturally explainable, LLMs suffer\\nfrom hallucinations which lowers their trust for such applications.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 90}, page_content='9.3. BETTER DATASETS 87\\n9.3 Better Datasets\\nMany works have highlighted the limitations of existing MHQA datasets.\\nChen and Durrett (2019) show experimentally that multi-choice ques-\\ntions are easier to hack by models, regardless of number of answer\\ncandidates being small or very large. Similarly, Minet al.(2019b) show\\nthat the distractor setting, even with as many as 500 distractors, is\\neasier to hack for single-hop QA models. Yanget al.(2018) and Zhang\\net al.(2020) argue that datasets that are created using KBs suffer from\\nlack of diversity in question and answer types. Following these observa-\\ntions, it is encouraged that the future datasets are in the open-domain\\nsetting, have questions with either span-based or generative answers,\\nand do not rely completely on the structure of existing KBs.\\nDas et al.(2019), Xionget al.(2019), Minet al.(2019b) and Min\\net al.(2019a) find that a significant portion of questions in the existing\\ndatasets are single-hop solvable due to a variety of reasons. One of these\\nreasons is that the source of the questions is same as the set of contexts.\\nTherefore datasets like (Welblet al., 2018; Kočisk` yet al., 2018; Wang\\net al., 2021b; Trischleret al., 2016; Choiet al., 2018) that use separate\\nsources for generating and answering questions are encouraged. However,\\nattention needs to be given to the cases where there is a discrepancy\\nbetween what is mentioned in the two sources (Fanget al., 2020).\\nMost of the existing works have focused on datasets with MCQ or\\nquestions with span answers and more focus on the more challenging\\nproblem of generative MHQA is desirable.\\n9.4 Better Evaluation Metrics\\nAs discussed in Section 7, a variety of evaluation metrics have been\\nused for evaluating MHQA models. However, existing metrics face some\\nchallenges and might not be sufficient for evaluating MHQA. Since\\nMHQA is a more complex task compared to single-hop QA, more\\nmetrics specific to MHQA are encouraged. One promising direction is\\nto perform per-hop evaluation and accumulate the per-hop scores to\\nget a final score. This kind of evaluation would require the models to\\nbe explainable as well as interpretable.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 91}, page_content='88 Future of MHQA\\nSome challenges are common to both single-hop and multi-hop QA\\nevaluation metrics. For instance, while evaluating span based answers,\\nmetrics based on lexical matching would markU.S. as incorrect when\\nthe gold answer isUnited States(Fang et al., 2020). Therefore, eval-\\nuation metrics should be able to deal with synonyms when matching\\nthe answers. Another issue might be a metric giving a score of 0 to\\nthe answer U.K. when the gold answer is London. These cases are\\nfrequent in datasets like WikiHop where the sources of question and\\nthe contexts are different (De Caoet al., 2019) and might have answers\\nmentioned with different granularity. Therefore, it might be useful to\\ngive some partial score for the answer being geographically close or for\\nthe answer having a coarser granularity. Similarly, answeringJanuary,\\n1989 should earn some score if the gold answer isDecember, 1988due\\nto the predicted answer being temporally close to the gold answer.\\nSome partial score can also be rewarded to answers having a coarser\\ngranularity (December, 1988vs 1988). On similar lines, using hypernym\\nrelations from ConceptNet or WordNet (Miller, 1995) for evaluating\\nthe answer can be a possible direction.\\nFollowing a similar reasoning, the evaluation should match the\\nsemantics of the answer rather than the lexical overlap. Therefore,\\nevaluation metrics like word mover similarity (Kusneret al., 2015)\\nor sentence mover similarity (Clarket al., 2019) that perform soft\\nmatching over embeddings might be a promising direction. Since the\\nevaluation of language generation tasks are widely known to have a\\nscope of improvement, evaluation of generative MHQA is also an open\\nproblem and new evaluation techniques are encouraged.\\n9.5 Methods to Incorporate Commonsense\\nA hop in multi-hop reasoning can be performed using some retrieved\\ncontext, where the context may either be retrieved from the corpus or\\nfrom the commonsense knowledge. Fanget al. (2020) find that16%\\nof the failures of their model were caused by missing commonsense\\nbackground knowledge. Baueret al.(2018) propose a novel method for\\nincorporating commonsense for MHQA that shows impressive results.\\nMore techniques for exploiting the rich commonsense knowledge bases'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 92}, page_content='9.6. ARITHMETIC QUESTIONS 89\\nto perform multi-step reasoning can be a promising direction to explore.\\n9.6 Arithmetic Questions\\nThe inability of QA systems to perform arithmetic operations is well\\nknown (Patelet al., 2021; Hendryckset al., 2021; Schubotzet al., 2018)\\nand this inability is exacerbated in the multi-hop setting. Minet al.\\n(2019a) observe that45% of the comparison questions in HotpotQA are\\nnumerical questions. Qiuet al.(2019), Fanget al.(2020), and Minet al.\\n(2019b) find that their model is unable to give a correct answer when\\nthe query is a comparison between two dates “February 20, 1959” and\\n“February 10, 1967”. Arithmetic calculation may also be required for\\nnon-comparison type questions. For example, answering “Who was the\\npresident of USA in 1994” from the context “Bill Clinton: 1993-2001”\\nrequires some arithmetic computation. Wanget al.(2021a) approached\\nthis kind of temporal problems by computing the overlap of content\\ntime expressions that occur in text with the computed question’s time\\nscope using kernel density estimate. Another example is (Wanget al.,\\n2021c) that answers “when” type questions by predicting the event\\ndates based on the analysis of multi-variate time series derived from\\nunderlying news collection. Even powerful LLMs have been shown to\\nperform poorly with arithmetic reasoning (Gaoet al., 2023; Maltoni\\nand Ferrara, 2024) and multiple methods are being proposed for the\\nsame (Imaniet al., 2023; Guo, 2023; Maltoni and Ferrara, 2024; Gao\\net al., 2023)\\nMulti-hop QA systems that are capable of solving arithmetic com-\\nparisons and computations would greatly enhance accuracy of MHQA.\\nSimilarly, it is observed that some particular types of questions\\n(temporal, geographical, count) are more challenging than others (Ding\\net al., 2019; Zhang et al., 2021; Min et al., 2019b; De Cao et al.,\\n2019). Figure 3 in Zhanget al.(2021) shows the complexity and model\\nperformance on different types of questions in HotpotQA. Targeting the\\nmore challenging questions specifically could also lead to better MHQA\\nsystems.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 93}, page_content='90 Future of MHQA\\n9.7 Better Incorporation of Powerful LLMs\\nLLMs have been widely adopted due to their performance exceeding\\nexpectations for most tasks. Existing works employ LLMs for some\\ncomponents of the task to boost performance. The MHQA community\\nwould benefit from keeping up with the rapid developments of LLMs,\\nincorporating the advancements in the models’ abilities and efficiency.\\nConsequently, developing more robust and powerful LLMs suited to\\nmulti-step reasoning would greatly help the development of MHQA\\nsystems\\n9.8 Conclusion\\nAs we conclude, acknowledging both the strengths and weaknesses of\\nexisting data, models, and evaluation methods in multi-hop QA provides\\na solid foundation for charting future research paths. By leveraging\\ninsights gained from these limitations, we can propel advancements\\ntowards more robust, adaptable, and comprehensive question answering\\nsystems, shaping the landscape of AI-driven knowledge exploration for\\nyears to come.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 94}, page_content='Appendices'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 95}, page_content='A\\nBackground\\nA.1 BM25\\nBM25 is a ranking function used to retrieve documents given a search\\nquery. BM25 stands stands forBest Match 251. It uses a bag-of-words\\nmechanism to score proximity between the search query and the docu-\\nments. Given a queryQ= q1,q2,...,q n, whereqi denotes a keyword in\\nthe queryQ, the BM25 score of the documentD is defined as follows -\\nBM25(D,Q) =\\nn∑\\ni=1\\nIDF(qi). freq(qi,D).(k1 + 1)\\nfreq(qi,D) + k1.(1 −b+ b. |D|\\navg.doc.len.)\\n(A.1)\\nwhere freq(qi,D) is the number of timesqi occurs in D, |.|denotes\\nthe number of words in D,avg.doc.len. denotes the average number of\\nwords in the document,k1 and b are free parameters2, and IDF(qi)\\ndenotes the inverse document frequency weight of query term usually\\n1BM25 is also known as Okapi BM25, which was used first by the Okapi\\ninformation retrieval system implemented by London’s City University (https://en.\\nwikipedia.org/wiki/Okapi_BM25).\\n2Typically k1 ∈ [1.2, 2.0] and b = 0.75.\\n92'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 96}, page_content='A.2. RECURRENT NEURAL NETWORKS 93\\ncomputed as follows -\\nIDF(qi) = ln(N −n(qi) + 0.5\\nn(qi) + 0.5 ) + 1 (A.2)\\nwhere N is the total number of documents in the collection, andn(qi)\\nis the number of documents containingqi.\\nEven though the technique was devised in 1970s-80s, BM25 and its\\nvariations are still widely adopted for document retrieval, especially\\nwhen the document corpus is very large and usingdense retrievers3 has\\na big computational overhead.\\nA.2 Recurrent Neural Networks\\nRecurrent Neural Networks (RNNs) are a class of artificial neural\\nnetworks that have loop connections that allow information propagation\\nacross time through the same neurons. Prior to transformer networks\\n(Vaswaniet al., 2017b), RNNs were the most popular framework class\\nto process sequential information, and are still widely adopted in real-\\nworld systems. Most practical RNN-based architectures have additional\\nstored states that allow the vanilla RNN architecture to overcome its\\nshortcoming of short-term memory loss. Gated recurrent units (GRU)\\ncells (Cho et al., 2014) and long short term memory (LSTM) cells\\n(Hochreiter and Schmidhuber, 1997b) are two of the most popular\\nstateful RNN cells that use gated mechanism to handle long term\\nmemory. (Seeet al., 2017b) proposed a pointer generator network to\\novercome the over-repetition of RNN generated output using coverage\\nloss. We point the readers to the comprehensive survey of recurrent\\nneural networks by (Liptonet al., 2015) for extensive explanation on\\nthe topic.\\nA.3 Transformers for Language Modeling\\nEven the advanced RNN models like LSTMs and GRUs have a tough\\ntime dealing with long sequences. Luonget al.(2015) introduced the\\n3Dense retriever is a general umbrella term used to refer to the neural network\\nbased retrieval systems.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 97}, page_content='94 Background\\nattention mechanism which allows the model to focus on certain parts\\nof the input when predicting a particular output token. Doing so signif-\\nicantly helps with tasks like machine translation where certain words of\\nthe input sequence are directly related to a word in the output sequence.\\nMany forms of attention have since been used effectively for various\\ntasks.\\nVaswaniet al.(2017a) extended the idea of attention by removing\\nthe recurrent component of the model altogether and proposed the\\ntransformer model where both the encoder and the decoder consist of\\nseveral self-attention and feed forward layers. The transformer model\\nalso introduced the multi-head attention. These components allows\\nfor very large models which can have a lot more parameters without\\ncomprising on the performance. Transformers are also proved to be very\\nversatile, having great success in a large number of natural language\\napplications.\\nWhile the original transformers model was trained using the next-\\ntoken prediction task implying the unidirectionality of the encoder\\nmodel, BERT (Kenton and Toutanova, 2019) was a bidirectional en-\\ncoder based transformer which was trained using the masked language\\nmodeling task. BERT has proved to be a versatile model and the word\\nrepresentations learned using BERT have been used as embeddings for\\nalmost all natural language tasks.\\nSuccess of transformer models including BERT led to their use as\\nlarge pre-training models and several models like AlBERT (Lanet al.,\\n2019), RoBERTa (Liuet al., 2019) and GPT were proposed. AlBERT\\nuses parameter reduction techniques which allow for smaller and faster\\ntraining of the BERT models while achieving a similar level of accuracy\\nas BERT. RoBERTa is a much more robustly optimized version of\\nBERT, trained with optimized design and hyperparameters choices,\\nwhich could significantly outperform the originally trained BERT model.\\nPre-training of large language models (LLMs) has become increas-\\ningly popular leading to larger and larger models trained on huge corpora\\nof natural language. The different versions of the model follow the same\\nprinciple, with GPT-1 having 117 million parameters and GPT-4 having\\nabout a 100 trillion parameters. GPTs are trained on huge corpora using\\nthe next token prediction task. An extensively detailed explanation of'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 98}, page_content='A.4. GRAPH NEURAL NETWORKS 95\\ndifferent architectures and training techniques for transformer based\\nmodels is neither feasible nor in the scope for this work. Therefore, we\\npoint the readers to the comprehensive survey of transformers by (Lin\\net al., 2022) for further details on the topic.\\nA.4 Graph Neural Networks\\nGraphs are a very simple and versatile method of representing data and\\nits inherent structure. Neural Networks could be adapted to incorporate\\nthis structure leading to Graph Neural Networks (GNNs). GNNs can be\\nadopted for various different types of data and tasks, leading to several\\nimprovements increasing their capabilities. The integral part of all these\\nmodels is the message passing algorithm briefly explained below.\\nGiven a graphG= (V,E) having n= |V|nodes, the representation\\nof each node is updated following the given steps:\\n• Initialization: The representation of every nodev is initialized\\nas h0\\nv = Xv, whereXv is the feature vector.\\n• Update: For each layeri, the representations of each nodev is\\nupdated as:\\nhi\\nv = σuϵN(v)(WiΣ hi−1\\nu\\nN(v) + Uihi−1\\nv ) (A.3)\\nwhere σ is the activation function,Wi and Ui are the weight\\nmatrices corresponding to the layer i and N(v) is the set of\\nneighbouring nodes of the nodev.\\n• Prediction: The representations after layerK are passed to a\\nlinear network for the eventual prediction task.\\nAt every layer, the representation of nodevis updated with an activation\\napplied to the weighted average of representations of the nodes directly\\nconnected to v. Therefore, afterk layers, the nodev is supposed to\\nreceive the ‘message’ from all nodes having a path tov of length≤k.\\nThe weighted average also ensures that the nodes that are closer tov\\nin the graph end up affecting its representation more.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 99}, page_content='96 Background\\nA layer of a Graph Convolutional Network (GCN) (Kipf and Welling,\\n2016b) consists of a GNN layer followed by a Linear layer. Relation\\nGCN (R-GCN) (Schlichtkrullet al., 2017) allow for different kinds of\\nedges by having different weight matrices for nodes connected tov via\\ndifferent kind of edges. Graph Attention Networks (GAN) (Veličković\\net al., 2018) incorporate self attention into GNNs by using the attention\\nweights while performing the message passing algorithm. Several other\\nmodifications of GNNs are proposed for different tasks.\\nWe point the readers to the comprehensive survey of graph neural\\nnetworks by (Wuet al., 2020) for further reading on the topic.\\nA.5 Large Language models\\nLanguage modelsrefer to a class of self-supervised NLP models that\\nare trained on large unlabeled datasets to learn to predict the likelihood\\nof a word or sequence of words occurring based on the context provided\\nby the preceding words. This ability to estimate the probability of a\\nword given its context forms the foundation of language modeling. These\\nmodels undergo training on various tasks, such as next-word prediction\\n(Brown et al., 2020), masked language modeling (the task of predicting\\nrandomly missing tokens), and next-sentence prediction (Kenton and\\nToutanova, 2019), without the need for labeled data. Due to their\\nreliance on extensive training data, language models develop a strong\\ngrasp of underlying language patterns and concepts. Generally, language\\nmodels are not designed for specific tasks and can be fine-tuned with\\nminimal data for various downstream applications. Extensive research\\nhas shown that utilizing large language models (LLMs) pre-trained on\\nvast amounts of data yields impressive results in language understanding\\nand generation tasks (Tanet al., 2023; Wanget al., 2021d; Hendyet al.,\\n2023; Blair-Stanek et al., 2023). The advent of transformer models\\nhas made it possible to train such highly advanced language models,\\nresulting in popular models like BERT, T5, and GPT-3 (Kenton and\\nToutanova, 2019; Raffelet al., 2019; Brownet al., 2020).'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 100}, page_content='A.5. LARGE LANGUAGE MODELS 97\\nA.5.1 Generative Pre-trained Transformer (GPT)\\nGPT, a series of generative pre-trained large language models (Brownet\\nal., 2020), is characterized by its decoder-only transformer architecture.\\nUnlike other transformer models that have both encoder and decoder\\nblocks, GPT models consist solely of decoder blocks, eliminating the\\nencoder-decoder cross-attention layer from each block. The different\\nversions of GPT, namely GPT, GPT-2, GPT-3, and GPT-4, vary in\\nterms of model size and training data. For example, GPT-3 has 175\\nbillion model parameters and is trained on a massive corpus of 499\\nbillion tokens, while GPT-2 has 1.5 billion parameters and is trained\\non a dataset of 10 billion tokens.\\nA.5.2 Prompting GPT-3\\nGPT-3 has achieved remarkable success in various downstream natural\\nlanguage tasks, including question answering (Tanet al., 2023), Machine\\nTranslation (Hendyet al., 2023) and Entailment prediction (Wanget al.,\\n2021d), with minimal supervision required. During a typical run of the\\nmodel, an incomplete piece of text is provided as a ‘prompt’, and the\\nmodel iteratively generates the most likely tokens to complete the text.\\nThis prompting technique has demonstrated impressive performance\\nin the zero-shot setting, where the model is not provided with any\\nin-context examples and is expected to predict the correct output for\\nthe given question in the prompt (Figure A.1).\\nOn the other hand, few-shot prompting (Fei-Feiet al., 2006) involves in-\\ncluding a small number of sample input-output pairs within the prompt\\nas references for the model (Figure A.1). The inclusion of a few refer-\\nence examples provides valuable guidance to the model, allowing it to\\ngenerate more accurate and relevant responses.\\nIn their work, Weiet al. (2023) introduced the concept of chain-of-\\nthought (CoT) prompting, which goes a step beyond simply providing\\ninput-sample output pairs. CoT prompting includes a coherent sequence\\nof reasoning steps that gradually build up to the correct answer. By pre-\\nsenting the model with a step-by-step thought process, CoT prompting\\noffers explicit examples of how to arrive at the correct answer based on\\nthe given input facts. This method is particularly valuable for tackling'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 101}, page_content='98 Background\\nFigure A.1: Example of zero-shot (top), few-shot (middle), and CoT (bottom)\\nprompting for the same question.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 102}, page_content='A.5. LARGE LANGUAGE MODELS 99\\ncomplex tasks that demand multiple layers of reasoning including the\\ntask that this study focuses on. Figure A.1 shows examples of zero-shot,\\nfew-shot, and CoT prompts for an arithmetic question. Here, the prompt\\nconsists of 2 in-context examples is 2.\\nFor further background and details, we refer the readers to the\\ncomprehensive survey on LLMs by Zhaoet al.(2023b)'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 103}, page_content='References\\nAinslie, J., S. Ontanon, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A.\\nRavula, S. Sanghai, Q. Wang, and L. Yang. (2020). “ETC: Encoding\\nLong and Structured Inputs in Transformers”. In:Proceedings of\\nthe 2020 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP). Online: Association for Computational Lin-\\nguistics. 268–284. doi: 10.18653/v1/2020.emnlp-main.19. url:\\nhttps://aclanthology.org/2020.emnlp-main.19.\\nAllam, A. M. N. and M. H. Haggag. (2012). “The question answering\\nsystems: A survey”.International Journal of Research and Reviews\\nin Information Sciences (IJRRIS). 2(3).\\nAlvarez-Melis, D. and T. Jaakkola. (2017). “A causal framework for ex-\\nplaining the predictions of black-box sequence-to-sequence models”.\\nIn: Proceedings of the 2017 Conference on Empirical Methods in\\nNatural Language Processing. Copenhagen, Denmark: Association\\nfor Computational Linguistics. 412–421.doi: 10.18653/v1/D17-1042.\\nurl: https://aclanthology.org/D17-1042.\\nArras, L., F. Horn, G. Montavon, K.-R. Müller, and W. Samek. (2016).\\n“What is Relevant in a Text Document?\": An Interpretable Machine\\nLearning Approach. CoRR abs/1612.07843 (2016)”.arXiv preprint\\narXiv:1612.07843.\\n100'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 104}, page_content='REFERENCES 101\\nBalepur, N., J. Huang, S. Moorjani, H. Sundaram, and K. C.-C. Chang.\\n(2023). “Mastering the ABCDs of Complex Questions: Answer-\\nBasedClaimDecompositionforFine-grainedSelf-Evaluation”.arXiv:\\n2305.14750 [cs.CL].\\nBanarescu, L., C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Herm-\\njakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider. (2013).\\n“Abstract Meaning Representation for Sembanking”. In:Proceedings\\nof the 7th Linguistic Annotation Workshop and Interoperability with\\nDiscourse. Ed. by A. Pareja-Lora, M. Liakata, and S. Dipper. Sofia,\\nBulgaria: Association for Computational Linguistics. 178–186.url:\\nhttps://aclanthology.org/W13-2322.\\nBanerjee, S. and A. Lavie. (2005). “METEOR: An Automatic Met-\\nric for MT Evaluation with Improved Correlation with Human\\nJudgments”. In:Proceedings of the ACL Workshop on Intrinsic and\\nExtrinsic Evaluation Measures for Machine Translation and/or Sum-\\nmarization. Ann Arbor, Michigan: Association for Computational\\nLinguistics. 65–72.url: https://aclanthology.org/W05-0909.\\nBarzilay, R., K. McKeown, and M. Elhadad. (1999). “Information fusion\\nin the context of multi-document summarization”. In:Proceedings\\nof the 37th annual meeting of the Association for Computational\\nLinguistics. 550–557.\\nBauer, L., Y. Wang, and M. Bansal. (2018). “Commonsense for Gen-\\nerative Multi-Hop Question Answering Tasks”. In:Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language\\nProcessing. 4220–4230.\\nBecker, R., F. Corò, G. D’Angelo, and H. Gilbert. (2020). “Balancing\\nSpreads of Influence in a Social Network”.Proceedings of the AAAI\\nConference on Artificial Intelligence. 34(Apr.): 3–10.doi: 10.1609/\\naaai.v34i01.5327. url: https://ojs.aaai.org/index.php/AAAI/\\narticle/view/5327.\\nBhagavatula, C. S., T. Noraset, and D. Downey. (2013). “Methods\\nfor exploring and mining tables on wikipedia”. In:Proceedings of\\nthe ACM SIGKDD workshop on interactive data exploration and\\nanalytics. 18–26.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 105}, page_content='102 REFERENCES\\nBiran, O. and C. Cotton. (2017). “Explanation and justification in\\nmachine learning: A survey”. In:IJCAI-17 workshop on explainable\\nAI (XAI). Vol. 8. No. 1. 8–13.\\nBlair-Stanek, A., N. Holzenberger, and B. V. Durme. (2023). “Can\\nGPT-3 Perform Statutory Reasoning?” arXiv: 2302.06100[cs.CL].\\nBoros, E., J. G. Moreno, and A. Doucet. (2021). “Event Detection as\\nQuestionAnsweringwithEntityInformation”. CoRR.abs/2104.06969.\\narXiv: 2104.06969.url: https://arxiv.org/abs/2104.06969.\\nBouziane, A., D. Bouchiha, N. Doumi, and M. Malki. (2015). “Question\\nanswering systems: survey and trends”.Procedia Computer Science.\\n73: 366–375.\\nBowman, S. R., G. Angeli, C. Potts, and C. D. Manning. (2015). “A\\nlarge annotated corpus for learning natural language inference”. In:\\nConference on Empirical Methods in Natural Language Processing,\\nEMNLP 2015. Association for Computational Linguistics (ACL).\\n632–642.\\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-\\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M.\\nLitwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A.\\nRadford, I. Sutskever, and D. Amodei. (2020). “Language Models\\nare Few-Shot Learners”. In:Advances in Neural Information Pro-\\ncessing Systems. Ed. by H. Larochelle, M. Ranzato, R. Hadsell, M.\\nBalcan, and H. Lin. Vol. 33. Curran Associates, Inc. 1877–1901.\\nurl: https://proceedings.neurips.cc/paper_files/paper/2020/file/\\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\\nCao, X. and Y. Liu. (2021). “Coarse-grained decomposition and fine-\\ngrained interaction for multi-hop question answering”.Journal of\\nIntelligent Information Systems: 1–21.\\nCao, Y., M. Fang, and D. Tao. (2019). “BAG: Bi-directional Atten-\\ntion Entity Graph Convolutional Network for Multi-hop Reasoning\\nQuestion Answering”. In:Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers). 357–362.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 106}, page_content='REFERENCES 103\\nChen, D., A. Fisch, J. Weston, and A. Bordes. (2017a). “Reading\\nWikipedia to Answer Open-Domain Questions”. In:Proceedings of\\nthe 55th Annual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers). Vancouver, Canada: Association\\nfor Computational Linguistics. 1870–1879.doi: 10.18653/v1/P17-\\n1171. url: https://aclanthology.org/P17-1171.\\nChen,J.andG.Durrett.(2019).“UnderstandingDatasetDesignChoices\\nfor Multi-hop Reasoning”. In:Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers). Minneapolis, Minnesota: Association for Computa-\\ntional Linguistics. 4026–4032.doi: 10.18653/v1/N19-1405. url:\\nhttps://aclanthology.org/N19-1405.\\nChen, J., S.-t. Lin, and G. Durrett. (2019). “Multi-hop question answer-\\ning via reasoning chains”.arXiv preprint arXiv:1910.02610.\\nChen, Q., X. Zhu, Z.-H. Ling, S. Wei, H. Jiang, and D. Inkpen. (2017b).\\n“Enhanced LSTM for Natural Language Inference”. In:Proceedings\\nof the 55th Annual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers). Vancouver, Canada: Association\\nfor Computational Linguistics. 1657–1668.doi: 10.18653/v1/P17-\\n1152. url: https://aclanthology.org/P17-1152.\\nChen, W., J. Chen, Y. Su, Z. Chen, and W. Y. Wang. (2020a). “Logical\\nNatural Language Generation from Open-Domain Tables”.CoRR.\\nabs/2004.10404. arXiv: 2004.10404.url: https://arxiv.org/abs/\\n2004.10404.\\nChen, W., H. Zha, Z. Chen, W. Xiong, H. Wang, and W. Y. Wang.\\n(2020b). “HybridQA: A Dataset of Multi-Hop Question Answering\\nover Tabular and Textual Data”. In:Findings of the Association for\\nComputational Linguistics: EMNLP 2020. 1026–1036.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 107}, page_content='104 REFERENCES\\nChen, Z., W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R.\\nMoussa, M. Beane, T.-H. Huang, B. Routledge, and W. Y. Wang.\\n(2021). “FinQA: A Dataset of Numerical Reasoning over Financial\\nData”. In:Proceedings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing. Online and Punta Cana, Dominican\\nRepublic: Association for Computational Linguistics. 3697–3711.\\ndoi: 10.18653/v1/2021.emnlp-main.300.url: https://aclanthology.\\norg/2021.emnlp-main.300.\\nCho, K., B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk,\\nand Y. Bengio. (2014). “Learning phrase representations using RNN\\nencoder-decoder for statistical machine translation”. In:Conference\\non Empirical Methods in Natural Language Processing (EMNLP\\n2014).\\nChoi, E., H. He, M. Iyyer, M. Yatskar, W. Yih, Y. Choi, P. Liang, and\\nL. Zettlemoyer. (2018). “QuAC : Question Answering in Context”.\\nCoRR. abs/1808.07036. arXiv: 1808.07036.url: http://arxiv.org/\\nabs/1808.07036.\\nChung, H. W., L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai,\\nM. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu,\\nV. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J.\\nDevlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. (2022). “Scaling\\nInstruction-Finetuned Language Models”.doi: 10.48550/ARXIV.\\n2210.11416. url: https://arxiv.org/abs/2210.11416.\\nChurch, K. W. and P. Hanks. (1990). “Word Association Norms, Mutual\\nInformation, and Lexicography”.Computational Linguistics. 16(1):\\n22–29. url: https://aclanthology.org/J90-1003.\\nClark, E., A. Celikyilmaz, and N. A. Smith. (2019). “Sentence mover’s\\nsimilarity: Automatic evaluation for multi-sentence texts”. In:Pro-\\nceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics. 2748–2760.\\nClark, P., I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord. (2018). “Think you have solved question answering?\\ntryarc,theai2reasoningchallenge”. arXiv preprint arXiv:1803.05457.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 108}, page_content='REFERENCES 105\\nCobbe, K., V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J.\\nSchulman.(2021).“TrainingVerifierstoSolveMathWordProblems”.\\narXiv: 2110.14168[cs.LG].\\nDai, H., B. Dai, Y. Zhang, S. Li, and L. Song. (2017). “Recurrent Hidden\\nSemi-Markov Model”. In:ICLR.\\nDas, R., A. Godbole, D. Kavarthapu, Z. Gong, A. Singhal, M. Yu, X.\\nGuo, T. Gao, H. Zamani, M. Zaheer,et al.(2019). “Multi-step entity-\\ncentric information retrieval for multi-hop question answering”. In:\\nProceedings of the 2nd Workshop on Machine Reading for Question\\nAnswering. 113–118.\\nDasigi, P., K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner.\\n(2021). “A Dataset of Information-Seeking Questions and Answers\\nAnchored in Research Papers”. In:Proceedings of the 2021 Confer-\\nence of the North American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies. Online: Asso-\\nciation for Computational Linguistics. 4599–4610.doi: 10.18653/\\nv1/2021.naacl-main.365. url: https://aclanthology.org/2021.naacl-\\nmain.365.\\nDe Cao, N., W. Aziz, and I. Titov. (2019). “Question Answering by Rea-\\nsoning Across Documents with Graph Convolutional Networks”. In:\\nProceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 2306–2317.\\nDemszky, D., K. Guu, and P. Liang. (2018). “Transforming Question\\nAnswering Datasets Into Natural Language Inference Datasets”.\\nCoRR. abs/1809.02922. arXiv: 1809.02922.url: http://arxiv.org/\\nabs/1809.02922.\\nDeng, Z., Y. Zhu, Y. Chen, M. Witbrock, and P. Riddle. (2022). “Inter-\\npretable AMR-Based Question Decomposition for Multi-hop Ques-\\ntion Answering”. In:Proceedings of the Thirty-First International\\nJoint Conference on Artificial Intelligence, IJCAI-22. Ed. by L. D.\\nRaedt. International Joint Conferences on Artificial Intelligence\\nOrganization. 4093–4099.doi: 10.24963/ijcai.2022/568.url: https:\\n//doi.org/10.24963/ijcai.2022/568.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 109}, page_content='106 REFERENCES\\nDiefenbach, D., V. Lopez, K. Singh, and P. Maret. (2018). “Core tech-\\nniquesofquestionansweringsystemsoverknowledgebases:asurvey”.\\nKnowledge and Information systems. 55(3): 529–569.\\nDimitrakis, E., K. Sgontzos, and Y. Tzitzikas. (2020). “A survey on ques-\\ntion answering systems over linked data and documents”.Journal\\nof intelligent information systems. 55(2): 233–259.\\nDing, M., C. Zhou, Q. Chen, H. Yang, and J. Tang. (2019). “Cogni-\\ntive Graph for Multi-Hop Reading Comprehension at Scale”. In:\\nProceedings of the 57th Annual Meeting of the Association for Com-\\nputational Linguistics. Florence, Italy: Association for Computa-\\ntional Linguistics. 2694–2703.doi: 10.18653/v1/P19-1259. url:\\nhttps://aclanthology.org/P19-1259.\\nDu, X., J. Shao, and C. Cardie. (2017). “Learning to Ask: Neural Ques-\\ntion Generation for Reading Comprehension”. In:Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguis-\\ntics (Volume 1: Long Papers). Vancouver, Canada: Association for\\nComputational Linguistics. 1342–1352.doi: 10.18653/v1/P17-1123.\\nurl: https://aclanthology.org/P17-1123.\\nDua, D., C. dos Santos, P. Ng, B. Athiwaratkun, B. Xiang, M. Gardner,\\nand S. Singh. (2021). “Generative Context Pair Selection for Multi-\\nhop Question Answering”. In:Proceedings of the 2021 Conference\\non Empirical Methods in Natural Language Processing. 7009–7015.\\nDua, D., S. Singh, and M. Gardner. (2020). “Benefits of Intermediate\\nAnnotations in Reading Comprehension”. In:Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics.\\nOnline: Association for Computational Linguistics. 5627–5634.doi:\\n10.18653/v1/2020.acl-main.497. url: https://aclanthology.org/2020.\\nacl-main.497.\\nDua, D., Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner.\\n(2019). “DROP: A Reading Comprehension Benchmark Requiring\\nDiscrete Reasoning Over Paragraphs”. In:Proceedings of the 2019\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume\\n1 (Long and Short Papers). Minneapolis, Minnesota: Association for\\nComputational Linguistics. 2368–2378.doi: 10.18653/v1/N19-1246.\\nurl: https://aclanthology.org/N19-1246.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 110}, page_content='REFERENCES 107\\nEtezadi, R. and M. Shamsfard. (2023). “The state of the art in open\\ndomain complex question answering: a survey”.Applied Intelligence.\\n53(4): 4124–4144.\\nFan, A., C. Gardent, C. Braud, and A. Bordes. (2019). “Using Lo-\\ncal Knowledge Graph Construction to Scale Seq2Seq Models to\\nMulti-Document Inputs”. In:Proceedings of the 2019 Conference\\non Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). Hong Kong, China: Association for Computa-\\ntional Linguistics. 4186–4196.doi: 10.18653/v1/D19-1428. url:\\nhttps://aclanthology.org/D19-1428.\\nFang, Y., S. Sun, Z. Gan, R. Pillai, S. Wang, and J. Liu. (2020).\\n“Hierarchical Graph Network for Multi-hop Question Answering”.\\nIn: Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP). 8823–8838.\\nFei-Fei, L., R. Fergus, and P. Perona. (2006). “One-Shot Learning\\nof Object Categories”.IEEE transactions on pattern analysis and\\nmachine intelligence. 28(May): 594–611.doi: 10.1109/TPAMI.2006.\\n79.\\nFeldman, Y. and R. El-Yaniv. (2019). “Multi-Hop Paragraph Retrieval\\nfor Open-Domain Question Answering”. In:Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics.\\nFlorence, Italy: Association for Computational Linguistics. 2296–\\n2309. doi: 10.18653/v1/P19-1222.url: https://aclanthology.org/\\nP19-1222.\\nFeng, S., W. Shi, Y. Bai, V. Balachandran, T. He, and Y. Tsvetkov.\\n(2024a). “Knowledge Card: Filling LLMs’ Knowledge Gaps with\\nPlug-in Specialized Language Models”. arXiv: 2305.09955[cs.CL].\\nFeng, S., W. Shi, Y. Wang, W. Ding, V. Balachandran, and Y. Tsvetkov.\\n(2024b). “Don’t Hallucinate, Abstain: Identifying LLM Knowledge\\nGaps via Multi-LLM Collaboration”. arXiv: 2402.00367[cs.CL].\\nFeng, Y., M. Yu, W. Xiong, X. Guo, J. Huang, S. Chang, M. Campbell,\\nM. Greenspan, and X. Zhu. (2020). “Learning to recover reasoning\\nchains for multi-hop question answering via cooperative games”.\\narXiv preprint arXiv:2004.02393.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 111}, page_content='108 REFERENCES\\nFu, B., Y. Qiu, C. Tang, Y. Li, H. Yu, and J. Sun. (2020). “A survey on\\ncomplex question answering over knowledge base: Recent advances\\nand challenges”.arXiv preprint arXiv:2007.13069.\\nGao, L., A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and\\nG. Neubig. (2023). “PAL: Program-aided Language Models”. arXiv:\\n2211.10435 [cs.CL].\\nGao, Y., J. Wang, L. Bing, I. King, and M. R. Lyu. (2018). “Difficulty\\nControllable Question Generation for Reading Comprehension”.\\nCoRR. abs/1807.03586. arXiv: 1807.03586.url: http://arxiv.org/\\nabs/1807.03586.\\nGatt, A. and E. Krahmer. (2018). “Survey of the state of the art in nat-\\nural language generation: Core tasks, applications and evaluation”.\\nJournal of Artificial Intelligence Research. 61: 65–170.\\nGeva, M., Y. Goldberg, and J. Berant. (2019a). “Are We Modeling the\\nTask or the Annotator? An Investigation of Annotator Bias in Natu-\\nral Language Understanding Datasets”. In:Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP). Hong Kong, China: Association for\\nComputational Linguistics. 1161–1166.doi: 10.18653/v1/D19-1107.\\nurl: https://aclanthology.org/D19-1107.\\nGeva, M., E. Malmi, I. Szpektor, and J. Berant. (2019b). “DiscoFuse:\\nA Large-Scale Dataset for Discourse-Based Sentence Fusion”. In:\\nProceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). 3443–3455.\\nGhalandari, D. G. and G. Ifrim. (2020). “Examining the state-of-the-art\\nin news timeline summarization”.arXiv preprint arXiv:2005.10107.\\nGilpin, L. H., D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal.\\n(2018). “Explaining explanations: An overview of interpretability of\\nmachine learning”. In:2018 IEEE 5th International Conference on\\ndata science and advanced analytics (DSAA). IEEE. 80–89.\\nGoldstein, J., V. O. Mittal, J. G. Carbonell, and M. Kantrowitz.\\n(2000). “Multi-document summarization by sentence extraction”. In:\\nNAACL-ANLP 2000 workshop: automatic summarization.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 112}, page_content='REFERENCES 109\\nGraves, A., G. Wayne, and I. Danihelka. (2014). “Neural turing ma-\\nchines”. arXiv preprint arXiv:1410.5401.\\nGu, J., Z. Lu, H. Li, and V. O. Li. (2016). “Incorporating Copying\\nMechanism in Sequence-to-Sequence Learning”. In:Proceedings of\\nthe 54th Annual Meeting of the Association for Computational Lin-\\nguistics (Volume 1: Long Papers). Berlin, Germany: Association for\\nComputational Linguistics. 1631–1640.doi: 10.18653/v1/P16-1154.\\nurl: https://aclanthology.org/P16-1154.\\nGulcehre, C., S. Ahn, R. Nallapati, B. Zhou, and Y. Bengio. (2016).\\n“Pointing the Unknown Words”. In:Proceedings of the 54th Annual\\nMeeting of the Association for Computational Linguistics (Volume\\n1: Long Papers). Berlin, Germany: Association for Computational\\nLinguistics. 140–149. doi: 10.18653/v1/P16-1014. url: https:\\n//aclanthology.org/P16-1014.\\nGuo, Y. (2023). “ArthModel: Enhance Arithmetic Skills to Large Lan-\\nguage Model”. arXiv: 2311.18609[cs.CL].\\nGupta, D., H. Chauhan, R. T. Akella, A. Ekbal, and P. Bhattacharyya.\\n(2020). “Reinforced Multi-task Approach for Multi-hop Question\\nGeneration”. In:Proceedings of the 28th International Conference\\non Computational Linguistics. 2760–2775.\\nGururangan, S., S. Swayamdipta, O. Levy, R. Schwartz, S. Bowman,\\nand N. A. Smith. (2018). “Annotation Artifacts in Natural Language\\nInference Data”. In:Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 2 (Short Papers). New Or-\\nleans, Louisiana: Association for Computational Linguistics. 107–112.\\ndoi: 10.18653/v1/N18-2017. url: https://aclanthology.org/N18-\\n2017.\\nHaghighi, A. and L. Vanderwende. (2009). “Exploring content models for\\nmulti-document summarization”. In:Proceedings of human language\\ntechnologies: The 2009 annual conference of the North American\\nChapter of the Association for Computational Linguistics. 362–370.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 113}, page_content='110 REFERENCES\\nHaji, S., K. Suekane, H. Sano, and T. Takagi. (2023). “Exploratory\\nInference Chain: Exploratorily Chaining Multi-hop Inferences with\\nLarge Language Models for Question-Answering”. In:2023 IEEE\\n17th International Conference on Semantic Computing (ICSC). 175–\\n182. doi: 10.1109/ICSC56153.2023.00036.\\nHan, S., H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, L. Benson, L.\\nSun, E. Zubova, Y. Qiao, M. Burtell, D. Peng, J. Fan, Y. Liu, B.\\nWong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang, S. Joty,\\nA. R. Fabbri, W. Kryscinski, X. V. Lin, C. Xiong, and D. Radev.\\n(2022). “FOLIO: Natural Language Reasoning with First-Order\\nLogic”. arXiv: 2209.00840[cs.CL].\\nHe, P., X. Liu, J. Gao, and W. Chen. (2021). “DeBERTa: Decoding-\\nenhanced BERT with Disentangled Attention”. arXiv: 2006.03654\\n[cs.CL].\\nHendrycks, D., C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D.\\nSong, and J. Steinhardt. (2021). “Measuring mathematical problem\\nsolving with the math dataset”.arXiv preprint arXiv:2103.03874.\\nHendy, A., M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Mat-\\nsushita, Y. J. Kim, M. Afify, and H. H. Awadalla. (2023). “How\\nGood Are GPT Models at Machine Translation? A Comprehensive\\nEvaluation”. arXiv: 2302.09210[cs.CL].\\nHermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M.\\nSuleyman, and P. Blunsom. (2015). “Teaching machines to read and\\ncomprehend”. Advances in neural information processing systems.\\n28.\\nHochreiter, S. and J. Schmidhuber. (1997a). “Long short-term memory”.\\nNeural computation. 9(8): 1735–1780.\\nHochreiter, S. and J. Schmidhuber. (1997b). “Long short-term memory”.\\nNeural computation. 9(8): 1735–1780.\\nHöffner, K., S. Walter, E. Marx, R. Usbeck, J. Lehmann, and A.-C.\\nNgonga Ngomo. (2017). “Survey on challenges of question answering\\nin the semantic web”.Semantic Web. 8(6): 895–920.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 114}, page_content='REFERENCES 111\\nHuang, Y. and M. Yang. (2021). “Breadth First Reasoning Graph for\\nMulti-hop Question Answering”. In:Proceedings of the 2021 Con-\\nference of the North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies. Online: As-\\nsociation for Computational Linguistics. 5810–5821.doi: 10.18653/\\nv1/2021.naacl-main.464. url: https://aclanthology.org/2021.naacl-\\nmain.464.\\nImani, S., L. Du, and H. Shrivastava. (2023). “MathPrompter: Mathe-\\nmatical Reasoning using Large Language Models”. arXiv: 2303.05398\\n[cs.CL].\\nInoue, N., P. Stenetorp, and K. Inui. (2020). “R4C: A Benchmark for\\nEvaluating RC Systems to Get the Right Answer for the Right Rea-\\nson”. In:Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics. Online: Association for Computa-\\ntional Linguistics. 6740–6750.doi: 10.18653/v1/2020.acl-main.602.\\nurl: https://aclanthology.org/2020.acl-main.602.\\nJansen, P. (2018). “Multi-hop Inference for Sentence-level TextGraphs:\\nHow Challenging is Meaningfully Combining Information for Science\\nQuestion Answering?” In:Proceedings of the Twelfth Workshop on\\nGraph-Based Methods for Natural Language Processing (TextGraphs-\\n12). 12–17.\\nJansen, P., E. Wainwright, S. Marmorstein, and C. Morrison. (2018).\\n“WorldTree: A Corpus of Explanation Graphs for Elementary Sci-\\nence Questions supporting Multi-hop Inference”. In:Proceedings of\\nthe Eleventh International Conference on Language Resources and\\nEvaluation (LREC 2018). Miyazaki, Japan: European Language\\nResources Association (ELRA).url: https://aclanthology.org/L18-\\n1433.\\nJärvelin, K. and J. Kekäläinen. (2002). “Cumulated Gain-Based Eval-\\nuation of IR Techniques”.ACM Trans. Inf. Syst.20(4): 422–446.\\nissn: 1046-8188.doi: 10.1145/582415.582418.url: https://doi.org/\\n10.1145/582415.582418.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 115}, page_content='112 REFERENCES\\nJatowt, A., C. A. Yeung, and K. Tanaka. (2013). “Estimating document\\nfocus time”. In:22nd ACM International Conference on Information\\nand Knowledge Management, CIKM’13, San Francisco, CA, USA,\\nOctober 27 - November 1, 2013. Ed. by Q. He, A. Iyengar, W. Nejdl,\\nJ. Pei, and R. Rastogi. ACM. 2273–2278.doi: 10.1145/2505515.\\n2505655. url: https://doi.org/10.1145/2505515.2505655.\\nJhamtani, H. and P. Clark. (2020). “Learning to Explain: Datasets and\\nModels for Identifying Valid Reasoning Chains in Multihop Question-\\nAnswering”. In:Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP). Online: Associa-\\ntion for Computational Linguistics. 137–150.doi: 10.18653/v1/2020.\\nemnlp-main.10. url: https://aclanthology.org/2020.emnlp-main.10.\\nJia, R. and P. Liang. (2017). “Adversarial Examples for Evaluating\\nReading Comprehension Systems”. In:Proceedings of the 2017 Con-\\nference on Empirical Methods in Natural Language Processing. 2021–\\n2031.\\nJin, Q., Z. Yuan, G. Xiong, Q. Yu, H. Ying, C. Tan, M. Chen, S. Huang,\\nX. Liu, and S. Yu. (2022). “Biomedical Question Answering: A\\nSurvey of Approaches and Challenges”.ACM Computing Surveys\\n(CSUR). 55(2): 1–36.\\nJoshi, N., H. Zhang, K. Kalyanaraman, Z. Hu, K. Chellapilla, H. He,\\nand L. E. Li. (2023a). “Improving Multi-Hop Reasoning in LLMs by\\nLearning from Rich Human Feedback”. In:Neuro-Symbolic Learning\\nand Reasoning in the era of Large Language Models. url: https:\\n//openreview.net/forum?id=wxfqhp9bNR.\\nJoshi, N., H. Zhang, K. Kalyanaraman, Z. Hu, K. Chellapilla, H. He,\\nand L. E. Li. (2023b). “Improving Multi-Hop Reasoning in LLMs by\\nLearning from Rich Human Feedback”. In:Neuro-Symbolic Learning\\nand Reasoning in the era of Large Language Models. url: https:\\n//openreview.net/forum?id=wxfqhp9bNR.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 116}, page_content='REFERENCES 113\\nKadavath, S., T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez,\\nN. Schiefer, Z. Hatfield-Dodds, N. DasSarma, E. Tran-Johnson, S.\\nJohnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen,\\nY. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson,\\nJ. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer,\\nD. Amodei, T. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish,\\nC. Olah, and J. Kaplan. (2022). “Language Models (Mostly) Know\\nWhat They Know”. arXiv: 2207.05221[cs.CL].\\nKadlec, R., O. Bajgar, and J. Kleindienst. (2017). “Knowledge Base\\nCompletion: Baselines Strike Back”. In: Proceedings of the 2nd\\nWorkshop on Representation Learning for NLP. Vancouver, Canada:\\nAssociation for Computational Linguistics. 69–74.doi: 10.18653/\\nv1/W17-2609. url: https://aclanthology.org/W17-2609.\\nKenton, J. D. M.-W. C. and L. K. Toutanova. (2019). “BERT: Pre-\\ntraining of Deep Bidirectional Transformers for Language Under-\\nstanding”. In:Proceedings of NAACL-HLT. 4171–4186.\\nKhashabi, D., S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth.\\n(2018). “Looking beyond the surface: A challenge set for reading\\ncomprehension over multiple sentences”. In:Proceedings of the 2018\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume\\n1 (Long Papers). 252–262.\\nKhot, T., P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal. (2020).\\n“Qasc: A dataset for question answering via sentence composition”.\\nIn: Proceedings of the AAAI Conference on Artificial Intelligence.\\nVol. 34. No. 05. 8082–8090.\\nKhot, T., A. Sabharwal, and P. Clark. (2019). “What’s Missing: A\\nKnowledge Gap Guided Approach for Multi-hop Question Answer-\\ning”. In:Proceedings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP).\\n2814–2828.\\nKhot, T., H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark,\\nand A. Sabharwal. (2023). “Decomposed Prompting: A Modular\\nApproach for Solving Complex Tasks”. arXiv: 2210.02406[cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 117}, page_content='114 REFERENCES\\nKim, J.-H., J. Jun, and B.-T. Zhang. (2018). “Bilinear attention net-\\nworks”. Advances in Neural Information Processing Systems. 31.\\nKipf, T. N. and M. Welling. (2016a). “Semi-Supervised Classification\\nwith Graph Convolutional Networks”.CoRR. abs/1609.02907. arXiv:\\n1609.02907. url: http://arxiv.org/abs/1609.02907.\\nKipf, T. N. and M. Welling. (2016b). “Semi-Supervised Classification\\nwith Graph Convolutional Networks”.CoRR. abs/1609.02907. arXiv:\\n1609.02907. url: http://arxiv.org/abs/1609.02907.\\nKočisk` y, T., J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette. (2018). “The narrativeqa reading comprehen-\\nsion challenge”.Transactions of the Association for Computational\\nLinguistics. 6: 317–328.\\nKovaleva, O., A. Romanov, A. Rogers, and A. Rumshisky. (2019).\\n“Revealing the Dark Secrets of BERT”. In:Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP). Hong Kong, China: Association for\\nComputational Linguistics. 4365–4374.doi: 10.18653/v1/D19-1445.\\nurl: https://aclanthology.org/D19-1445.\\nKumar, V., Y. Hua, G. Ramakrishnan, G. Qi, L. Gao, and Y.-F. Li.\\n(2019). “Difficulty-controllable multi-hop question generation from\\nknowledge graphs”. In: International Semantic Web Conference.\\nSpringer. 382–398.\\nKurdi, G., J. Leo, B. Parsia, U. Sattler, and S. Al-Emari. (2019). “A\\nSystematic Review of Automatic Question Generation for Educa-\\ntional Purposes”.International Journal of Artificial Intelligence in\\nEducation. 30(Nov.).doi: 10.1007/s40593-019-00186-y.\\nKusner, M., Y. Sun, N. Kolkin, and K. Weinberger. (2015). “From word\\nembeddings to document distances”. In:International conference\\non machine learning. PMLR. 957–966.\\nLan, Y., G. He, J. JIANG, J. JIANG, W. X. ZHAO, and J.-R. WEN.\\n(2021). “A survey on complex knowledge base question answering:\\nMethods, challenges and solutions”. In: IJCAI.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 118}, page_content='REFERENCES 115\\nLan, Z., M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut.\\n(2019). “ALBERT: A Lite BERT for Self-supervised Learning of Lan-\\nguage Representations”. In:International Conference on Learning\\nRepresentations.\\nLebanoff, L., J. Muchovej, F. Dernoncourt, D. S. Kim, S. Kim, W.\\nChang, and F. Liu. (2019). “Analyzing Sentence Fusion in Abstrac-\\ntive Summarization”. In:Proceedings of the 2nd Workshop on New\\nFrontiers in Summarization. 104–110.\\nLei, F., X. Li, Y. Wei, S. He, Y. Huang, J. Zhao, and K. Liu. (2023).\\n“S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid\\nQuestion Answering”. arXiv: 2305.11725[cs.CL].\\nLewis, M., Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O.\\nLevy, V. Stoyanov, and L. Zettlemoyer. (2019). “BART: Denoising\\nSequence-to-Sequence Pre-training for Natural Language Gener-\\nation, Translation, and Comprehension”.CoRR. abs/1910.13461.\\narXiv: 1910.13461.url: http://arxiv.org/abs/1910.13461.\\nLi, J., M. Ren, Y. Gao, and Y. Yang. (2023). “Ask to Understand:\\nQuestion Generation for Multi-hop Question Answering”. In:Chi-\\nnese Computational Linguistics. Ed. by M. Sun, B. Qin, X. Qiu,\\nJ. Jing, X. Han, G. Rao, and Y. Chen. Singapore: Springer Nature\\nSingapore. 19–36.isbn: 978-981-99-6207-5.\\nLi, J., W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky. (2017).\\n“Adversarial Learning for Neural Dialogue Generation”. In:Pro-\\nceedings of the 2017 Conference on Empirical Methods in Natural\\nLanguage Processing. Copenhagen, Denmark: Association for Com-\\nputational Linguistics. 2157–2169.doi: 10.18653/v1/D17-1230.url:\\nhttps://aclanthology.org/D17-1230.\\nLi, R. and X. Du. (2023). “Leveraging Structured Information for\\nExplainable Multi-hop Question Answering and Reasoning”. arXiv:\\n2311.03734 [cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 119}, page_content='116 REFERENCES\\nLiang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan,\\nB. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Ré, D. Acosta-\\nNavas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F.\\nRong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng,\\nM. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chatterji, O.\\nKhattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar,\\nS. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W.\\nWang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda. (2023). “Holistic\\nEvaluation of Language Models”. arXiv: 2211.09110[cs.CL].\\nLin, C.-Y. (2004). “ROUGE: A Package for Automatic Evaluation\\nof Summaries”. In:Text Summarization Branches Out. Barcelona,\\nSpain: Association for Computational Linguistics. 74–81.url: https:\\n//aclanthology.org/W04-1013.\\nLin, T., Y. Wang, X. Liu, and X. Qiu. (2022). “A survey of transformers”.\\nAI Open. 3: 111–132.issn: 2666-6510.doi: https://doi.org/10.1016/\\nj.aiopen.2022.10.001. url: https://www.sciencedirect.com/science/\\narticle/pii/S2666651022000146.\\nLin, Z., D. Zhang, Q. Tac, D. Shi, G. Haffari, Q. Wu, M. He, and Z.\\nGe. (2021). “Medical Visual Question Answering: A Survey”.arXiv\\npreprint arXiv:2111.10056.\\nLindberg, D., F. Popowich, J. Nesbit, and P. Winne. (2013). “Gener-\\nating Natural Language Questions to Support Learning On-Line”.\\nIn: Proceedings of the 14th European Workshop on Natural Lan-\\nguage Generation. Sofia, Bulgaria: Association for Computational\\nLinguistics. 105–114.url: https://aclanthology.org/W13-2114.\\nLipton, Z. C., J. Berkowitz, and C. Elkan. (2015). “A critical review\\nof recurrent neural networks for sequence learning”.arXiv preprint\\narXiv:1506.00019.\\nLiu, L. and M. T. Özsu. (2009). “Mean average precision”.Encyclopedia\\nof Database Systems2009. 1703.\\nLiu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V. Stoyanov. (2019). “Roberta: A robustly opti-\\nmized bert pretraining approach”.arXiv preprint arXiv:1907.11692.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 120}, page_content='REFERENCES 117\\nLuong, T., H. Pham, and C. D. Manning. (2015). “Effective Approaches\\nto Attention-based Neural Machine Translation”. In:Proceedings\\nof the 2015 Conference on Empirical Methods in Natural Language\\nProcessing. Lisbon, Portugal: Association for Computational Lin-\\nguistics. 1412–1421. doi: 10.18653/v1/D15-1166. url: https:\\n//aclanthology.org/D15-1166.\\nMa, C., W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng. (2020).\\n“Multi-document summarization via deep learning techniques: A\\nsurvey”. arXiv preprint arXiv:2011.04843.\\nMadaan, A., N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U.\\nAlon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder,\\nK. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. (2023).\\n“Self-Refine: Iterative Refinement with Self-Feedback”. arXiv: 2303.\\n17651 [cs.CL].\\nMalon, C. and B. Bai. (2020). “Generating followup questions for inter-\\npretablemulti-hopquestionanswering”. arXiv preprint arXiv:2002.12344.\\nMaltoni, D. and M. Ferrara. (2024). “Arithmetic with Language Models:\\nfrom Memorization to Computation”. arXiv: 2308.01154[cs.AI].\\nMavi, V., A. Saparov, and C. Zhao. (2023). “Retrieval-Augmented\\nChain-of-Thought in Semi-structured Domains”. In:Proceedings of\\nthe Natural Legal Language Processing Workshop 2023. Ed. by D.\\nPreot, iuc-Pietro, C. Goanta, I. Chalkidis, L. Barrett, G. ( Spanakis,\\nand N. Aletras. Singapore: Association for Computational Linguis-\\ntics. 178–191. doi: 10.18653/v1/2023.nllp-1.18. url: https:\\n//aclanthology.org/2023.nllp-1.18.\\nMelo, F. (2013). “Area under the ROC Curve”. In:Encyclopedia of\\nSystems Biology. Ed. by W. Dubitzky, O. Wolkenhauer, K.-H. Cho,\\nand H. Yokota. New York, NY: Springer New York. 38–39.isbn:\\n978-1-4419-9863-7. doi: 10.1007/978-1-4419-9863-7_209. url:\\nhttps://doi.org/10.1007/978-1-4419-9863-7_209.\\nMihaylov, T., P. Clark, T. Khot, and A. Sabharwal. (2018). “Can a\\nSuit of Armor Conduct Electricity? A New Dataset for Open Book\\nQuestion Answering”. In:Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing. 2381–2391.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 121}, page_content='118 REFERENCES\\nMiller, G. A. (1995). “WordNet: A Lexical Database for English”.Com-\\nmun. ACM. 38(11): 39–41.issn: 0001-0782.doi: 10.1145/219717.\\n219748. url: https://doi.org/10.1145/219717.219748.\\nMin, S., J. Michael, H. Hajishirzi, and L. Zettlemoyer. (2020). “Am-\\nbigQA:Answeringambiguousopen-domainquestions”. arXiv preprint\\narXiv:2004.10645.\\nMin, S., E. Wallace, S. Singh, M. Gardner, H. Hajishirzi, and L. Zettle-\\nmoyer. (2019a). “Compositional Questions Do Not Necessitate Multi-\\nhop Reasoning”. In:Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics. Florence, Italy: Associa-\\ntion for Computational Linguistics. 4249–4257.doi: 10.18653/v1/\\nP19-1416. url: https://aclanthology.org/P19-1416.\\nMin, S., V. Zhong, R. Socher, and C. Xiong. (2018). “Efficient and\\nRobust Question Answering from Minimal Context over Documents”.\\nIn: Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers). Melbourne,\\nAustralia: Association for Computational Linguistics. 1725–1735.\\ndoi: 10.18653/v1/P18-1160. url: https://aclanthology.org/P18-\\n1160.\\nMin, S., V. Zhong, L. Zettlemoyer, and H. Hajishirzi. (2019b). “Multi-\\nhop Reading Comprehension through Question Decomposition and\\nRescoring”. In:Proceedings of the 57th Annual Meeting of the Associ-\\nation for Computational Linguistics. Florence, Italy: Association for\\nComputational Linguistics. 6097–6109.doi: 10.18653/v1/P19-1613.\\nurl: https://aclanthology.org/P19-1613.\\nMishra, A. and S. K. Jain. (2016). “A survey on question answering sys-\\ntems with classification”.Journal of King Saud University-Computer\\nand Information Sciences. 28(3): 345–361.\\nNair,I.,S.Somasundaram,A.Saxena,andK.Goswami.(2023).“Drilling\\nDown into the Discourse Structure with LLMs for Long Document\\nQuestion Answering”. arXiv: 2311.13565[cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 122}, page_content='REFERENCES 119\\nNallapati, R., B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang. (2016).\\n“Abstractive Text Summarization using Sequence-to-sequence RNNs\\nand Beyond”. In:Proceedings of the 20th SIGNLL Conference on\\nComputational Natural Language Learning. Ed. by S. Riezler and\\nY. Goldberg. Berlin, Germany: Association for Computational Lin-\\nguistics. 280–290. doi: 10.18653/v1/K16-1028. url: https://\\naclanthology.org/K16-1028.\\nNayeem, M. T., T. A. Fuad, and Y. Chali. (2018). “Abstractive unsuper-\\nvised multi-document summarization using paraphrastic sentence\\nfusion”. In: Proceedings of the 27th International Conference on\\nComputational Linguistics. 1191–1204.\\nNema, P. and M. M. Khapra. (2018). “Towards a Better Metric for\\nEvaluating Question Generation Systems”. In:Proceedings of the\\n2018 Conference on Empirical Methods in Natural Language Process-\\ning. Brussels, Belgium: Association for Computational Linguistics.\\n3950–3959. doi: 10.18653/v1/D18-1429.url: https://aclanthology.\\norg/D18-1429.\\nNovikova, J., O. Dušek, A. Cercas Curry, and V. Rieser. (2017). “Why\\nWe Need New Evaluation Metrics for NLG”. In:Proceedings of\\nthe 2017 Conference on Empirical Methods in Natural Language\\nProcessing. Copenhagen, Denmark: Association for Computational\\nLinguistics. 2241–2252. doi: 10.18653/v1/D17-1238. url: https:\\n//aclanthology.org/D17-1238.\\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F.\\nKelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,\\nJ. Leike, and R. Lowe. (2022). “Training language models to follow\\ninstructions with human feedback”. arXiv: 2203.02155[cs.CL].\\nPan, L., W. Chen, W. Xiong, M.-Y. Kan, and W. Y. Wang. (2021).\\n“Unsupervised Multi-hop Question Answering by Question Genera-\\ntion”. In:Proceedings of the 2021 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human\\nLanguage Technologies. 5866–5880.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 123}, page_content='120 REFERENCES\\nPapineni, K., S. Roukos, T. Ward, and W.-J. Zhu. (2002). “Bleu: a\\nmethod for automatic evaluation of machine translation”. In:Pro-\\nceedings of the 40th annual meeting of the Association for Computa-\\ntional Linguistics. 311–318.\\nPatel, A., S. Bhattamishra, and N. Goyal. (2021). “Are NLP Models\\nreally able to Solve Simple Math Word Problems?” In:Proceedings\\nof the 2021 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies.\\n2080–2094.\\nPatel, P., S. Mishra, M. Parmar, and C. Baral. (2022). “Is a Question\\nDecomposition Unit All We Need?” arXiv: 2205.12538[cs.CL].\\nPennington, J., R. Socher, and C. D. Manning. (2014). “Glove: Global\\nvectors for word representation”. In:Proceedings of the 2014 confer-\\nence on empirical methods in natural language processing (EMNLP).\\n1532–1543.\\nPeters, M. E., M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer. (2018). “Deep Contextualized Word Representa-\\ntions”. In:Proceedings of the 2018 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers). New Orleans,\\nLouisiana: Association for Computational Linguistics. 2227–2237.\\ndoi: 10.18653/v1/N18-1202. url: https://aclanthology.org/N18-\\n1202.\\nQi,P.,X.Lin,L.Mehr,Z.Wang,andC.D.Manning.(2019).“Answering\\nComplex Open-domain Questions Through Iterative Query Genera-\\ntion”. In:Proceedings of the 2019 Conference on Empirical Methods\\nin Natural Language Processing and the 9th International Joint Con-\\nference on Natural Language Processing (EMNLP-IJCNLP). Hong\\nKong, China: Association for Computational Linguistics. 2590–2602.\\ndoi: 10.18653/v1/D19-1261. url: https://aclanthology.org/D19-\\n1261.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 124}, page_content='REFERENCES 121\\nQiu, L., Y. Xiao, Y. Qu, H. Zhou, L. Li, W. Zhang, and Y. Yu. (2019).\\n“Dynamically Fused Graph Network for Multi-hop Reasoning”. In:\\nProceedings of the 57th Annual Meeting of the Association for Com-\\nputational Linguistics. Florence, Italy: Association for Computa-\\ntional Linguistics. 6140–6150.doi: 10.18653/v1/P19-1617. url:\\nhttps://aclanthology.org/P19-1617.\\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,et\\nal. (2019). “Language models are unsupervised multitask learners”.\\nOpenAI blog. 1(8): 9.\\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, and P. J. Liu. (2019). “Exploring the Limits of\\nTransfer Learning with a Unified Text-to-Text Transformer”.CoRR.\\nabs/1910.10683. arXiv: 1910.10683.url: http://arxiv.org/abs/1910.\\n10683.\\nRahgouy, M., H. B. Giglou, D. Feng, T. Rahgooy, G. Dozier, and C. D.\\nSeals. (2023). “Navigating the Fermi Multiverse: Assessing LLMs\\nfor Complex Multi-hop Queries”. In:\\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. (2016a). “SQuAD:\\n100,000+ Questions for Machine Comprehension of Text”. In:Pro-\\nceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing. Austin, Texas: Association for Computational\\nLinguistics. 2383–2392. doi: 10.18653/v1/D16-1264. url: https:\\n//aclanthology.org/D16-1264.\\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. (2016b). “SQuAD:\\n100,000+ Questions for Machine Comprehension of Text”. In:Pro-\\nceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing. 2383–2392.\\nRennie, S. J., E. Marcheret, Y. Mroueh, J. Ross, and V. Goel. (2017).\\n“Self-Critical Sequence Training for Image Captioning”. In:Pro-\\nceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR).'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 125}, page_content='122 REFERENCES\\nRoy, R. S. and A. Anand. (2021).Question Answering for the Curated\\nWeb: Tasks and Methods in QA over Knowledge Bases and Text\\nCollections. Synthesis Lectures on Information Concepts, Retrieval,\\nand Services. Morgan & Claypool Publishers.isbn: 978-3-031-79511-\\n4. doi: 10.2200/S0113ED1V01Y202109ICR076. url: https://doi.\\norg/10.2200/S0113ED1V01Y202109ICR076.\\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. (1985). “Learning\\ninternal representations by error propagation”.Tech. rep.California\\nUniv San Diego La Jolla Inst for Cognitive Science.\\nSachan, D. S., L. Wu, M. Sachan, and W. Hamilton. (2020). “Stronger\\nTransformers for Neural Multi-Hop Question Generation”.arXiv\\npreprint arXiv:2010.11374.\\nSachan, M. and E. Xing. (2016). “Easy Questions First? A Case Study\\non Curriculum Learning for Question Answering”. In:Proceedings\\nof the 54th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). Berlin, Germany: Association\\nfor Computational Linguistics. 453–463.doi: 10.18653/v1/P16-1043.\\nurl: https://aclanthology.org/P16-1043.\\nSaeidi, M., M. Bartolo, P. Lewis, S. Singh, T. Rocktäschel, M. Sheldon,\\nG. Bouchard, and S. Riedel. (2018). “Interpretation of Natural\\nLanguage Rules in Conversational Machine Reading”. In:EMNLP.\\nSamek, W., T. Wiegand, and K.-R. Müller. (2017). “Explainable artifi-\\ncial intelligence: Understanding, visualizing and interpreting deep\\nlearning models”.arXiv preprint arXiv:1708.08296.\\nSaparov, A. and H. He. (2023). “Language Models Are Greedy Rea-\\nsoners: A Systematic Formal Analysis of Chain-of-Thought”. In:\\nThe Eleventh International Conference on Learning Representations.\\nurl: https://openreview.net/forum?id=qFVVBzXxR2V.\\nSchlichtkrull, M., T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov,\\nand M. Welling. (2017). “Modeling Relational Data with Graph\\nConvolutional Networks (2017)”.Preprint.\\nSchubotz, M., P. Scharpf, K. Dudhat, Y. Nagar, F. Hamborg, and B.\\nGipp. (2018). “Introducing mathqa: a math-aware question answer-\\ning system”.Information Discovery and Delivery.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 126}, page_content='REFERENCES 123\\nScialom, T., B. Piwowarski, and J. Staiano. (2019). “Self-Attention\\nArchitectures for Answer-Agnostic Neural Question Generation”.\\nIn: Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics. Florence, Italy: Association for Compu-\\ntational Linguistics. 6027–6032.doi: 10.18653/v1/P19-1604.url:\\nhttps://aclanthology.org/P19-1604.\\nSee, A., P. J. Liu, and C. D. Manning. (2017a). “Get To The Point:\\nSummarization with Pointer-Generator Networks”. In:Proceedings\\nof the 55th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers). 1073–1083.\\nSee, A., P. J. Liu, and C. D. Manning. (2017b). “Get to the point:\\nSummarization with pointer-generator networks”.arXiv preprint\\narXiv:1704.04368.\\nSeo, M. J., A. Kembhavi, A. Farhadi, and H. Hajishirzi. (2016). “Bidirec-\\ntionalAttentionFlowforMachineComprehension”. CoRR.abs/1611.01603.\\narXiv: 1611.01603.url: http://arxiv.org/abs/1611.01603.\\nShao, N., Y. Cui, T. Liu, S. Wang, and G. Hu. (2020). “Is Graph Struc-\\nture Necessary for Multi-hop Question Answering?” In:Proceedings\\nof the 2020 Conference on Empirical Methods in Natural Language\\nProcessing (EMNLP). 7187–7192.\\nShao, N., Y. Cui, T. Liu, S. Wang, and G. Hu. (2021). “Memory\\naugmented sequential paragraph retrieval for multi-hop question\\nanswering”. arXiv preprint arXiv:2102.03741.\\nShi, Q., H. Cui, H. Wang, Q. Zhu, W. Che, and T. Liu. (2024). “Ex-\\nploring Hybrid Question Answering via Program-based Prompting”.\\narXiv: 2402.10812[cs.CL].\\nSidiropoulos, G., N. Voskarides, S. Vakulenko, and E. Kanoulas. (2021a).\\n“Combining Lexical and Dense Retrieval for Computationally Effi-\\ncient Multi-hop Question Answering”. In:Proceedings of the Second\\nWorkshop on Simple and Efficient Natural Language Processing. 58–\\n63.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 127}, page_content='124 REFERENCES\\nSidiropoulos, G., N. Voskarides, S. Vakulenko, and E. Kanoulas. (2021b).\\n“Combining Lexical and Dense Retrieval for Computationally Effi-\\ncient Multi-hop Question Answering”. In:Proceedings of the Second\\nWorkshop on Simple and Efficient Natural Language Processing.\\nVirtual: Association for Computational Linguistics. 58–63. doi:\\n10.18653/v1/2021.sustainlp-1.7. url: https://aclanthology.org/2021.\\nsustainlp-1.7.\\nSlobodkin, A., O. Goldman, A. Caciularu, I. Dagan, and S. Ravfo-\\ngel. (2023). “The Curious Case of Hallucinatory (Un)answerability:\\nFinding Truths in the Hidden States of Over-Confident Large Lan-\\nguage Models”. In:Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing. Ed. by H. Bouamor, J.\\nPino, and K. Bali. Singapore: Association for Computational Lin-\\nguistics. 3607–3625.doi: 10.18653/v1/2023.emnlp-main.220.url:\\nhttps://aclanthology.org/2023.emnlp-main.220.\\nSoares, M. A. C. and F. S. Parreiras. (2020). “A literature review on\\nquestion answering techniques, paradigms and systems”.Journal of\\nKing Saud University-Computer and Information Sciences. 32(6):\\n635–646.\\nSong, L., Z. Wang, M. Yu, Y. Zhang, R. Florian, and D. Gildea. (2018).\\n“Exploring graph-structured passage representation for multi-hop\\nreading comprehension with graph neural networks”.arXiv preprint\\narXiv:1809.02040.\\nSpeer, R., J. Chin, and C. Havasi. (2016). “ConceptNet 5.5: An Open\\nMultilingualGraphofGeneralKnowledge. Singh2002(2016)”. arXiv\\npreprint arxiv:1612.03975.\\nSrivastava, Y., V. Murali, S. R. Dubey, and S. Mukherjee. (2020).\\n“Visual question answering using deep learning: A survey and perfor-\\nmance analysis”. In:International Conference on Computer Vision\\nand Image Processing. Springer. 75–86.\\nSteen, J. and K. Markert. (2019). “Abstractive timeline summariza-\\ntion”. In: Proceedings of the 2nd Workshop on New Frontiers in\\nSummarization. 21–31.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 128}, page_content='REFERENCES 125\\nSu, D., Y. Xu, W. Dai, Z. Ji, T. Yu, and P. Fung. (2020). “Multi-\\nhop Question Generation with Graph Convolutional Network”. In:\\nFindings of the Association for Computational Linguistics: EMNLP\\n2020. 4636–4647.\\nSun, H., W. W. Cohen, and R. Salakhutdinov. (2021). “Iterative Hi-\\nerarchical Attention for Answering Complex Questions over Long\\nDocuments”. arXiv preprint arXiv:2106.00200.\\nTalmor, A. and J. Berant. (2018). “The Web as a Knowledge-Base\\nfor Answering Complex Questions”. In:Proceedings of the 2018\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Volume\\n1 (Long Papers). New Orleans, Louisiana: Association for Compu-\\ntational Linguistics. 641–651.doi: 10.18653/v1/N18-1059. url:\\nhttps://aclanthology.org/N18-1059.\\nTan, Y., D. Min, Y. Li, W. Li, N. Hu, Y. Chen, and G. Qi. (2023). “Eval-\\nuation of ChatGPT as a Question Answering System for Answering\\nComplex Questions”. arXiv: 2303.07992[cs.CL].\\nTang, D., N. Duan, T. Qin, and M. Zhou. (2017). “Question Answering\\nand Question Generation as Dual Tasks”.CoRR. abs/1706.02027.\\narXiv: 1706.02027.url: http://arxiv.org/abs/1706.02027.\\nTang, Y., H. T. Ng, and A. Tung. (2021). “Do Multi-Hop Question\\nAnswering Systems Know How to Answer the Single-Hop Sub-\\nQuestions?” In: Proceedings of the 16th Conference of the Euro-\\npean Chapter of the Association for Computational Linguistics:\\nMain Volume. Online: Association for Computational Linguistics.\\n3244–3249. doi: 10.18653/v1/2021.eacl-main.283. url: https:\\n//aclanthology.org/2021.eacl-main.283.\\nThayaparan, M., M. Valentino, V. Schlegel, and A. Freitas. (2019).\\n“Identifying Supporting Facts for Multi-hop Question Answering\\nwith Document Graph Networks”. In:Proceedings of the Thirteenth\\nWorkshop on Graph-Based Methods for Natural Language Processing\\n(TextGraphs-13). 42–51.\\nTian, K., E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn,\\nand C. D. Manning. (2023). “Just Ask for Calibration: Strategies\\nfor Eliciting Calibrated Confidence Scores from Language Models\\nFine-Tuned with Human Feedback”. arXiv: 2305.14975[cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 129}, page_content='126 REFERENCES\\nTrischler, A., T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman,\\nand K. Suleman. (2016). “NewsQA: A Machine Comprehension\\nDataset”. CoRR. abs/1611.09830. arXiv: 1611.09830. url: http:\\n//arxiv.org/abs/1611.09830.\\nTrivedi, H., N. Balasubramanian, T. Khot, and A. Sabharwal. (2020).\\n“Is Multihop QA in DiRe Condition? Measuring and Reducing\\nDisconnected Reasoning”. In:Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP).\\nOnline: Association for Computational Linguistics. 8846–8863.doi:\\n10.18653/v1/2020.emnlp-main.712. url: https://aclanthology.org/\\n2020.emnlp-main.712.\\nTrivedi, H., N. Balasubramanian, T. Khot, and A. Sabharwal. (2023).\\n“InterleavingRetrievalwithChain-of-ThoughtReasoningforKnowledge-\\nIntensive Multi-Step Questions”. arXiv: 2212.10509[cs.CL].\\nTrivedi, H., H. Kwon, T. Khot, A. Sabharwal, and N. Balasubramanian.\\n(2019). “Repurposing Entailment for Multi-Hop Question Answering\\nTasks”.In: Proceedings of the 2019 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers). 2948–\\n2958.\\nTu, M., K. Huang, G. Wang, J. Huang, X. He, and B. Zhou. (2020).\\n“Select, answer and explain: Interpretable multi-hop reading com-\\nprehension over multiple documents”. In:Proceedings of the AAAI\\nConference on Artificial Intelligence. Vol. 34. No. 05. 9073–9080.\\nVan den Oord, A., Y. Li, and O. Vinyals. (2018). “Representation\\nlearning with contrastive predictive coding”.arXiv e-prints: arXiv–\\n1807.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin. (2017a). “Attention is all you need”.\\nAdvances in neural information processing systems. 30.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, and I. Polosukhin. (2017b). “Attention is all you need”.\\nAdvances in neural information processing systems. 30.\\nVedantam, R., C. L. Zitnick, and D. Parikh. (2014). “Cider: consensus-\\nbasedimagedescriptionevaluation.CoRR”. arXiv preprint arXiv:1411.5726.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 130}, page_content='REFERENCES 127\\nVeličković, P., G. Cucurull, A. Casanova, A. Romero, P. Liò, and\\nY. Bengio. (2018). “Graph Attention Networks”. In:International\\nConference on Learning Representations.\\nWang, B. (2021). “Mesh-Transformer-JAX: Model-Parallel Implemen-\\ntation of Transformer Language Model with JAX”. https://github.\\ncom/kingoflolz/mesh-transformer-jax.\\nWang, H., M. Yu, X. Guo, R. Das, W. Xiong, and T. Gao. (2019). “Do\\nMulti-hop Readers Dream of Reasoning Chains?” In:Proceedings\\nof the 2nd Workshop on Machine Reading for Question Answering.\\n91–97.\\nWang, J., A. Jatowt, M. Färber, and M. Yoshikawa. (2020). “Answering\\nEvent-Related Questions over Long-Term News Article Archives”.\\nIn: Advances in Information Retrieval - 42nd European Conference\\non IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020,\\nProceedings, Part I. Vol. 12035.Lecture Notes in Computer Science.\\nSpringer. 774–789.\\nWang, J., A. Jatowt, M. Färber, and M. Yoshikawa. (2021a). “Im-\\nproving question answering for event-focused questions in temporal\\ncollections of news articles”.Inf. Retr. J.24(1): 29–54.\\nWang, J., A. Jatowt, and M. Yoshikawa. (2021b). “ArchivalQA: A Large-\\nscale Benchmark Dataset for Open Domain Question Answering\\nover Archival News Collections”.CoRR. abs/2109.03438. arXiv:\\n2109.03438. url: https://arxiv.org/abs/2109.03438.\\nWang, J., A. Jatowt, and M. Yoshikawa. (2021c). “Event Occurrence\\nDate Estimation based on Multivariate Time Series Analysis over\\nTemporal Document Collections”. In:SIGIR ’21: The 44th Inter-\\nnational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, Virtual Event, Canada, July 11-15, 2021.\\nACM. 398–407.\\nWang, S. and J. Jiang. (2016). “Machine comprehension using match-\\nlstm and answer pointer”.arXiv preprint arXiv:1608.07905.\\nWang,S.,H.Fang,M.Khabsa,H.Mao,andH.Ma.(2021d).“Entailment\\nas Few-Shot Learner”. arXiv: 2104.14690[cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 131}, page_content='128 REFERENCES\\nWang, X., J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A.\\nChowdhery, and D. Zhou. (2023). “Self-Consistency Improves Chain\\nof Thought Reasoning in Language Models”. arXiv: 2203.11171\\n[cs.CL].\\nWei, J., X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\\nQ. Le, and D. Zhou. (2023). “Chain-of-Thought Prompting Elicits\\nReasoning in Large Language Models”. arXiv: 2201.11903[cs.CL].\\nWeiss, D. B., P. Roit, O. Ernst, and I. Dagan. (2021). “Extending\\nMulti-Text Sentence Fusion Resources via Pyramid Annotations”.\\narXiv preprint arXiv:2110.04517.\\nWelbl, J., P. Stenetorp, and S. Riedel. (2018). “Constructing datasets for\\nmulti-hop reading comprehension across documents”.Transactions\\nof the Association for Computational Linguistics. 6: 287–302.\\nWelleck, S., I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston.\\n(2019). “Neural Text Generation With Unlikelihood Training”. In:\\nInternational Conference on Learning Representations.\\nWilliams, A., N. Nangia, and S. Bowman. (2018). “A Broad-Coverage\\nChallenge Corpus for Sentence Understanding through Inference”. In:\\nProceedings of the 2018 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers). New Orleans, Louisiana: As-\\nsociation for Computational Linguistics. 1112–1122.doi: 10.18653/\\nv1/N18-1101. url: https://aclanthology.org/N18-1101.\\nWilliams, R. J. (1992). “Simple Statistical Gradient-Following Algo-\\nrithms for Connectionist Reinforcement Learning”.Mach. Learn.\\n8(3–4): 229–256.issn: 0885-6125.doi: 10.1007/BF00992696.url:\\nhttps://doi.org/10.1007/BF00992696.\\nWishart, D. S., C. Knox, A. C. Guo, D. Cheng, S. Shrivastava, D. Tzur,\\nB. Gautam, and M. Hassanali. (2008). “DrugBank: a knowledgebase\\nfor drugs, drug actions and drug targets”.Nucleic acids research.\\n36(suppl_1): D901–D906.\\nWu, J., L. Yang, Y. Ji, W. Huang, B. F. Karlsson, and M. Oku-\\nmura. (2024). “GenDec: A robust generative Question-decomposition\\nmethod for Multi-hop reasoning”. arXiv: 2402.11166[cs.CL].'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 132}, page_content='REFERENCES 129\\nWu, Q., D. Teney, P. Wang, C. Shen, A. Dick, and A. van den Hen-\\ngel. (2017). “Visual question answering: A survey of methods and\\ndatasets”. Computer Vision and Image Understanding. 163: 21–40.\\nWu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,\\nM. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\\nM. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H.\\nKazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J.\\nSmith, D. Riesa, A. Rudnick, O. Vinyals, G. Corrado, M. Hughes,\\nand J. Dean. (2016). “Google’s Neural Machine Translation System:\\nBridging the Gap between Human and Machine Translation”.CoRR.\\nabs/1609.08144. arXiv: 1609.08144.url: http://arxiv.org/abs/1609.\\n08144.\\nWu, Z., S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. (2020). “A\\ncomprehensive survey on graph neural networks”.IEEE transactions\\non neural networks and learning systems. 32(1): 4–24.\\nXiong, W., X. L. Li, S. Iyer, J. Du, P. S. H. Lewis, W. Y. Wang,\\nY. Mehdad, W. Yih, S. Riedel, D. Kiela, and B. Oguz. (2020).\\n“Answering Complex Open-Domain Questions with Multi-Hop Dense\\nRetrieval”. CoRR. abs/2009.12756. arXiv: 2009.12756.url: https:\\n//arxiv.org/abs/2009.12756.\\nXiong, W., M. Yu, X. Guo, H. Wang, S. Chang, M. Campbell, and\\nW. Y. Wang. (2019). “Simple yet Effective Bridge Reasoning for\\nOpen-Domain Multi-Hop Question Answering”. In:Proceedings of\\nthe 2nd Workshop on Machine Reading for Question Answering.\\n48–52.\\nXu, W., Y. Deng, H. Zhang, D. Cai, and W. Lam. (2021). “Exploiting\\nReasoning Chains for Multi-hop Science Question Answering”. arXiv:\\n2109.02905 [cs.CL].\\nYadav, V., S. Bethard, and M. Surdeanu. (2019a). “Alignment over\\nHeterogeneous Embeddings for Question Answering”. In:Proceed-\\nings of the 2019 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long and Short Papers). Minneapolis, Min-\\nnesota: Association for Computational Linguistics. 2681–2691.doi:\\n10.18653/v1/N19-1274. url: https://aclanthology.org/N19-1274.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 133}, page_content='130 REFERENCES\\nYadav, V., S. Bethard, and M. Surdeanu. (2019b). “Quick and (not so)\\nDirty: Unsupervised Selection of Justification Sentences for Multi-\\nhop Question Answering”. In:Proceedings of the 2019 Conference\\non Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). Hong Kong, China: Association for Computa-\\ntional Linguistics. 2578–2589.doi: 10.18653/v1/D19-1260. url:\\nhttps://aclanthology.org/D19-1260.\\nYadav,V.,S.Bethard,andM.Surdeanu.(2020).“UnsupervisedAlignment-\\nbased Iterative Evidence Retrieval for Multi-hop Question Answer-\\ning”. In:Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics. 4514–4525.\\nYadav, V., S. Bethard, and M. Surdeanu. (2021). “If You Want to Go\\nFar Go Together: Unsupervised Joint Candidate Evidence Retrieval\\nfor Multi-hop Question Answering”. In:Proceedings of the 2021\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies. 4571–\\n4581.\\nYan, R., X. Wan, J. Otterbacher, L. Kong, X. Li, and Y. Zhang. (2011).\\n“Evolutionary timeline summarization: a balanced optimization\\nframework via iterative substitution”. In:Proceedings of the 34th\\ninternational ACM SIGIR conference on Research and development\\nin Information Retrieval. 745–754.\\nYang, Z., P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov,\\nand C. D. Manning. (2018). “HotpotQA: A Dataset for Diverse,\\nExplainable Multi-hop Question Answering”. In:Proceedings of\\nthe 2018 Conference on Empirical Methods in Natural Language\\nProcessing. 2369–2380.\\nYao, K., L. Zhang, T. Luo, L. Tao, and Y. Wu. (2018). “Teaching\\nMachines to Ask Questions”. In:Proceedings of the Twenty-Seventh\\nInternational Joint Conference on Artificial Intelligence, IJCAI-\\n18. International Joint Conferences on Artificial Intelligence Orga-\\nnization. 4546–4552. doi: 10.24963/ijcai.2018/632. url: https:\\n//doi.org/10.24963/ijcai.2018/632.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 134}, page_content='REFERENCES 131\\nYavuz, S., K. Hashimoto, Y. Zhou, N. S. Keskar, and C. Xiong. (2022).\\n“Modeling Multi-hop Question Answering as Single Sequence Pre-\\ndiction”. arXiv: 2205.09226[cs.CL].\\nYe, D., Y. Lin, Z. Liu, Z. Liu, and M. Sun. (2019). “Multi-paragraph\\nreasoning with knowledge-enhanced graph neural network”.arXiv\\npreprint arXiv:1911.02170.\\nYu, J., W. Liu, S. Qiu, Q. Su, K. Wang, X. Quan, and J. Yin. (2020).\\n“Low-Resource Generation of Multi-hop Reasoning Questions”. In:\\nProceedings of the 58th Annual Meeting of the Association for Com-\\nputational Linguistics. Online: Association for Computational Lin-\\nguistics. 6729–6739. doi: 10.18653/v1/2020.acl-main.601. url:\\nhttps://aclanthology.org/2020.acl-main.601.\\nYu, Y., A. Jatowt, A. Doucet, K. Sugiyama, and M. Yoshikawa. (2021).\\n“Multi-TimeLine Summarization (MTLS): Improving Timeline Sum-\\nmarization by Generating Multiple Summaries”. In:Proceedings\\nof the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural\\nLanguage Processing (Volume 1: Long Papers). Online: Association\\nfor Computational Linguistics. 377–387.\\nZaib, M., W. E. Zhang, Q. Z. Sheng, A. Mahmood, and Y. Zhang. (2021).\\n“Conversationalquestionanswering:Asurvey”. arXiv preprint arXiv:2106.00874.\\nZelikman, E., Y. Wu, J. Mu, and N. D. Goodman. (2022). “STaR: Boot-\\nstrapping Reasoning With Reasoning”. arXiv: 2203.14465[cs.LG].\\nZhang, M., F. Li, Y. Wang, Z. Zhang, Y. Zhou, and X. Li. (2020).\\n“Coarse and Fine Granularity Graph Reasoning for Interpretable\\nMulti-Hop Question Answering”.IEEE Access. 8: 56755–56765.doi:\\n10.1109/ACCESS.2020.2981134.\\nZhang, X. and M. Lapata. (2017). “Sentence Simplification with Deep\\nReinforcement Learning”. In:Proceedings of the 2017 Conference\\non Empirical Methods in Natural Language Processing. Copenhagen,\\nDenmark: Association for Computational Linguistics. 584–594.doi:\\n10.18653/v1/D17-1062. url: https://aclanthology.org/D17-1062.'), Document(metadata={'source': '/Users/aryankeshri/Downloads/Entivin_Task/Resources/Multihop_QA.pdf', 'page': 135}, page_content='132 REFERENCES\\nZhang, Y., P. Nie, A. Ramamurthy, and L. Song. (2021). “Answering\\nAny-hop Open-domain Questions with Iterative Document Rerank-\\ning”. In:Proceedings of the 44th International ACM SIGIR Confer-\\nence on Research and Development in Information Retrieval. 481–\\n490.\\nZhao, R., X. Li, S. Joty, C. Qin, and L. Bing. (2023a). “Verify-and-\\nEdit: A Knowledge-Enhanced Chain-of-Thought Framework”. arXiv:\\n2305.03268 [cs.CL].\\nZhao, W. X., K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\\nB. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen,\\nJ. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and\\nJ.-R. Wen. (2023b). “A Survey of Large Language Models”. arXiv:\\n2303.18223 [cs.CL].\\nZhao, Y., X. Ni, Y. Ding, and Q. Ke. (2018a). “Paragraph-level Neural\\nQuestion Generation with Maxout Pointer and Gated Self-attention\\nNetworks”. In:Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing. Brussels, Belgium: Associ-\\nation for Computational Linguistics. 3901–3910.doi: 10.18653/v1/\\nD18-1424. url: https://aclanthology.org/D18-1424.\\nZhao, Y., X. Ni, Y. Ding, and Q. Ke. (2018b). “Paragraph-level Neural\\nQuestion Generation with Maxout Pointer and Gated Self-attention\\nNetworks”. In:Proceedings of the 2018 Conference on Empirical\\nMethods in Natural Language Processing. Brussels, Belgium: Associ-\\nation for Computational Linguistics. 3901–3910.doi: 10.18653/v1/\\nD18-1424. url: https://aclanthology.org/D18-1424.\\nZhou, B., K. Richardson, X. Yu, and D. Roth. (2022). “Learning to\\nDecompose: Hypothetical Question Decomposition Based on Com-\\nparable Texts”. arXiv: 2210.16865[cs.CL].\\nZhou, D., N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans,\\nC. Cui, O. Bousquet, Q. V. Le, and E. H. Chi. (2023). “Least-to-Most\\nPrompting Enables Complex Reasoning in Large Language Models”.\\nIn: The Eleventh International Conference on Learning Representa-\\ntions. url: https://openreview.net/forum?id=WZH7099tgfM.\\nZhu, F., W. Lei, C. Wang, J. Zheng, S. Poria, and T.-S. Chua. (2021).\\n“Retrieving and reading: A comprehensive survey on open-domain\\nquestion answering”.arXiv preprint arXiv:2101.00774.')]\n"
     ]
    }
   ],
   "source": [
    "print(pages)\n",
    "# print(f\"{pages[4].metadata}\\n\")\n",
    "# print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1277846",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1338920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa9231",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720d6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryankeshri/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/_4/ky3k7xbn09562xrb4g_gkbsc0000gn/T/ipykernel_62943/2337468291.py:35: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "# question = \"What are Multihop RAG?\"\n",
    "question = \"Which country is the capital of the author of Pride and Prejudice located in?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4d7ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryankeshri/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8a7e2",
   "metadata": {},
   "source": [
    "## Make tool\n",
    "\n",
    "make tool for your multihop rag here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a28c11e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Who is the author of Pride and Prejudice?\\n2. What is the capital of the country where the author of Pride and Prejudice was born or resided?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Decomposer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a question decomposer that decomposes a question into sub-questions, if the original question is complex or indirect or not straightforward.\\n\n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "requery_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Decompose into two or more sub-question as per the requirement.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_decomposer = requery_prompt | llm | StrOutputParser()\n",
    "question_decomposer.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10680689",
   "metadata": {},
   "source": [
    "## Create graphs\n",
    "\n",
    "Now let's create our graph for implementing multi-hop features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a80dc",
   "metadata": {},
   "source": [
    "### Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba074ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph. \n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        iterative_Search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    questions: List[str]\n",
    "    generation: str\n",
    "    requery_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa6e9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "    \n",
    "    Args: \n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns: \n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"--Retrieve--\\n\")\n",
    "    documents = []\n",
    "    questions = state['questions']\n",
    "    \n",
    "    for question in questions:\n",
    "        # Retrieval \n",
    "        documents.extend(retriever.get_relevant_documents(question))\n",
    "        \n",
    "    return {\"documents\": documents, \"questions\": questions}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate Answer: \n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns: \n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---Generate---\\n\")\n",
    "    \n",
    "    questions = state['questions']\n",
    "    documents = state['documents']\n",
    "    generation = \"\"\n",
    "    # RAG generation\n",
    "    print(documents)\n",
    "    for question in questions:\n",
    "        generation = generation + rag_chain.invoke({'context': documents, 'question': question})\n",
    "        \n",
    "    return {\"documents\": documents, \"questions\": questions[0], \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Args: \n",
    "        state (dict): The current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---Check Document Relevance to Question---\\n\")\n",
    "    question = state[\"questions\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    re_query = \"No\"\n",
    "    for d in documents: # Its a single loop cause, in the begining there is only one question in the list. and this function is called only in the begining\n",
    "\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\\n\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\\n\")\n",
    "            re_query = \"Yes\"\n",
    "            continue\n",
    "#         else:\n",
    "#             print(\"---No relevant document found---\\n\")\n",
    "#             re_query = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"questions\": question, \"requery_search\": re_query}\n",
    "\n",
    "\n",
    "def decompose_query(state):\n",
    "    \"\"\"\n",
    "    Decompose the query into sub-queries for better retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appends multiple questions to questions key with a re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---Decompose Query---\\n\")\n",
    "    question = state['questions'][0]\n",
    "    \n",
    "    # Decomposed questions \n",
    "    questions_str = question_decomposer.invoke({\"question\": question})\n",
    "    print(questions_str)\n",
    "    print(\"\\n\")\n",
    "    new_questions = questions_str.split('\\n')\n",
    "    \n",
    "    return {\"documents\": documents, \"questions\": new_questions}\n",
    "\n",
    "def decide_requery(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or decompose a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---Assesing Graded Documents---\\n\")\n",
    "    state['questions']\n",
    "    requery = state['requery_search']\n",
    "    state['documents']\n",
    "    \n",
    "    if requery == \"Yes\":\n",
    "        print(\"---Decomposing Question into sub-questions---\\n\")\n",
    "        return \"decompose_query\"\n",
    "    else:\n",
    "        print(\"---Decision Generate---\\n\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dadf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"decompose_query\", decompose_query)\n",
    "workflow.add_node(\"retrieve_after_decomposition\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "# workflow.add_node(\"\")\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_requery,\n",
    "    {\n",
    "        \"decompose_query\": \"decompose_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"decompose_query\", \"retrieve_after_decomposition\")\n",
    "workflow.add_edge(\"retrieve_after_decomposition\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad01f1",
   "metadata": {},
   "source": [
    "## Use the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f0019ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Retrieve--\n",
      "\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---Check Document Relevance to Question---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\n",
      "---Assesing Graded Documents---\n",
      "\n",
      "---Decomposing Question into sub-questions---\n",
      "\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---Decompose Query---\n",
      "\n",
      "I'm sorry, but the question provided is too vague and lacks context for me to decompose it into sub-questions. Could you please provide more information or clarify the question so that I can better understand the underlying intent and decompose it effectively?\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the types of agent memory?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Node\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1657\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1658\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1659\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1660\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1661\u001b[0m         ):\n\u001b[1;32m   1662\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Downloads/Entivin_Task/lang_graph_rag/lib/python3.9/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[17], line 102\u001b[0m, in \u001b[0;36mdecompose_query\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m new_questions \u001b[38;5;241m=\u001b[39m questions_str\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdocuments\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_questions}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"questions\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_hop_rag",
   "language": "python",
   "name": "lang_graph_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
